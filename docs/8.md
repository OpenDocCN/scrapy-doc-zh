# 命令行工具

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

0.10 新版功能.

Scrapy 通过控制 `scrapy` 命令行工具，这里称为“scrapy工具”，用于区分子命令，我们称之为“命令”或“scrapy命令”。

Scrapy工具提供了多个命令，用于多种目的，每个命令接受一组不同的参数和选项。

(The `scrapy deploy` 命令已在1.0中删除，以支持独立的 `scrapyd-deploy` . 见 [Deploying your project](https://scrapyd.readthedocs.io/en/latest/deploy.html) ）

## 配置设置

Scrapy将查找ini样式的配置参数 `scrapy.cfg` 标准位置的文件：

1.  `/etc/scrapy.cfg` 或 `c:\scrapy\scrapy.cfg` （全系统）
2.  `~/.config/scrapy.cfg` （ `$XDG_CONFIG_HOME` ） `~/.scrapy.cfg` （ `$HOME` ）用于全局（用户范围）设置，以及
3.  `scrapy.cfg` 在Scrapy项目的根目录中（参见下一节）。

这些文件中的设置按列出的首选顺序合并：用户定义的值比系统范围的默认值具有更高的优先级，并且在定义时，项目范围的设置将覆盖所有其他设置。

Scrapy还可以理解并通过许多环境变量进行配置。目前有：

*   `SCRAPY_SETTINGS_MODULE` （见 [指定设置](settings.html#topics-settings-module-envvar) ）
*   `SCRAPY_PROJECT` （见 [在项目之间共享根目录](#topics-project-envvar) ）
*   `SCRAPY_PYTHON_SHELL` （见 [Scrapy shell](shell.html#topics-shell) ）

## Scrapy 项目的默认结构

在深入研究命令行工具及其子命令之前，让我们先了解一个零碎项目的目录结构。

尽管可以修改，但默认情况下，所有零碎项目都具有相同的文件结构，类似于：

```py
scrapy.cfg
myproject/
    __init__.py
    items.py
    middlewares.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...

```

目录 `scrapy.cfg` 文件驻留称为 项目根目录. 该文件包含定义项目设置的python模块的名称。下面是一个例子：

```py
[settings]
default = myproject.settings

```

## 在项目之间共享根目录

一个项目根目录，其中包含 `scrapy.cfg` ，可以由多个零碎项目共享，每个项目都有自己的设置模块。

在这种情况下，必须为下面的设置模块定义一个或多个别名 `[settings]` 在你 `scrapy.cfg` 文件：：

```py
[settings]
default = myproject1.settings
project1 = myproject1.settings
project2 = myproject2.settings

```

默认情况下， `scrapy` 命令行工具将使用 `default` 设置。使用 `SCRAPY_PROJECT` 用于指定其他项目的环境变量 `scrapy` 使用：

```py
$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot

```

## 使用 `scrapy` 工具

您可以通过运行无参数的scrapy工具开始，它将打印一些用法帮助和可用的命令：

```py
Scrapy X.Y - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]

```

如果您在一个零碎的项目中，第一行将打印当前活动的项目。在本例中，它是从项目外部运行的。如果从项目内部运行，它将打印如下内容：

```py
Scrapy X.Y - project: myproject

Usage:
  scrapy <command> [options] [args]

[...]

```

### 创建项目

你通常会做的第一件事就是 `scrapy` 工具是创建零碎项目：

```py
scrapy startproject myproject [project_dir]

```

它将在 `project_dir` 目录。如果 `project_dir` 没有指定， `project_dir` 将与 `myproject` .

接下来，进入新的项目目录：

```py
cd project_dir

```

你已经准备好使用 `scrapy` 从那里管理和控制项目的命令。

### 控制性项目

你使用 `scrapy` 从项目内部使用工具来控制和管理它们。

例如，要创建新的 Spider ：

```py
scrapy genspider mydomain mydomain.com

```

一些下流的命令（比如 [`crawl`](#std:command-crawl) ）必须从零碎的项目内部运行。见 [commands reference](#topics-commands-ref) 下面是关于必须从项目内部运行哪些命令的详细信息，而不是。

还要记住，当从内部项目运行某些命令时，它们的行为可能略有不同。例如，fetch命令将使用 Spider 重写的行为（例如 `user_agent` 属性来重写用户代理）如果要获取的URL与某个特定的 Spider 相关联。这是故意的，因为 `fetch` 命令用于检查 Spider 如何下载页面。

## 可用工具命令

本节包含可用的内置命令列表，其中包含说明和一些用法示例。记住，您可以通过运行以下命令获取有关每个命令的更多信息：

```py
scrapy <command> -h

```

您可以使用以下命令查看所有可用命令：

```py
scrapy -h

```

有两种命令，一种是只从零碎项目（特定于项目的命令）内部工作的命令，另一种是不使用活动零碎项目（全局命令）的命令，尽管从项目内部运行时它们的行为可能略有不同（因为它们将使用项目覆盖设置）。

全局命令：

*   [`startproject`](#std:command-startproject)
*   [`genspider`](#std:command-genspider)
*   [`settings`](#std:command-settings)
*   [`runspider`](#std:command-runspider)
*   [`shell`](#std:command-shell)
*   [`fetch`](#std:command-fetch)
*   [`view`](#std:command-view)
*   [`version`](#std:command-version)

仅Project命令：

*   [`crawl`](#std:command-crawl)
*   [`check`](#std:command-check)
*   [`list`](#std:command-list)
*   [`edit`](#std:command-edit)
*   [`parse`](#std:command-parse)
*   [`bench`](#std:command-bench)

### 启动项目

*   Syntax： `scrapy startproject &lt;project_name&gt; [project_dir]`
*   需要项目：_否_

创建一个名为 `project_name` 下 `project_dir` 目录。如果 `project_dir` 没有指定， `project_dir` 将与 `project_name` .

使用实例：

```py
$ scrapy startproject myproject

```

### 基因 Spider

*   Syntax： `scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;`
*   需要项目：_否_

在当前文件夹或当前项目的 `spiders` 文件夹（如果从项目内部调用）。这个 `&lt;name&gt;` 参数设置为spider的 `name` ，同时 `&lt;domain&gt;` 用于生成 `allowed_domains` 和 `start_urls` Spider 的属性。

使用实例：

```py
$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'

```

这只是一个基于预先定义的模板创建spider的快捷命令，但肯定不是创建spider的唯一方法。您可以自己创建 Spider 源代码文件，而不是使用这个命令。

### 爬行

*   Syntax： `scrapy crawl &lt;spider&gt;`
*   需要项目：_是_

开始用 Spider 爬行。

用法示例：

```py
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]

```

### 检查

*   Syntax： `scrapy check [-l] &lt;spider&gt;`
*   需要项目：_是_

运行合同检查。

用法示例：

```py
$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4

```

### 列表

*   Syntax： `scrapy list`
*   需要项目：_是_

列出当前项目中所有可用的spider。每行输出一个 Spider 。

使用实例：

```py
$ scrapy list
spider1
spider2

```

### 编辑

*   Syntax： `scrapy edit &lt;spider&gt;`
*   需要项目：_是_

使用中定义的编辑器编辑给定的 Spider `EDITOR` 环境变量或（如果未设置） [`EDITOR`](settings.html#std:setting-EDITOR) 设置。

这个命令仅作为最常见情况下的快捷方式提供，开发人员当然可以自由选择任何工具或IDE来编写和调试spider。

使用实例：

```py
$ scrapy edit spider1

```

### 取来

*   Syntax： `scrapy fetch &lt;url&gt;`
*   需要项目：_否_

使用ScrapyDownloader下载给定的URL，并将内容写入标准输出。

这个命令的有趣之处在于它获取了 Spider 如何下载它的页面。例如，如果 Spider `USER_AGENT` 覆盖用户代理的属性，它将使用该属性。

所以这个命令可以用来“查看” Spider 如何获取特定的页面。

如果在项目之外使用，则不会应用特定的每 Spider 行为，它只会使用默认的scrapy下载器设置。

支持的选项：

*   `--spider=SPIDER` ：绕过Spider自动检测并强制使用特定Spider
*   `--headers` ：打印响应的HTTP头而不是响应的正文
*   `--no-redirect` ：不遵循HTTP 3xx重定向（默认为遵循它们）

用法示例：

```py
$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}

```

### 看法

*   Syntax： `scrapy view &lt;url&gt;`
*   需要项目：_否_

在浏览器中打开给定的URL，因为您的废 Spider 会“看到”它。有时候 Spider 看到的页面与普通用户不同，所以这可以用来检查 Spider “看到”什么，并确认它是你所期望的。

支持的选项：

*   `--spider=SPIDER` ：绕过Spider自动检测并强制使用特定Spider
*   `--no-redirect` ：不遵循HTTP 3xx重定向（默认为遵循它们）

使用实例：

```py
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]

```

### 壳

*   Syntax： `scrapy shell [url]`
*   需要项目：_否_

为给定的URL（如果给定）启动scrapy shell；如果没有给定URL，则为空。还支持Unix风格的本地文件路径，无论是相对于 `./` 或 `../` 前缀或绝对文件路径。见 [Scrapy shell](shell.html#topics-shell) 更多信息。

支持的选项：

*   `--spider=SPIDER` ：绕过Spider自动检测并强制使用特定Spider
*   `-c code` ：评估shell中的代码，打印结果并退出
*   `--no-redirect` ：不遵循HTTP 3xx重定向（默认为遵循它们）；这只影响在命令行上作为参数传递的URL；一旦进入shell， `fetch(url)` 默认情况下仍将遵循HTTP重定向。

使用实例：

```py
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
(200, 'http://www.example.com/')

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(200, 'http://example.com/')

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')

```

### 解析

*   Syntax： `scrapy parse &lt;url&gt; [options]`
*   需要项目：_是_

获取给定的URL，并使用处理它的spider，使用 `--callback` 选项，或 `parse` 如果没有给出。

支持的选项：

*   `--spider=SPIDER` ：绕过Spider自动检测并强制使用特定Spider
*   `--a NAME=VALUE` ：set spider参数（可以重复）
*   `--callback` 或 `-c` ：用作分析响应的回调的spider方法
*   `--meta` 或 `-m` ：将传递给回调请求的附加请求元。这必须是有效的JSON字符串。示例：--meta='“foo”：“bar”'
*   `--pipelines` ：通过管道处理项目
*   `--rules` 或 `-r` 使用 [`CrawlSpider`](spiders.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider") 发现用于解析响应的回调（即spider方法）的规则
*   `--noitems` ：不显示刮掉的项目
*   `--nolinks` ：不显示提取的链接
*   `--nocolour` ：避免使用Pygments对输出着色
*   `--depth` 或 `-d` ：应递归执行请求的深度级别（默认值：1）
*   `--verbose` 或 `-v` ：显示每个深度级别的信息

使用实例：

```py
$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': 'Example item',
 'category': 'Furniture',
 'length': '12 cm'}]

# Requests  -----------------------------------------------------------------
[]

```

### 设置

*   Syntax： `scrapy settings [options]`
*   需要项目：_否_

获取 Scrapy 设置的值。

如果在项目中使用，它将显示项目设置值，否则它将显示该设置的默认碎片值。

示例用法：

```py
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0

```

### 运行 Spider

*   Syntax： `scrapy runspider &lt;spider_file.py&gt;`
*   需要项目：_否_

运行一个包含在python文件中的spider，而不必创建一个项目。

示例用法：

```py
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]

```

### 版本

*   Syntax： `scrapy version [-v]`
*   需要项目：_否_

打印残缺版本。如果使用 `-v` 它还打印python、twisted和platform信息，这对bug报告很有用。

### 长凳

0.17 新版功能.

*   Syntax： `scrapy bench`
*   需要项目：_否_

运行一个快速基准测试。 [Benchmarking](benchmarking.html#benchmarking) .

## 自定义项目命令

还可以使用 [`COMMANDS_MODULE`](#std:setting-COMMANDS_MODULE) 设置。请参阅中的scrappy命令 [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands) 有关如何实现命令的示例。

### COMMANDS_MODULE

违约： `''` （空字符串）

用于查找自定义 Scrapy 命令的模块。这用于为您的Scrapy项目添加自定义命令。

例子：：

```py
COMMANDS_MODULE = 'mybot.commands'

```

### 通过setup.py入口点注册命令

注解

这是一个实验特性，小心使用。

还可以通过添加 `scrapy.commands` 库入口点中的节 `setup.py` 文件。

下面的示例添加了 `my_command` 命令：

```py
from setuptools import setup, find_packages

setup(name='scrapy-mymodule',
  entry_points={
    'scrapy.commands': [
      'my_command=my_scrapy_module.commands:MyCommand',
    ],
  },
 )

```