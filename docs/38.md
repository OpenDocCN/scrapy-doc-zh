# 作业：暂停和恢复爬行

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

有时，对于大型网站，暂停爬行并稍后恢复爬行是可取的。

Scrapy通过提供以下功能来支持此功能：

*   在磁盘上保持预定请求的计划程序
*   重复的筛选器，用于将访问的请求保持在磁盘上
*   在批处理之间保持某些 Spider 状态（键/值对）持久的扩展。

## 作业目录

要启用持久性支持，只需定义 _job directory_ 通过 `JOBDIR` 设置。这个目录将用于存储保持单个作业（即 Spider 运行）状态所需的所有数据。需要注意的是，这个目录不能由不同的 Spider 共享，甚至不能由同一 Spider 的不同作业/运行共享，因为它用于存储 _single_ 工作。

## 如何使用它

要在启用持久性支持的情况下启动spider，请按如下方式运行：

```py
scrapy crawl somespider -s JOBDIR=crawls/somespider-1

```

然后，您可以随时安全地停止 Spider （通过按ctrl-c或发送信号），然后通过发出相同的命令恢复 Spider ：

```py
scrapy crawl somespider -s JOBDIR=crawls/somespider-1

```

## 保持批之间的持久状态

有时您需要在暂停/恢复批处理之间保持一些持久的 Spider 状态。你可以使用 `spider.state` 属性，它应该是dict。当spider启动和停止时，有一个内置扩展负责从作业目录序列化、存储和加载该属性。

下面是一个使用spider状态的回调示例（为了简洁起见，省略了其他spider代码）：

```py
def parse_item(self, response):
    # parse item here
    self.state['items_count'] = self.state.get('items_count', 0) + 1

```

## 持久性问题

如果您想使用零碎的持久性支持，需要记住以下几点：

### cookies过期

cookies可能会过期。因此，如果您不快速恢复您的 Spider ，那么计划的请求可能不再有效。如果 Spider 不依赖饼干，这就不是问题了。

### 请求序列化

请求必须可由 `pickle` 模块，以便持久性工作，所以您应该确保您的请求是可序列化的。

这里最常见的问题是使用 `lambda` 无法持久化的请求回调函数。

例如，这不起作用：

```py
def some_callback(self, response):
    somearg = 'test'
    return scrapy.Request('http://www.example.com', callback=lambda r: self.other_callback(r, somearg))

def other_callback(self, response, somearg):
    print("the argument passed is: %s" % somearg)

```

但这将：

```py
def some_callback(self, response):
    somearg = 'test'
    return scrapy.Request('http://www.example.com', callback=self.other_callback, meta={'somearg': somearg})

def other_callback(self, response):
    somearg = response.meta['somearg']
    print("the argument passed is: %s" % somearg)

```

如果要记录无法序列化的请求，可以设置 [`SCHEDULER_DEBUG`](settings.html#std:setting-SCHEDULER_DEBUG) 设置为 `True` 在项目的“设置”页中。它是 `False` 默认情况下。