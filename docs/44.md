# 核心API

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

0.15 新版功能.

本节记录了Scrapy核心API，它是为扩展和中间件的开发人员设计的。

## 爬虫API

Scrapy API的主要入口点是 [`Crawler`](#scrapy.crawler.Crawler "scrapy.crawler.Crawler") 对象，通过 `from_crawler` 类方法。这个对象提供对所有Scrapy核心组件的访问，它是扩展访问它们并将其功能连接到Scrapy的唯一方法。

扩展管理器负责加载和跟踪已安装的扩展，并通过 [`EXTENSIONS`](settings.html#std:setting-EXTENSIONS) 包含所有可用扩展名及其顺序的字典的设置，类似于 [configure the downloader middlewares](downloader-middleware.html#topics-downloader-middleware-setting) .

```py
class scrapy.crawler.Crawler(spidercls, settings)
```

爬虫对象必须用 [`scrapy.spiders.Spider`](spiders.html#scrapy.spiders.Spider "scrapy.spiders.Spider") 子类和A [`scrapy.settings.Settings`](#scrapy.settings.Settings "scrapy.settings.Settings") 对象。

```py
settings
```

此爬网程序的设置管理器。

这被扩展和中间软件用来访问这个爬虫程序的碎片设置。

有关 Scrapy 设置的介绍，请参见 [设置](settings.html#topics-settings) .

对于API见 [`Settings`](#scrapy.settings.Settings "scrapy.settings.Settings") 类。

```py
signals
```

这个爬虫的信号管理器。

这被扩展和中间商用来将自己连接到零碎的功能中。

有关信号的介绍，请参见 [信号](signals.html#topics-signals) .

对于API见 `SignalManager` 类。

```py
stats
```

这个爬虫的统计收集程序。

这用于从扩展和中间软件记录其行为的统计信息，或访问由其他扩展收集的统计信息。

有关stats集合的介绍，请参见 [统计数据集合](stats.html#topics-stats) .

对于API见 [`StatsCollector`](#scrapy.statscollectors.StatsCollector "scrapy.statscollectors.StatsCollector") 类。

```py
extensions
```

跟踪已启用扩展的扩展管理器。

大多数扩展不需要访问这个属性。

有关扩展名的介绍和scrapy上可用扩展名的列表，请参见 [扩展](extensions.html#topics-extensions) .

```py
engine
```

执行引擎，它协调调度程序、下载程序和spider之间的核心爬行逻辑。

有些扩展可能希望访问scrapy引擎，检查或修改下载程序和调度程序的行为，尽管这是一种高级用法，而且这个API还不稳定。

```py
spider
```

Spider 当前正在被爬行。这是构建爬虫程序时提供的 Spider 类的实例，它是在 [`crawl()`](#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl") 方法。

```py
crawl(*args, **kwargs)
```

通过用给定的 `args` 和 `kwargs` 参数，同时设置运行中的执行引擎。

返回在爬网完成时激发的延迟。

## 设置API

```py
scrapy.settings.SETTINGS_PRIORITIES
```

设置Scrapy中使用的默认设置优先级的键名称和优先级级别的字典。

每个项目定义一个设置入口点，为其提供标识代码名和整数优先级。在 [`Settings`](#scrapy.settings.Settings "scrapy.settings.Settings") 类。

```py
SETTINGS_PRIORITIES = {
    'default': 0,
    'command': 10,
    'project': 20,
    'spider': 30,
    'cmdline': 40,
}

```

有关每个设置源的详细说明，请参阅： [设置](settings.html#topics-settings) .

```py
scrapy.settings.get_settings_priority(priority)
```

在中查找给定字符串优先级的小助手函数 [`SETTINGS_PRIORITIES`](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES") 并返回其数值，或直接返回给定的数值优先级。

```py
class scrapy.settings.Settings(values=None, priority='project')
```

基类：[`scrapy.settings.BaseSettings`](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings")

此对象存储内部组件配置的碎片设置，并可用于任何进一步的自定义。

它是一个直接的子类，支持 [`BaseSettings`](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings") . 另外，在实例化这个类之后，新对象将具有上面描述的全局默认设置 [内置设置参考](settings.html#topics-settings-ref) 已经填充。

```py
class scrapy.settings.BaseSettings(values=None, priority='project')
```

此类的实例的行为类似于字典，但将优先级与其 `(key, value)` 对，并且可以冻结（即标记为不可变）。

键值项可以在初始化时通过 `values` 他们会接受 `priority` 水平（除非 `values` 已经是的实例 [`BaseSettings`](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings") 在这种情况下，将保留现有的优先级）。如果 `priority` 参数是字符串，优先级名称将在 [`SETTINGS_PRIORITIES`](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES") . 否则，应提供特定的整数。

创建对象后，可以使用 [`set()`](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set") 方法，并且可以使用字典的方括号符号或 [`get()`](#scrapy.settings.BaseSettings.get "scrapy.settings.BaseSettings.get") 实例的方法及其值转换变量。请求存储的密钥时，将检索具有最高优先级的值。

```py
copy()
```

对当前设置进行深度复制。

此方法返回 [`Settings`](#scrapy.settings.Settings "scrapy.settings.Settings") 类，使用相同的值及其优先级填充。

对新对象的修改不会反映在原始设置上。

```py
copy_to_dict()
```

复制当前设置并转换为dict。

此方法返回一个新的dict，该dict使用与当前设置相同的值及其优先级填充。

对返回的dict的修改不会反映在原始设置上。

例如，此方法对于在Scrapy Shell中打印设置很有用。

```py
freeze()
```

禁用对当前设置的进一步更改。

调用此方法后，设置的当前状态将变为不可变。尝试通过 [`set()`](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set") 方法及其变体是不可能的，将被警告。

```py
frozencopy()
```

返回当前设置的不可变副本。

A的别名 [`freeze()`](#scrapy.settings.BaseSettings.freeze "scrapy.settings.BaseSettings.freeze") 调用返回的对象 [`copy()`](#scrapy.settings.BaseSettings.copy "scrapy.settings.BaseSettings.copy") .

```py
get(name, default=None)
```

在不影响其原始类型的情况下获取设置值。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **default** (_any_) -- 如果找不到设置，则返回的值

 |
| --- | --- |

```py
getbool(name, default=False)
```

获取设置值作为布尔值。

`1` ， `'1'` 是真的 `` and `` “真” `` return `` 对 [``](#id1), while `` 0 [``](#id3), `` “0” [``](#id5), `` 假 [``](#id7), `` “假” `` and `` 没有 `` return `` 假`。

例如，通过设置为的环境变量填充的设置 `'0'` 将返回 `False` 使用此方法时。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **default** (_any_) -- 如果找不到设置，则返回的值

 |
| --- | --- |

```py
getdict(name, default=None)
```

获取一个设置值作为字典。如果设置原始类型为字典，则返回其副本。如果它是一个字符串，它将作为JSON字典进行计算。如果它是一个 [`BaseSettings`](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings") 实例本身，它将被转换为一个字典，其中包含所有当前设置值，这些值将由返回 [`get()`](#scrapy.settings.BaseSettings.get "scrapy.settings.BaseSettings.get") 以及丢失有关优先级和可变性的所有信息。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **default** (_any_) -- 如果找不到设置，则返回的值

 |
| --- | --- |

```py
getfloat(name, default=0.0)
```

以浮点形式获取设置值。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **default** (_any_) -- 如果找不到设置，则返回的值

 |
| --- | --- |

```py
getint(name, default=0)
```

以int形式获取设置值。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **default** (_any_) -- 如果找不到设置，则返回的值

 |
| --- | --- |

```py
getlist(name, default=None)
```

以列表形式获取设置值。如果设置的原始类型是列表，则将返回其副本。如果是一个字符串，它将被“，”拆分。

例如，通过设置为的环境变量填充的设置 `'one,two'` 使用此方法时将返回一个列表['one'、'two']。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **default** (_any_) -- 如果找不到设置，则返回的值

 |
| --- | --- |

```py
getpriority(name)
```

返回设置的当前数字优先级值，或 `None` 如果给定 `name` 不存在。

| 参数: | **name** (_string_) -- 设置名称 |
| --- | --- |

```py
getwithbase(name)
```

获取类似字典的设置及其 &lt;cite&gt;_BASE&lt;/cite&gt; 对应的。

| 参数: | **name** (_string_) -- 类似字典的设置的名称 |
| --- | --- |

```py
maxpriority()
```

返回所有设置中存在的最高优先级的数值，或返回 `default` 从 [`SETTINGS_PRIORITIES`](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES") 如果没有存储设置。

```py
set(name, value, priority='project')
```

存储具有给定优先级的键/值属性。

应填充设置 _before_ 配置爬虫对象（通过 `configure()` 方法），否则它们不会有任何效果。

| 参数: | 

*   **name** (_string_) -- 设置名称
*   **value** (_any_) -- 要与设置关联的值
*   **priority** (_string_ _or_ _int_) -- 设置的优先级。应该是 [`SETTINGS_PRIORITIES`](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES") 或整数

 |
| --- | --- |

```py
setmodule(module, priority='project')
```

存储具有给定优先级的模块的设置。

这是一个调用 [`set()`](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set") 对于每个全局声明的大写变量 `module` 提供的 `priority` .

| 参数: | 

*   **module** (_module object_ _or_ _string_) -- 模块或模块路径
*   **priority** (_string_ _or_ _int_) -- 设置的优先级。应该是 [`SETTINGS_PRIORITIES`](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES") 或整数

 |
| --- | --- |

```py
update(values, priority='project')
```

存储具有给定优先级的键/值对。

这是一个调用 [`set()`](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set") 每一项 `values` 提供的 `priority` .

如果 `values` 是一个字符串，它被假定为JSON编码并被解析为一个dict `json.loads()` 第一。如果是 [`BaseSettings`](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings") 例如，每个键的优先级将被使用，并且 `priority` 参数被忽略。这允许使用单个命令插入/更新具有不同优先级的设置。

| 参数: | 

*   **values** (dict or string or [`BaseSettings`](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings")) -- 设置名称和值
*   **priority** (_string_ _or_ _int_) -- 设置的优先级。应该是 [`SETTINGS_PRIORITIES`](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES") 或整数

 |
| --- | --- |

## SpiderLoader API

```py
class scrapy.loader.SpiderLoader
```

这个类负责检索和处理整个项目中定义的 Spider 类。

通过在 [`SPIDER_LOADER_CLASS`](settings.html#std:setting-SPIDER_LOADER_CLASS) 项目设置。他们必须全面实施 `scrapy.interfaces.ISpiderLoader` 保证无误执行的接口。

```py
from_settings(settings)
```

Scrapy使用该类方法创建该类的实例。它使用当前的项目设置调用，并加载在 [`SPIDER_MODULES`](settings.html#std:setting-SPIDER_MODULES) 设置。

| 参数: | **settings** ([`Settings`](#scrapy.settings.Settings "scrapy.settings.Settings") instance) -- 项目设置 |
| --- | --- |

```py
load(spider_name)
```

获取具有给定名称的 Spider 类。它将在先前加载的spider中查找具有名称的spider类 `spider_name` 如果找不到，将引发keyerror。

| 参数: | **spider_name** (_str_) -- Spider 类名 |
| --- | --- |

```py
list()
```

获取项目中可用 Spider 的名称。

```py
find_by_request(request)
```

列出能够处理给定请求的 Spider 的名称。将尝试将请求的URL与 Spider 的域相匹配。

| 参数: | **request** ([`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") instance) -- 查询请求 |
| --- | --- |

## 信号API

## 统计收集器API

在 [`scrapy.statscollectors`](stats.html#module-scrapy.statscollectors "scrapy.statscollectors: Stats Collectors") 模块和它们都实现由 [`StatsCollector`](#scrapy.statscollectors.StatsCollector "scrapy.statscollectors.StatsCollector") 类（它们都从中继承）。

```py
class scrapy.statscollectors.StatsCollector
```

```py
get_value(key, default=None)
```

返回给定stats键的值，如果该键不存在，则返回默认值。

```py
get_stats()
```

以dict形式获取当前运行的spider的所有统计信息。

```py
set_value(key, value)
```

为给定的stats键设置给定值。

```py
set_stats(stats)
```

使用传入的dict重写当前状态 `stats` 争论。

```py
inc_value(key, count=1, start=0)
```

假定给定的起始值（未设置时），按给定的计数递增给定的stats键的值。

```py
max_value(key, value)
```

仅当同一个键的当前值小于值时，才为给定键设置给定值。如果给定键没有当前值，则始终设置该值。

```py
min_value(key, value)
```

仅当同一键的当前值大于值时，才为给定键设置给定值。如果给定键没有当前值，则始终设置该值。

```py
clear_stats()
```

清除所有统计。

以下方法不是stats集合API的一部分，而是在实现自定义stats收集器时使用的：

```py
open_spider(spider)
```

打开给定的 Spider 以收集统计信息。

```py
close_spider(spider)
```

关闭给定的 Spider 。调用之后，就不能访问或收集更多的特定统计信息。