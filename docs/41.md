# 下载器中间件

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

下载器中间件是Scrapy请求/响应处理的Hook框架。这是一个轻，低层次的系统，全球范围内改变斯拉皮的请求和响应。

## 激活下载器中间件

要激活下载器中间件组件，请将其添加到 [`DOWNLOADER_MIDDLEWARES`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES) 设置，这是一个dict，其键是中间件类路径，其值是中间件顺序。

举个例子：

```py
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.CustomDownloaderMiddleware': 543,
}

```

这个 [`DOWNLOADER_MIDDLEWARES`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES) 设置与合并 [`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE) 在scrappy中定义的设置（不打算被覆盖），然后按顺序排序，以获得已启用中间件的最终排序列表：第一个中间件更接近引擎，最后一个更接近下载程序。也就是说， [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") 每个中间件的方法将以增加的中间件顺序（100、200、300…）调用，并且 [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") 每个中间件的方法将按降序调用。

要决定分配给中间件的顺序，请参见 [`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE) 根据要插入中间件的位置设置和选择一个值。顺序很重要，因为每个中间件执行不同的操作，并且您的中间件可能依赖于之前（或之后）应用的一些中间件。

如果要禁用内置中间件（定义于 [`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE) 并在默认情况下启用）您必须在项目的 [`DOWNLOADER_MIDDLEWARES`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES) 设置和分配 `None` 作为其价值。例如，如果要禁用用户代理中间件：

```py
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.CustomDownloaderMiddleware': 543,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
}

```

最后，请记住，某些中间商可能需要通过特定设置启用。有关更多信息，请参阅每个中间件文档。

## 编写自己的下载中间件

每个下载器中间件都是一个python类，它定义了下面定义的一个或多个方法。

主要入口点是 `from_crawler` 类方法，它接收 [`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") 实例。这个 [`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") 例如，对象允许您访问 [settings](settings.html#topics-settings) .

```py
class scrapy.downloadermiddlewares.DownloaderMiddleware
```

注解

任何下载器中间件方法也可能返回延迟。

```py
process_request(request, spider)
```

对于通过下载中间件的每个请求调用此方法。

[`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") 应该是：返回 `None` 返回A [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") 对象，返回 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 对象，或提升 [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") .

如果它回来 `None` ，scrapy将继续处理此请求，执行所有其他中间软件，直到最后调用适当的下载器处理程序执行请求（及其下载的响应）。

如果它返回 [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") 对象，Scrapy不用调用了 _any_ 其他 [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") 或 [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 方法或适当的下载函数；它将返回该响应。这个 [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") 每次响应都会调用已安装中间件的方法。

如果它返回 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 对象，Scrapy将停止调用进程请求方法并重新安排返回的请求。一旦执行新返回的请求，将对下载的响应调用适当的中间件链。

如果它引发了 [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") 例外情况 [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 将调用已安装的下载器中间件的方法。如果它们都不处理异常，则请求的errback函数（ `Request.errback` ）。如果没有代码处理引发的异常，则忽略该异常，不记录该异常（与其他异常不同）。

| 参数: | 

*   **request** ([`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") object) -- 正在处理的请求
*   **spider** ([`Spider`](spiders.html#scrapy.spiders.Spider "scrapy.spiders.Spider") object) -- 此请求所针对的 Spider

 |
| --- | --- |

```py
process_response(request, response, spider)
```

[`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") 应该是：返回 [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") 对象，返回 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 反对或提高 [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") 例外。

如果它返回 [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") （可能是相同的给定响应，也可能是全新的响应），该响应将继续使用 [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") 链中的下一个中间件。

如果它返回 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 对象，中间件链将停止，返回的请求将被重新安排以便将来下载。这与从返回请求时的行为相同 [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") .

如果它引发了 [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") 异常，请求的errback函数（ `Request.errback` ）。如果没有代码处理引发的异常，则忽略该异常，不记录该异常（与其他异常不同）。

| 参数: | 

*   **request** (is a [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") object) -- 发起响应的请求
*   **response** ([`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") object) -- 正在处理的响应
*   **spider** ([`Spider`](spiders.html#scrapy.spiders.Spider "scrapy.spiders.Spider") object) -- 此响应所针对的 Spider

 |
| --- | --- |

```py
process_exception(request, exception, spider)
```

Scrapy电话 [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 当下载处理程序或 [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") （从下载器中间件）引发异常（包括 [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") 例外）

[`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 应该返回：或者 `None` ，A [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") 对象，或 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 对象。

如果它回来 `None` ，Scrapy将继续处理此异常，执行任何其他 [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 安装的中间件的方法，直到没有中间件，默认的异常处理开始。

如果它返回 [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") 对象 [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") 已安装中间件的方法链已启动，Scrapy不需要调用任何其他方法。 [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 中间件的方法。

如果它返回 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 对象，将重新安排返回的请求以便将来下载。这将停止执行 [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") 中间件的方法与返回响应相同。

| 参数: | 

*   **request** (is a [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") object) -- 生成异常的请求
*   **exception** (an `Exception` object) -- 引发的异常
*   **spider** ([`Spider`](spiders.html#scrapy.spiders.Spider "scrapy.spiders.Spider") object) -- 此请求所针对的 Spider

 |
| --- | --- |

```py
from_crawler(cls, crawler)
```

如果存在，则调用该类方法从 [`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") . 它必须返回中间件的新实例。爬虫对象提供对所有零碎核心组件（如设置和信号）的访问；它是中间件访问它们并将其功能连接到零碎的一种方式。

| 参数: | **crawler** ([`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") object) -- 使用此中间件的爬虫程序 |
| --- | --- |

## 内置下载器中间件参考

本页介绍了所有随Scrapy一起提供的下载器中间件组件。有关如何使用它们以及如何编写自己的下载器中间件的信息，请参见 [downloader middleware usage guide](#topics-downloader-middleware) .

有关默认启用的组件列表（及其顺序），请参见 [`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE) 设置。

### CookiesMiddleware

```py
class scrapy.downloadermiddlewares.cookies.CookiesMiddleware
```

此中间件允许使用需要cookie的站点，例如那些使用会话的站点。它跟踪Web服务器发送的cookie，并像Web浏览器一样，在随后的请求（来自该 Spider ）上发送它们。

以下设置可用于配置cookie中间件：

*   [`COOKIES_ENABLED`](#std:setting-COOKIES_ENABLED)
*   [`COOKIES_DEBUG`](#std:setting-COOKIES_DEBUG)

#### 每个 Spider 有多个cookie会话

0.15 新版功能.

通过使用 [`cookiejar`](#std:reqmeta-cookiejar) 请求元键。默认情况下，它使用一个cookie jar（会话），但您可以通过一个标识符来使用不同的标识符。

例如：：

```py
for i, url in enumerate(urls):
    yield scrapy.Request(url, meta={'cookiejar': i},
        callback=self.parse_page)

```

记住 [`cookiejar`](#std:reqmeta-cookiejar) meta-key不是“粘性的”。您需要在随后的请求中继续传递它。例如：：

```py
def parse_page(self, response):
    # do some processing
    return scrapy.Request("http://www.example.com/otherpage",
        meta={'cookiejar': response.meta['cookiejar']},
        callback=self.parse_other_page)

```

#### COOKIES_ENABLED

违约： `True`

是否启用cookie中间件。如果禁用，则不会向Web服务器发送cookie。

注意，尽管 [`COOKIES_ENABLED`](#std:setting-COOKIES_ENABLED) 设置中频 `Request.` [`meta['dont_merge_cookies']`](request-response.html#std:reqmeta-dont_merge_cookies) 评估为 `True` 请求cookies将 **not** 发送到Web服务器并在中接收cookie [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") 将 **not** 与现有cookie合并。

有关详细信息，请参阅 `cookies` 参数在 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") .

#### COOKIES_DEBUG

违约： `False`

如果启用，Scrapy将记录请求中发送的所有cookie（即 `Cookie` 标题）和响应中收到的所有cookie（即 `Set-Cookie` 标题）。

下面是一个使用 [`COOKIES_DEBUG`](#std:setting-COOKIES_DEBUG) 启用：：

```py
2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened
2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)
[...]

```

### DefaultHeadersMiddleware

```py
class scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware
```

此中间件设置在 [`DEFAULT_REQUEST_HEADERS`](settings.html#std:setting-DEFAULT_REQUEST_HEADERS) 设置。

### DownloadTimeoutMiddleware

```py
class scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware
```

此中间件为中指定的请求设置下载超时 [`DOWNLOAD_TIMEOUT`](settings.html#std:setting-DOWNLOAD_TIMEOUT) 设置或 `download_timeout` Spider 属性。

注解

您还可以使用设置每个请求的下载超时 [`download_timeout`](request-response.html#std:reqmeta-download_timeout) request.meta key；即使禁用downloadTimeoutMiddleware，也支持此功能。

### HttpAuthMiddleware

```py
class scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware
```

此中间件使用 [Basic access authentication](https://en.wikipedia.org/wiki/Basic_access_authentication) （又名。HTTP AUTH）。

要从某些spider启用HTTP身份验证，请设置 `http_user` 和 `http_pass` 这些 Spider 的属性。

例子：：

```py
from scrapy.spiders import CrawlSpider

class SomeIntranetSiteSpider(CrawlSpider):

    http_user = 'someuser'
    http_pass = 'somepass'
    name = 'intranet.example.com'

    # .. rest of the spider code omitted ...

```

### HttpCacheMiddleware

```py
class scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware
```

此中间件为所有HTTP请求和响应提供低级缓存。它必须与缓存存储后端以及缓存策略相结合。

带有三个HTTP缓存存储后端的废料船：

> *   [文件系统存储后端（默认）](#httpcache-storage-fs)
> *   [DBM存储后端](#httpcache-storage-dbm)
> *   [LEVELDB存储后端](#httpcache-storage-leveldb)

您可以使用 [`HTTPCACHE_STORAGE`](#std:setting-HTTPCACHE_STORAGE) 设置。或者您也可以实现自己的存储后端。

Scrapy附带两个HTTP缓存策略：

> *   [RCF2616政策](#httpcache-policy-rfc2616)
> *   [虚拟策略（默认）](#httpcache-policy-dummy)

可以使用更改HTTP缓存策略 [`HTTPCACHE_POLICY`](#std:setting-HTTPCACHE_POLICY) 设置。或者您也可以实现自己的策略。

您还可以避免在使用 [`dont_cache`](#std:reqmeta-dont_cache) 元密钥相等 `True` .

#### 虚拟策略（默认）

此策略不知道任何HTTP缓存控制指令。每个请求及其相应的响应都被缓存。当再次看到相同的请求时，将返回响应，而不从Internet传输任何内容。

虚拟策略对于更快地测试spider（而不必每次都等待下载）以及在无法连接到Internet时尝试离线使用spider非常有用。目标是能够“重播” Spider 的奔跑 和以前一样.

要使用此策略，请设置：

*   [`HTTPCACHE_POLICY`](#std:setting-HTTPCACHE_POLICY) to `scrapy.extensions.httpcache.DummyPolicy`

#### RCF2616政策

此策略提供了一个符合RFC2616的HTTP缓存，即具有HTTP缓存控制意识，旨在生产，并在连续运行中使用，以避免下载未修改的数据（以节省带宽和加快爬行速度）。

实施内容：

*   不要试图用存储响应/请求 `no-store` 缓存控制指令集

*   如果 `no-cache` 甚至为新响应设置了缓存控制指令

*   计算新鲜度寿命 `max-age` 缓存控制指令

*   计算新鲜度寿命 `Expires` 响应报头

*   计算新鲜度寿命 `Last-Modified` 响应头（firefox使用的启发式方法）

*   计算当前年龄 `Age` 响应报头

*   计算当前年龄 `Date` 页眉

*   基于以下内容重新验证过时响应 `Last-Modified` 响应报头

*   基于以下内容重新验证过时响应 `ETag` 响应报头

*   集合 `Date` 接收到的任何响应的头丢失了它

*   支持 `max-stale` 请求中的缓存控制指令

    这允许spider使用完整的rfc2616缓存策略进行配置，但避免按请求进行重新验证，同时保持与HTTP规范一致。

    例子：

    添加 `Cache-Control: max-stale=600` 请求头接受超过其过期时间不超过600秒的响应。

    另见：RFC2616，14.9.3

缺少的内容：

*   `Pragma: no-cache` 支持https://www.w3.org/protocols/rfc2616/rfc2616-sec14.html sec14.9.1
*   `Vary` 头支持https://www.w3.org/protocols/rfc2616/rfc2616-sec13.html sec13.6
*   Invalidation after updates or deletes [https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10](https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10)
*   …可能还有其他……

要使用此策略，请设置：

*   [`HTTPCACHE_POLICY`](#std:setting-HTTPCACHE_POLICY) to `scrapy.extensions.httpcache.RFC2616Policy`

#### 文件系统存储后端（默认）

文件系统存储后端可用于HTTP缓存中间件。

要使用此存储后端，请设置：

*   [`HTTPCACHE_STORAGE`](#std:setting-HTTPCACHE_STORAGE) to `scrapy.extensions.httpcache.FilesystemCacheStorage`

每个请求/响应对存储在包含以下文件的不同目录中：

> *   `request_body` -普通请求主体
> *   `request_headers` -请求头（原始HTTP格式）
> *   `response_body` -普通反应体
> *   `response_headers` -请求头（原始HTTP格式）
> *   `meta` -python中这个缓存资源的一些元数据 `repr()` 格式（grep友好格式）
> *   `pickled_meta` -中的相同元数据 `meta` 但是为了更有效的反序列化而进行的pickled

目录名是根据请求指纹生成的（请参见 `scrapy.utils.request.fingerprint` 一级子目录用于避免在同一目录中创建过多的文件（在许多文件系统中效率低下）。示例目录可以是：

```py
/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7

```

#### DBM存储后端

0.13 新版功能.

DBM_uu存储后端也可用于HTTP缓存中间件。

默认情况下，它使用anydbm_u模块，但您可以使用 [`HTTPCACHE_DBM_MODULE`](#std:setting-HTTPCACHE_DBM_MODULE) 设置。

要使用此存储后端，请设置：

*   [`HTTPCACHE_STORAGE`](#std:setting-HTTPCACHE_STORAGE) to `scrapy.extensions.httpcache.DbmCacheStorage`

#### LEVELDB存储后端

0.23 新版功能.

还为HTTP缓存中间件提供了一个级别数据库存储后端。

不建议将此后端用于开发，因为只有一个进程可以同时访问LEVELDB数据库，因此不能对同一个spider运行爬网并并行打开scrapy shell。

要使用此存储后端：

*   设置 [`HTTPCACHE_STORAGE`](#std:setting-HTTPCACHE_STORAGE) 到 `scrapy.extensions.httpcache.LeveldbCacheStorage`
*   安装 [LevelDB python bindings](https://pypi.python.org/pypi/leveldb) 喜欢 `pip install leveldb`

#### httpcache中间件设置

这个 [`HttpCacheMiddleware`](#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware") 可通过以下设置进行配置：

##### HTTPCACHE_ENABLED

0.11 新版功能.

违约： `False`

是否启用HTTP缓存。

在 0.11 版更改: 0.11之前， [`HTTPCACHE_DIR`](#std:setting-HTTPCACHE_DIR) 用于启用缓存。

##### HTTPCACHE_EXPIRATION_SECS

违约： `0`

缓存请求的过期时间（秒）。

超过此时间的缓存请求将被重新下载。如果为零，则缓存请求将永不过期。

在 0.11 版更改: 在0.11之前，零意味着缓存请求总是过期。

##### HTTPCACHE_DIR

违约： `'httpcache'`

用于存储（低级）HTTP缓存的目录。如果为空，则将禁用HTTP缓存。如果给定了相对路径，则相对于项目数据目录。有关详细信息，请参阅： [Scrapy 项目的默认结构](commands.html#topics-project-structure) .

##### HTTPCACHE_IGNORE_HTTP_CODES

0.10 新版功能.

违约： `[]`

不要用这些HTTP代码缓存响应。

##### HTTPCACHE_IGNORE_MISSING

违约： `False`

如果启用，在缓存中找不到的请求将被忽略，而不是下载。

##### HTTPCACHE_IGNORE_SCHEMES

0.10 新版功能.

违约： `['file']`

不要用这些URI方案缓存响应。

##### HTTPCACHE_STORAGE

违约： `'scrapy.extensions.httpcache.FilesystemCacheStorage'`

实现缓存存储后端的类。

##### HTTPCACHE_DBM_MODULE

0.13 新版功能.

违约： `'anydbm'`

要在中使用的数据库模块 [DBM storage backend](#httpcache-storage-dbm) . 此设置特定于DBM后端。

##### HTTPCACHE_POLICY

0.18 新版功能.

违约： `'scrapy.extensions.httpcache.DummyPolicy'`

实现缓存策略的类。

##### HTTPCACHE_GZIP

1.0 新版功能.

违约： `False`

如果启用，将使用gzip压缩所有缓存数据。此设置特定于文件系统后端。

##### HTTPCACHE_ALWAYS_STORE

1.1 新版功能.

违约： `False`

如果启用，将无条件缓存页。

Spider 可能希望缓存中有所有可用的响应，以便将来与一起使用 `Cache-Control: max-stale` 例如。dummypolicy缓存所有响应，但从不重新验证它们，有时需要更细微的策略。

此设置仍然尊重 `Cache-Control: no-store` 回应中的指示。如果你不想要，过滤 `no-store` 在您向缓存中间件提供的响应中超出了缓存控制头。

##### HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS

1.1 新版功能.

违约： `[]`

要忽略的响应中的缓存控制指令列表。

站点通常设置“无存储”、“无缓存”、“必须重新验证”等，但是如果 Spider 遵守这些指令，它会对 Spider 产生的流量感到不安。这允许有选择地忽略缓存控制指令，这些指令对于正在被爬网的站点来说是不重要的。

我们假设 Spider 不会在请求中发出缓存控制指令，除非它确实需要它们，所以请求中的指令不会被过滤。

### HttpCompressionMiddleware

```py
class scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware
```

此中间件允许从网站发送/接收压缩（gzip、deflate）流量。

此中间件还支持解码 [brotli-compressed](https://www.ietf.org/rfc/rfc7932.txt) 回答，提供 [brotlipy](https://pypi.python.org/pypi/brotlipy) 已安装。

#### httpcompression中间件设置

##### COMPRESSION_ENABLED

违约： `True`

是否启用压缩中间件。

### HttpProxyMiddleware

0.8 新版功能.

```py
class scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware
```

此中间件通过设置 `proxy` 元价值 [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request") 物体。

像Python标准库模块一样 [urllib](https://docs.python.org/2/library/urllib.html) 和 [urllib2](https://docs.python.org/2/library/urllib2.html) ，它遵循以下环境变量：

*   `http_proxy`
*   `https_proxy`
*   `no_proxy`

您也可以设置meta键 `proxy` 每个请求的值 `http://some_proxy_server:port` 或 `http://username:password@some_proxy_server:port` . 请记住，此值将优先于 `http_proxy` / `https_proxy` 环境变量，它也将忽略 `no_proxy` 环境变量。

### RedirectMiddleware

```py
class scrapy.downloadermiddlewares.redirect.RedirectMiddleware
```

此中间件根据响应状态处理请求的重定向。

请求通过的URL（在重定向时）可以在 `redirect_urls` [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") 关键。

每个重定向背后的原因 [`redirect_urls`](#std:reqmeta-redirect_urls) 可以在 `redirect_reasons` [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") 关键。例如： `[301, 302, 307, 'meta refresh']` .

原因的格式取决于处理相应重定向的中间件。例如， [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware") 以整数表示触发响应状态代码，而 [`MetaRefreshMiddleware`](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware") 总是使用 `'meta refresh'` 字符串作为原因。

这个 [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware") 可以通过以下设置进行配置（有关详细信息，请参阅设置文档）：

*   [`REDIRECT_ENABLED`](#std:setting-REDIRECT_ENABLED)
*   [`REDIRECT_MAX_TIMES`](settings.html#std:setting-REDIRECT_MAX_TIMES)

如果 [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") 有 `dont_redirect` key设置为true，该中间件将忽略该请求。

如果要处理 Spider 中的某些重定向状态代码，可以在 `handle_httpstatus_list` Spider 属性。

例如，如果您希望重定向中间件忽略301和302响应（并将它们传递给您的spider），可以这样做：

```py
class MySpider(CrawlSpider):
    handle_httpstatus_list = [301, 302]

```

这个 `handle_httpstatus_list` 关键 [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") 还可以用于指定每个请求允许哪些响应代码。您也可以设置meta键 `handle_httpstatus_all` 到 `True` 如果您想允许请求的任何响应代码。

#### 重定向中间件设置

##### REDIRECT_ENABLED

0.13 新版功能.

违约： `True`

是否启用重定向中间件。

##### REDIRECT_MAX_TIMES

违约： `20`

单个请求将遵循的最大重定向数。

### MetaRefreshMiddleware

```py
class scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware
```

此中间件处理基于meta-refresh html标记的请求重定向。

这个 [`MetaRefreshMiddleware`](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware") 可以通过以下设置进行配置（有关详细信息，请参阅设置文档）：

*   [`METAREFRESH_ENABLED`](#std:setting-METAREFRESH_ENABLED)
*   [`METAREFRESH_MAXDELAY`](#std:setting-METAREFRESH_MAXDELAY)

这个中间件服从 [`REDIRECT_MAX_TIMES`](settings.html#std:setting-REDIRECT_MAX_TIMES) 设置， [`dont_redirect`](#std:reqmeta-dont_redirect) ， [`redirect_urls`](#std:reqmeta-redirect_urls) 和 [`redirect_reasons`](#std:reqmeta-redirect_reasons) 按说明请求元键 [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware")

#### 元刷新中间件设置

##### METAREFRESH_ENABLED

0.17 新版功能.

违约： `True`

是否启用元刷新中间件。

##### METAREFRESH_MAXDELAY

违约： `100`

重定向后的最大元刷新延迟（秒）。有些站点使用meta-refresh重定向到会话过期的页面，因此我们将自动重定向限制为最大延迟。

### RetryMiddleware

```py
class scrapy.downloadermiddlewares.retry.RetryMiddleware
```

一种中间件，用于重试可能由临时问题（如连接超时或HTTP 500错误）引起的失败请求。

一旦爬行器完成对所有常规（非失败）页面的爬行，将在抓取过程中收集失败的页面，并在最后重新安排。

这个 [`RetryMiddleware`](#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware") 可以通过以下设置进行配置（有关详细信息，请参阅设置文档）：

*   [`RETRY_ENABLED`](#std:setting-RETRY_ENABLED)
*   [`RETRY_TIMES`](#std:setting-RETRY_TIMES)
*   [`RETRY_HTTP_CODES`](#std:setting-RETRY_HTTP_CODES)

如果 [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") 有 `dont_retry` key设置为true，该中间件将忽略该请求。

#### 重试IDdleware设置

##### RETRY_ENABLED

0.13 新版功能.

违约： `True`

是否启用重试中间件。

##### RETRY_TIMES

违约： `2`

除第一次下载外，还要重试的最大次数。

也可以使用指定每个请求的最大重试次数 [`max_retry_times`](request-response.html#std:reqmeta-max_retry_times) 属性 [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") . 初始化时， [`max_retry_times`](request-response.html#std:reqmeta-max_retry_times) 元键优先于 [`RETRY_TIMES`](#std:setting-RETRY_TIMES) 设置。

##### RETRY_HTTP_CODES

违约： `[500, 502, 503, 504, 522, 524, 408]`

要重试的HTTP响应代码。总是重试其他错误（DNS查找问题、连接丢失等）。

在某些情况下，您可能希望将400添加到 [`RETRY_HTTP_CODES`](#std:setting-RETRY_HTTP_CODES) 因为它是用于指示服务器过载的常见代码。默认情况下不包括它，因为HTTP规范这么说。

### RobotsTxtMiddleware

```py
class scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware
```

此中间件过滤掉robots.txt排除标准禁止的请求。

要确保scrapy尊重robots.txt，请确保启用中间件，并且 [`ROBOTSTXT_OBEY`](settings.html#std:setting-ROBOTSTXT_OBEY) 设置已启用。

如果 [`Request.meta`](request-response.html#scrapy.http.Request.meta "scrapy.http.Request.meta") 有 `dont_obey_robotstxt` 密钥设置为true，即使 [`ROBOTSTXT_OBEY`](settings.html#std:setting-ROBOTSTXT_OBEY) 启用。

### DownloaderStats

```py
class scrapy.downloadermiddlewares.stats.DownloaderStats
```

存储通过它的所有请求、响应和异常的统计信息的中间件。

要使用此中间件，必须启用 [`DOWNLOADER_STATS`](settings.html#std:setting-DOWNLOADER_STATS) 设置。

### UserAgentMiddleware

```py
class scrapy.downloadermiddlewares.useragent.UserAgentMiddleware
```

允许spider覆盖默认用户代理的中间件。

为了让spider重写默认的用户代理，其 `user_agent` 必须设置属性。

### AjaxCrawlMiddleware

```py
class scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware
```

Middleware that finds 'AJAX crawlable' page variants based on meta-fragment html tag. See [https://developers.google.com/webmasters/ajax-crawling/docs/getting-started](https://developers.google.com/webmasters/ajax-crawling/docs/getting-started) for more info.

注解

Scrapy查找“ajax可爬行”页面，查找类似 `'http://example.com/!#foo=bar'` 即使没有这个中间件。当URL不包含时，需要AjaxCrawlMiddleware `'!#'` . 这通常是“索引”或“主要”网站页面的情况。

#### AjaxCrawl中间件设置

##### AJAXCRAWL_ENABLED

0.21 新版功能.

违约： `False`

是否启用AjaxCrawl中间件。您可能希望启用它 [broad crawls](broad-crawls.html#topics-broad-crawls) .

#### httpproxymiddleware设置

##### HTTPPROXY_ENABLED

违约： `True`

是否启用 `HttpProxyMiddleware` .

##### HTTPPROXY_AUTH_ENCODING

违约： `"latin-1"`

上代理身份验证的默认编码 `HttpProxyMiddleware` .