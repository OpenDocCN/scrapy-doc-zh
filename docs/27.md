# 常见问题

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

## Scrapy与BeautifulSoup或LXML相比如何？

[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) 和 [lxml](http://lxml.de/) 是用于分析HTML和XML的库。Scrapy是一个应用程序框架，用于编写爬行网站并从中提取数据的网络 Spider 。

Scrapy提供了一个用于提取数据的内置机制（称为 [selectors](topics/selectors.html#topics-selectors) ）但是你可以很容易地使用 [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) （或） [lxml](http://lxml.de/) ）相反，如果你觉得和他们一起工作更舒服的话。毕竟，它们只是解析可以从任何Python代码导入和使用的库。

换句话说，比较 [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) （或） [lxml](http://lxml.de/) ）剪贴就像比较 [jinja2](http://jinja.pocoo.org/) 到 [Django](https://www.djangoproject.com/) .

## 我可以用 Scrapy 和 BeautifulSoup 吗？

是的，你可以。如上所述 [above](#faq-scrapy-bs-cmp) ， [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) 可用于分析Scrapy回调中的HTML响应。你只需要把反应的身体 `BeautifulSoup` 对象并从中提取所需的任何数据。

下面是一个使用BeautifulSoupAPI的 Spider 示例， `lxml` 作为HTML解析器：

```py
from bs4 import BeautifulSoup
import scrapy

class ExampleSpider(scrapy.Spider):
    name = "example"
    allowed_domains = ["example.com"]
    start_urls = (
        'http://www.example.com/',
    )

    def parse(self, response):
        # use lxml to get decent HTML parsing speed
        soup = BeautifulSoup(response.text, 'lxml')
        yield {
            "url": response.url,
            "title": soup.h1.string
        }

```

注解

`BeautifulSoup` 支持多个HTML/XML分析器。见 [BeautifulSoup's official documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) 哪些是可用的。

## Scrapy支持哪些Python版本？

scrappy在cpython（默认的python实现）和pypy（从pypy 5.9开始）下支持python 2.7和python 3.4+。python 2.6支持从scrapy 0.20开始被丢弃。Scrapy1.1中添加了python 3支持。在Scrapy 1.4中添加了Pypy支持，在Scrapy 1.5中添加了Pypy3支持。

注解

对于Windows上的python 3支持，建议使用anaconda/miniconda作为 [outlined in the installation guide](intro/install.html#intro-install-windows) .

## Scrapy 有没有从 Django“steal”X？

可能吧，但我们不喜欢这个词。我们认为django_u是一个伟大的开源项目，也是一个可以效仿的例子，所以我们把它作为scrappy的灵感来源。

我们相信，如果某件事已经做得很好，就没有必要再去改造它。这个概念，除了作为开源和自由软件的基础之一，不仅适用于软件，也适用于文档、程序、政策等。因此，我们选择从已经正确解决了这些问题的项目中复制想法，而不是自己解决每个问题，而是关注我们需要解决的实际问题。奥尔维。

如果Scrapy能为其他项目提供灵感，我们会感到骄傲。随时从我们这里偷东西！

## Scrapy与HTTP代理一起工作吗？

对。通过HTTP代理下载器中间件提供对HTTP代理的支持（因为scrapy 0.8）。见 [`HttpProxyMiddleware`](topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware") .

## 如何在不同的页面中抓取具有属性的项目？

见 [向回调函数传递附加数据](topics/request-response.html#topics-request-response-ref-request-callback-arguments) .

## scrapy崩溃：importError:没有名为win32api的模块

您需要安装 [pywin32](https://sourceforge.net/projects/pywin32/) 因为 [this Twisted bug](https://twistedmatrix.com/trac/ticket/3707) .

## 如何在 Spider 中模拟用户登录？

见 [使用formRequest.from_response（）模拟用户登录](topics/request-response.html#topics-request-response-ref-request-userlogin) .

## 是宽度优先还是深度优先？

默认情况下，Scrapy使用 [LIFO](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)) 用于存储挂起请求的队列，这基本上意味着它会爬入 [DFO order](https://en.wikipedia.org/wiki/Depth-first_search) . 这种订单在大多数情况下更方便。如果你真的想爬进去 [BFO order](https://en.wikipedia.org/wiki/Breadth-first_search) ，您可以通过设置以下设置来完成此操作：

```py
DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'
SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'

```

## 我可怜的爬虫有记忆漏洞。我能做什么？

见 [调试内存泄漏](topics/leaks.html#topics-leaks) .

另外，python还有一个内置的内存泄漏问题，如中所述。 [无泄漏泄漏](topics/leaks.html#topics-leaks-without-leaks) .

## 我怎么能让 Scrapy 消耗更少的记忆？

请参阅前面的问题。

## 我可以在spider中使用基本的HTTP身份验证吗？

是的，看到了 [`HttpAuthMiddleware`](topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware") .

## 为什么Scrapy用英语而不是我的母语下载页面？

尝试更改默认值 [Accept-Language](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4) request header by overriding the [`DEFAULT_REQUEST_HEADERS`](topics/settings.html#std:setting-DEFAULT_REQUEST_HEADERS) 设置。

## 我在哪里可以找到一些零碎项目的例子？

见 [实例](intro/examples.html#intro-examples) .

## 我可以在不创建项目的情况下运行 Spider 吗？

对。你可以使用 [`runspider`](topics/commands.html#std:command-runspider) 命令。例如，如果有一个 Spider 用 `my_spider.py` 您可以用以下方式运行它的文件：

```py
scrapy runspider my_spider.py

```

见 [`runspider`](topics/commands.html#std:command-runspider) 命令获取更多信息。

## 我收到“过滤的异地请求”消息。我怎么修理它们？

这些信息（记录 `DEBUG` 级别）不一定意味着有问题，因此您可能不需要修复它们。

这些消息由非现场 Spider 中间件抛出，这是一个 Spider 中间件（默认情况下启用），其目的是过滤掉对 Spider 所覆盖域之外的域的请求。

有关详细信息，请参阅： [`OffsiteMiddleware`](topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware "scrapy.spidermiddlewares.offsite.OffsiteMiddleware") .

## 在生产中，建议采用什么方式部署 Scrapy 履带？

见 [部署 Spider](topics/deploy.html#topics-deploy) .

## 我可以使用JSON进行大型出口吗？

这取决于你的产出有多大。见 [this warning](topics/exporters.html#json-with-large-data) 在里面 [`JsonItemExporter`](topics/exporters.html#scrapy.exporters.JsonItemExporter "scrapy.exporters.JsonItemExporter") 文档。

## 我可以从信号处理程序返回（扭曲）延迟吗？

一些信号支持从其处理程序返回延迟，而另一些则不支持。请参见 [内置信号参考](topics/signals.html#topics-signals-ref) 去知道哪些。

## 响应状态代码999是什么意思？

999是雅虎网站用来限制请求的自定义响应状态代码。尝试使用下载延迟来降低爬行速度 `2` （或更高）在你的 Spider ：

```py
class MySpider(CrawlSpider):

    name = 'myspider'

    download_delay = 2

    # [ ... rest of the spider code ... ]

```

或者通过在项目中设置全局下载延迟 [`DOWNLOAD_DELAY`](topics/settings.html#std:setting-DOWNLOAD_DELAY) 设置。

## 我可以调用吗？ `pdb.set_trace()` 从我的 Spider 身上调试它们？

是的，但是您也可以使用scriby shell，它允许您快速分析（甚至修改）您的spider正在处理的响应，这通常比普通的老版本更有用。 `pdb.set_trace()` .

有关详细信息，请参阅 [从spiders调用shell来检查响应](topics/shell.html#topics-shell-inspect-response) .

## 最简单的方法是将我的所有抓取项转储到json/csv/xml文件中？

要转储到JSON文件，请执行以下操作：

```py
scrapy crawl myspider -o items.json

```

要转储到csv文件，请执行以下操作：

```py
scrapy crawl myspider -o items.csv

```

要转储到XML文件，请执行以下操作：

```py
scrapy crawl myspider -o items.xml

```

有关详细信息，请参阅 [Feed 导出](topics/feed-exports.html#topics-feed-exports)

## 这个巨大的秘密是什么？ `__VIEWSTATE` 某些形式中使用的参数？

这个 `__VIEWSTATE` 参数用于使用ASP.NET/VB.NET生成的网站。有关其工作方式的详细信息，请参见 [this page](http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm) . 还有，这里有一个 [example spider](https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py) 会刮伤其中一个站点。

## 解析大型XML/CSV数据源的最佳方法是什么？

使用xpath选择器解析大型提要可能会有问题，因为它们需要在内存中构建整个提要的DOM，这可能会非常慢，并且会消耗大量内存。

为了避免在内存中一次分析所有提要，可以使用函数 `xmliter` 和 `csviter` 从 `scrapy.utils.iterators` 模块。事实上，这就是食性 Spider （参见 [Spider](topics/spiders.html#topics-spiders) ）在盖下使用。

## Scrapy是否自动管理cookies？

是的，Scrapy接收并跟踪服务器发送的cookie，并像任何普通的Web浏览器一样，在随后的请求中发送它们。

有关详细信息，请参阅 [请求和响应](topics/request-response.html#topics-request-response) 和 [CookiesMiddleware](topics/downloader-middleware.html#cookies-mw) .

## 我如何才能看到从Scrapy发送和接收的cookies？

启用 [`COOKIES_DEBUG`](topics/downloader-middleware.html#std:setting-COOKIES_DEBUG) 设置。

## 我怎样才能指示 Spider 停止自己呢？

提高 [`CloseSpider`](topics/exceptions.html#scrapy.exceptions.CloseSpider "scrapy.exceptions.CloseSpider") 回调异常。有关详细信息，请参阅： [`CloseSpider`](topics/exceptions.html#scrapy.exceptions.CloseSpider "scrapy.exceptions.CloseSpider") .

## 我怎样才能防止我的废机器人被禁止？

见 [避免被禁止](topics/practices.html#bans) .

## 我应该使用 Spider 参数或设置来配置我的 Spider 吗？

两个 [spider arguments](topics/spiders.html#spiderargs) 和 [settings](topics/settings.html#topics-settings) 可以用来配置 Spider 。没有strict的规则要求使用其中一个或另一个，但是设置更适合于参数，一旦设置，就不会改变太多，而spider参数的更改更频繁，甚至在每次spider运行时，有时甚至需要spider运行（例如，设置spider的起始URL）。

举个例子来说明，假设您有一个 Spider 需要登录到一个站点来获取数据，并且您只想从站点的某个部分（每次都不同）获取数据。在这种情况下，登录的凭证将是设置，而要擦除的部分的URL将是spider参数。

## 我正在抓取一个XML文档，而我的xpath选择器没有返回任何项

可能需要删除命名空间。见 [正在删除命名空间](topics/selectors.html#removing-namespaces) .