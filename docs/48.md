# 发行说明

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

## Scrapy 1.6.0（2019-01-30）

亮点：

*   更好的Windows支持；
*   python 3.7兼容性；
*   大的文档改进，包括从 `.extract_first()` + `.extract()` API到 `.get()` + `.getall()` 应用程序编程接口；
*   Feed 导出、文件管道和媒体管道改进；
*   更好的扩展性： [`item_error`](topics/signals.html#std:signal-item_error) 和 [`request_reached_downloader`](topics/signals.html#std:signal-request_reached_downloader) 信号； `from_crawler` 支持Feed 导出、Feed 仓库和双过滤器。
*   `scrapy.contracts` 修复和新功能；
*   Telnet控制台安全性改进，首次作为后端发布于 [Scrapy 1.5.2（2019-01-22）](#release-1-5-2) ；
*   清理弃用的代码；
*   各种错误修复、小的新特性和整个代码库的可用性改进。

### 选择器API更改

虽然这些不是scrapy本身的更改，而是scrapy用于xpath/css选择器的parsel_u库中的更改，但这些更改在这里值得一提。Scrapy现在依赖于parsel&gt;=1.5，并且Scrapy文档会更新以跟踪最近的 `parsel` API惯例。

最明显的变化是 `.get()` 和 `.getall()` 选择器方法现在比 `.extract_first()` 和 `.extract()` . 我们认为这些新方法会产生更简洁和可读的代码。见 [extract（）和extract_first（）。](topics/selectors.html#old-extraction-api) 了解更多详细信息。

注解

目前有 **no plans** 贬低 `.extract()` 和 `.extract_first()` 方法。

另一个有用的新特性是 `Selector.attrib` 和 `SelectorList.attrib` 属性，这使得获取HTML元素的属性更加容易。见 [选择元素属性](topics/selectors.html#selecting-attributes) .

CSS选择器缓存在parsel&gt;=1.5中，这使得在多次使用相同的css路径时更快。这是非常常见的情况下，剪贴 Spider ：回调通常被称为多次，在不同的网页。

如果使用自定义 `Selector` 或 `SelectorList` 子类 **backward incompatible** Parsel中的更改可能会影响代码。见 [parsel changelog](https://parsel.readthedocs.io/en/latest/history.html) 详细描述，以及完整的改进列表。

### Telnet控制台

向后不兼容: Scrapy的telnet控制台现在需要用户名和密码。见 [远程登录控制台](topics/telnetconsole.html#topics-telnetconsole) 了解更多详细信息。此更改修复了 安全问题; 看见 [Scrapy 1.5.2（2019-01-22）](#release-1-5-2) 发布详细说明。

### 新的可扩展性功能

*   `from_crawler` 对Feed 导出和Feed仓库增加了支持。除此之外，它还允许从自定义饲料仓库和出口商访问零碎设置。（ [issue 1605](https://github.com/scrapy/scrapy/issues/1605) ， [issue 3348](https://github.com/scrapy/scrapy/issues/3348) ）
*   `from_crawler` 对双过滤器增加了支持（ [issue 2956](https://github.com/scrapy/scrapy/issues/2956) ）；这允许从双面打印器访问设置或 Spider 。
*   [`item_error`](topics/signals.html#std:signal-item_error) 在管道中发生错误时激发（ [issue 3256](https://github.com/scrapy/scrapy/issues/3256) ；
*   [`request_reached_downloader`](topics/signals.html#std:signal-request_reached_downloader) 当下载程序收到新请求时激发；此信号可能有用，例如，对于自定义计划程序有用（ [issue 3393](https://github.com/scrapy/scrapy/issues/3393) ）
*   新建SiteMapSpider [`sitemap_filter()`](topics/spiders.html#scrapy.spiders.SitemapSpider.sitemap_filter "scrapy.spiders.SitemapSpider.sitemap_filter") 方法，该方法允许根据SiteMapSpider子类中的属性选择站点地图条目（ [issue 3512](https://github.com/scrapy/scrapy/issues/3512) ）
*   下载程序处理程序的延迟加载现在是可选的；这使得在自定义下载程序处理程序中能够更好地处理初始化错误。（ [issue 3394](https://github.com/scrapy/scrapy/issues/3394) ）

### 新的文件管道和媒体管道功能

*   显示s3filestore的更多选项： [`AWS_ENDPOINT_URL`](topics/settings.html#std:setting-AWS_ENDPOINT_URL) ， [`AWS_USE_SSL`](topics/settings.html#std:setting-AWS_USE_SSL) ， [`AWS_VERIFY`](topics/settings.html#std:setting-AWS_VERIFY) ， [`AWS_REGION_NAME`](topics/settings.html#std:setting-AWS_REGION_NAME) . 例如，这允许使用可选的或自托管的与AWS兼容的提供程序（ [issue 2609](https://github.com/scrapy/scrapy/issues/2609) ， [issue 3548](https://github.com/scrapy/scrapy/issues/3548) ）
*   对谷歌云存储的ACL支持： [`FILES_STORE_GCS_ACL`](topics/media-pipeline.html#std:setting-FILES_STORE_GCS_ACL) 和 [`IMAGES_STORE_GCS_ACL`](topics/media-pipeline.html#std:setting-IMAGES_STORE_GCS_ACL) （ [issue 3199](https://github.com/scrapy/scrapy/issues/3199) ）

### `scrapy.contracts` 改进

*   更好地处理合同代码中的异常（ [issue 3377](https://github.com/scrapy/scrapy/issues/3377) ；
*   `dont_filter=True` 用于合同请求，该请求允许使用相同的URL测试不同的回调（ [issue 3381](https://github.com/scrapy/scrapy/issues/3381) ；
*   `request_cls` 合同子类中的属性允许在合同中使用不同的请求类，例如FormRequest（ [issue 3383](https://github.com/scrapy/scrapy/issues/3383) ）
*   合同中的固定errback处理，例如，对于为返回非200响应的URL执行合同的情况（ [issue 3371](https://github.com/scrapy/scrapy/issues/3371) ）

### 可用性改进

*   robotstxtmiddleware的更多统计信息（ [issue 3100](https://github.com/scrapy/scrapy/issues/3100) ）
*   信息日志级别用于显示telnet主机/端口（ [issue 3115](https://github.com/scrapy/scrapy/issues/3115) ）
*   在robotstxtmiddleware中将消息添加到ignorerequest（ [issue 3113](https://github.com/scrapy/scrapy/issues/3113) ）
*   更好地验证 `url` 论点 `Response.follow` （ [issue 3131](https://github.com/scrapy/scrapy/issues/3131) ）
*   spider初始化出错时，从scrapy命令返回非零退出代码（ [issue 3226](https://github.com/scrapy/scrapy/issues/3226) ）
*   链接提取改进：“ftp”添加到方案列表中（ [issue 3152](https://github.com/scrapy/scrapy/issues/3152) ）将“flv”添加到常用视频扩展（ [issue 3165](https://github.com/scrapy/scrapy/issues/3165) ）
*   禁用导出程序时出现更好的错误消息（ [issue 3358](https://github.com/scrapy/scrapy/issues/3358) ；
*   `scrapy shell --help` 提到本地文件所需的语法（ `./file.html` - [issue 3496](https://github.com/scrapy/scrapy/issues/3496) .
*   referer头值添加到rfpdupefilter日志消息中（ [issue 3588](https://github.com/scrapy/scrapy/issues/3588) ）

### 错误修复

*   修复了Windows下.csv导出中多余空行的问题（ [issue 3039](https://github.com/scrapy/scrapy/issues/3039) ；
*   在为磁盘队列序列化对象时正确处理python 3中的picking错误（ [issue 3082](https://github.com/scrapy/scrapy/issues/3082) ）
*   复制请求时标志现在被保留（ [issue 3342](https://github.com/scrapy/scrapy/issues/3342) ；
*   formRequest.from_response clickdata不应忽略带有 `input[type=image]` （ [issue 3153](https://github.com/scrapy/scrapy/issues/3153) ）
*   formRequest.from响应应保留重复的密钥（ [issue 3247](https://github.com/scrapy/scrapy/issues/3247) ）

### 文档改进

*   重新编写文档是为了建议.get/.getall API而不是.extract/.extract_。也， [选择器](topics/selectors.html#topics-selectors) 文档被更新并重新构造以匹配最新的Parsel文档；它们现在包含更多的主题，例如 [选择元素属性](topics/selectors.html#selecting-attributes) 或 [CSS选择器的扩展](topics/selectors.html#topics-selectors-css-extensions) （ [issue 3390](https://github.com/scrapy/scrapy/issues/3390) ）
*   [使用浏览器的开发人员工具进行抓取](topics/developer-tools.html#topics-developer-tools) 是一个新的教程，它取代了旧的火狐和Firebug教程（ [issue 3400](https://github.com/scrapy/scrapy/issues/3400) ）
*   Scrapy_项目环境变量记录在案（ [issue 3518](https://github.com/scrapy/scrapy/issues/3518) ；
*   安装说明中添加了故障排除部分（ [issue 3517](https://github.com/scrapy/scrapy/issues/3517) ；
*   改进了教程中初学者资源的链接（ [issue 3367](https://github.com/scrapy/scrapy/issues/3367) ， [issue 3468](https://github.com/scrapy/scrapy/issues/3468) ；
*   固定的 [`RETRY_HTTP_CODES`](topics/downloader-middleware.html#std:setting-RETRY_HTTP_CODES) 文档中的默认值（ [issue 3335](https://github.com/scrapy/scrapy/issues/3335) ；
*   移除未使用的素材 `DEPTH_STATS` 文档选项（ [issue 3245](https://github.com/scrapy/scrapy/issues/3245) ；
*   其他清理（ [issue 3347](https://github.com/scrapy/scrapy/issues/3347) ， [issue 3350](https://github.com/scrapy/scrapy/issues/3350) ， [issue 3445](https://github.com/scrapy/scrapy/issues/3445) ， [issue 3544](https://github.com/scrapy/scrapy/issues/3544) ， [issue 3605](https://github.com/scrapy/scrapy/issues/3605) ）

### 折旧清除

1.0以前版本的 Scrapy 模块名称的兼容性垫片已移除（ [issue 3318](https://github.com/scrapy/scrapy/issues/3318) ）：

*   `scrapy.command`
*   `scrapy.contrib` （所有子模块）
*   `scrapy.contrib_exp` （所有子模块）
*   `scrapy.dupefilter`
*   `scrapy.linkextractor`
*   `scrapy.project`
*   `scrapy.spider`
*   `scrapy.spidermanager`
*   `scrapy.squeue`
*   `scrapy.stats`
*   `scrapy.statscol`
*   `scrapy.utils.decorator`

见 [模块重新定位](#module-relocations) 有关详细信息，或使用Scrapy 1.5.x Deprecation Warnings中的建议更新代码。

其他折旧移除：

*   已删除不推荐使用的scrapy.interfaces.ispIderManager；请使用scrapy.interfaces.ispIderLoader。
*   已弃用 `CrawlerSettings` 类已删除（ [issue 3327](https://github.com/scrapy/scrapy/issues/3327) ）
*   已弃用 `Settings.overrides` 和 `Settings.defaults` 属性被删除（ [issue 3327](https://github.com/scrapy/scrapy/issues/3327) ， [issue 3359](https://github.com/scrapy/scrapy/issues/3359) ）

### 其他改进、清理

*   所有碎片测试现在都在Windows上通过；碎片测试套件在CI上的Windows环境中执行（ [issue 3315](https://github.com/scrapy/scrapy/issues/3315) ）
*   python 3.7支持（ [issue 3326](https://github.com/scrapy/scrapy/issues/3326) ， [issue 3150](https://github.com/scrapy/scrapy/issues/3150) ， [issue 3547](https://github.com/scrapy/scrapy/issues/3547) ）
*   测试和CI修复（ [issue 3526](https://github.com/scrapy/scrapy/issues/3526) ， [issue 3538](https://github.com/scrapy/scrapy/issues/3538) ， [issue 3308](https://github.com/scrapy/scrapy/issues/3308) ， [issue 3311](https://github.com/scrapy/scrapy/issues/3311) ， [issue 3309](https://github.com/scrapy/scrapy/issues/3309) ， [issue 3305](https://github.com/scrapy/scrapy/issues/3305) ， [issue 3210](https://github.com/scrapy/scrapy/issues/3210) ， [issue 3299](https://github.com/scrapy/scrapy/issues/3299) ）
*   `scrapy.http.cookies.CookieJar.clear` 接受“域”、“路径”和“名称”可选参数（ [issue 3231](https://github.com/scrapy/scrapy/issues/3231) ）
*   附加文件包含在SDIST中（ [issue 3495](https://github.com/scrapy/scrapy/issues/3495) ；
*   代码样式修复（ [issue 3405](https://github.com/scrapy/scrapy/issues/3405) ， [issue 3304](https://github.com/scrapy/scrapy/issues/3304) ；
*   已删除不需要的.strip（）调用（ [issue 3519](https://github.com/scrapy/scrapy/issues/3519) ；
*   collections.deque用于存储MiddleWarManager方法，而不是列表（ [issue 3476](https://github.com/scrapy/scrapy/issues/3476) ）

## Scrapy 1.5.2（2019-01-22）

*   安全修补程序: telnet控制台扩展可以很容易地被发布内容到http://localhost:6023的流氓网站利用，我们还没有找到从scrappy利用它的方法，但是很容易欺骗浏览器这样做，并提高了本地开发环境的风险。

    修复程序向后不兼容, 默认情况下，它使用随机生成的密码启用telnet用户密码验证。如果您不能立即升级，请考虑设置 `TELNET_CONSOLE_PORT` 超出其默认值。

    见 [telnet console](topics/telnetconsole.html#topics-telnetconsole) 有关详细信息的文档

*   由于boto导入错误，gce环境下的backport ci build失败。

## Scrapy 1.5.1（2018-07-12）

这是一个包含重要错误修复的维护版本，但没有新功能：

*   `O(N^2)` 解决了影响python 3和pypy的gzip解压问题（ [issue 3281](https://github.com/scrapy/scrapy/issues/3281) ；
*   改进了对TLS验证错误的跳过（ [issue 3166](https://github.com/scrapy/scrapy/issues/3166) ；
*   ctrl-c处理在python 3.5中是固定的+（ [issue 3096](https://github.com/scrapy/scrapy/issues/3096) ；
*   测试修复 [issue 3092](https://github.com/scrapy/scrapy/issues/3092) ， [issue 3263](https://github.com/scrapy/scrapy/issues/3263) ；
*   文档改进（ [issue 3058](https://github.com/scrapy/scrapy/issues/3058) ， [issue 3059](https://github.com/scrapy/scrapy/issues/3059) ， [issue 3089](https://github.com/scrapy/scrapy/issues/3089) ， [issue 3123](https://github.com/scrapy/scrapy/issues/3123) ， [issue 3127](https://github.com/scrapy/scrapy/issues/3127) ， [issue 3189](https://github.com/scrapy/scrapy/issues/3189) ， [issue 3224](https://github.com/scrapy/scrapy/issues/3224) ， [issue 3280](https://github.com/scrapy/scrapy/issues/3280) ， [issue 3279](https://github.com/scrapy/scrapy/issues/3279) ， [issue 3201](https://github.com/scrapy/scrapy/issues/3201) ， [issue 3260](https://github.com/scrapy/scrapy/issues/3260) ， [issue 3284](https://github.com/scrapy/scrapy/issues/3284) ， [issue 3298](https://github.com/scrapy/scrapy/issues/3298) ， [issue 3294](https://github.com/scrapy/scrapy/issues/3294) ）

## Scrapy 1.5.0（2017-12-29）

这个版本在代码库中带来了一些新的小特性和改进。一些亮点：

*   文件管道和ImageSpipeline支持Google云存储。
*   随着到代理的连接现在可以重用，使用代理服务器进行爬行变得更加高效。
*   对警告、异常和日志消息进行了改进，使调试更加容易。
*   `scrapy parse` 命令现在允许通过 `--meta` 争论。
*   与python 3.6、pypy和pypy3的兼容性得到了改进；通过在CI上运行测试，pypy和pypy3现在得到了官方支持。
*   更好地默认处理HTTP 308、522和524状态代码。
*   像往常一样，文档得到了改进。

### 向后不兼容的更改

*   Scrapy1.5放弃了对python 3.3的支持。
*   默认的scrapy用户代理现在使用https链接到scrapy.org（ [issue 2983](https://github.com/scrapy/scrapy/issues/2983) ） 这在技术上是向后不兼容的; 覆盖 [`USER_AGENT`](topics/settings.html#std:setting-USER_AGENT) 如果你依赖旧的价值观。
*   记录被覆盖的设置 `custom_settings` 是固定的； **this is technically backward-incompatible** 因为记录器从 `[scrapy.utils.log]` 到 `[scrapy.crawler]` . 如果您正在分析垃圾日志，请更新日志分析器（ [issue 1343](https://github.com/scrapy/scrapy/issues/1343) ）
*   Linkextractor现在忽略 `m4v` 默认情况下，这是行为的更改。
*   522和524状态代码添加到 `RETRY_HTTP_CODES` （ [issue 2851](https://github.com/scrapy/scrapy/issues/2851) ）

### 新特点

*   支持 `&lt;link&gt;` 标签在 `Response.follow` （ [issue 2785](https://github.com/scrapy/scrapy/issues/2785) ）
*   支持 `ptpython` 雷普尔 [issue 2654](https://github.com/scrapy/scrapy/issues/2654) ）
*   Google云存储支持文件管道和图像管道（ [issue 2923](https://github.com/scrapy/scrapy/issues/2923) ）
*   新的 `--meta` “scrapy parse”命令的选项允许传递附加请求。（ [issue 2883](https://github.com/scrapy/scrapy/issues/2883) ）
*   使用时填充spider变量 `shell.inspect_response` （ [issue 2812](https://github.com/scrapy/scrapy/issues/2812) ）
*   处理HTTP 308永久重定向（ [issue 2844](https://github.com/scrapy/scrapy/issues/2844) ）
*   将522和524添加到 `RETRY_HTTP_CODES` （ [issue 2851](https://github.com/scrapy/scrapy/issues/2851) ）
*   启动时记录版本信息（ [issue 2857](https://github.com/scrapy/scrapy/issues/2857) ）
*   `scrapy.mail.MailSender` 现在在python 3中工作（它需要Twisted17.9.0）
*   重新使用与代理服务器的连接（ [issue 2743](https://github.com/scrapy/scrapy/issues/2743) ）
*   为下载器中间件添加模板（ [issue 2755](https://github.com/scrapy/scrapy/issues/2755) ）
*   未定义分析回调时NotImplementedError的显式消息（ [issue 2831](https://github.com/scrapy/scrapy/issues/2831) ）
*   CrawlerProcess有一个选项可以禁用安装根日志处理程序（ [issue 2921](https://github.com/scrapy/scrapy/issues/2921) ）
*   Linkextractor现在忽略 `m4v` 默认情况下的扩展
*   更好地记录响应消息 [`DOWNLOAD_WARNSIZE`](topics/settings.html#std:setting-DOWNLOAD_WARNSIZE) 和 [`DOWNLOAD_MAXSIZE`](topics/settings.html#std:setting-DOWNLOAD_MAXSIZE) 限制（限制） [issue 2927](https://github.com/scrapy/scrapy/issues/2927) ）
*   当URL被放入时显示警告 `Spider.allowed_domains` 而不是域（ [issue 2250](https://github.com/scrapy/scrapy/issues/2250) ）

### 错误修复

*   修复由重写的设置的日志记录 `custom_settings` ； **this is technically backward-incompatible** 因为记录器从 `[scrapy.utils.log]` 到 `[scrapy.crawler]` ，因此如果需要，请更新日志分析器（ [issue 1343](https://github.com/scrapy/scrapy/issues/1343) ）
*   默认的scrapy用户代理现在使用https链接到scrapy.org（ [issue 2983](https://github.com/scrapy/scrapy/issues/2983) ） 这在技术上是向后不兼容的; 覆盖 [`USER_AGENT`](topics/settings.html#std:setting-USER_AGENT) 如果你依赖旧的价值观。
*   修复pypy和pypy3测试失败，正式支持它们（ [issue 2793](https://github.com/scrapy/scrapy/issues/2793) ， [issue 2935](https://github.com/scrapy/scrapy/issues/2935) ， [issue 2990](https://github.com/scrapy/scrapy/issues/2990) ， [issue 3050](https://github.com/scrapy/scrapy/issues/3050) ， [issue 2213](https://github.com/scrapy/scrapy/issues/2213) ， [issue 3048](https://github.com/scrapy/scrapy/issues/3048) ）
*   在下列情况下修复DNS解析程序 `DNSCACHE_ENABLED=False` （ [issue 2811](https://github.com/scrapy/scrapy/issues/2811) ）
*   添加 `cryptography` Debian Jessie毒性试验环境（ [issue 2848](https://github.com/scrapy/scrapy/issues/2848) ）
*   添加验证以检查请求回调是否可调用（ [issue 2766](https://github.com/scrapy/scrapy/issues/2766) ）
*   端口 `extras/qpsclient.py` 到Python 3（Python） [issue 2849](https://github.com/scrapy/scrapy/issues/2849) ）
*   在python 3的场景下使用getfullargspec来停止取消预测警告（ [issue 2862](https://github.com/scrapy/scrapy/issues/2862) ）
*   更新不推荐使用的测试别名（ [issue 2876](https://github.com/scrapy/scrapy/issues/2876) ）
*   固定 `SitemapSpider` 支持备用链接（ [issue 2853](https://github.com/scrapy/scrapy/issues/2853) ）

### 文档

*   为添加了缺少的项目符号点 `AUTOTHROTTLE_TARGET_CONCURRENCY` 设置。（ [issue 2756](https://github.com/scrapy/scrapy/issues/2756) ）
*   更新贡献文档，记录新的支持渠道（ [issue 2762](https://github.com/scrapy/scrapy/issues/2762) ，问题：“3038”
*   在文档中包含对Scrapy Subreddit的引用
*   修复断开的链接；对外部链接使用https://（ [issue 2978](https://github.com/scrapy/scrapy/issues/2978) ， [issue 2982](https://github.com/scrapy/scrapy/issues/2982) ， [issue 2958](https://github.com/scrapy/scrapy/issues/2958) ）
*   文档CloseSpider扩展更好（ [issue 2759](https://github.com/scrapy/scrapy/issues/2759) ）
*   使用 `pymongo.collection.Collection.insert_one()` 在MongoDB示例中（ [issue 2781](https://github.com/scrapy/scrapy/issues/2781) ）
*   拼写错误和打字错误（ [issue 2828](https://github.com/scrapy/scrapy/issues/2828) ， [issue 2837](https://github.com/scrapy/scrapy/issues/2837) ， [issue 2884](https://github.com/scrapy/scrapy/issues/2884) ， [issue 2924](https://github.com/scrapy/scrapy/issues/2924) ）
*   澄清 `CSVFeedSpider.headers` 文件编制（ [issue 2826](https://github.com/scrapy/scrapy/issues/2826) ）
*   文件 `DontCloseSpider` 例外和澄清 `spider_idle` （ [issue 2791](https://github.com/scrapy/scrapy/issues/2791) ）
*   更新自述文件中的“releases”部分（ [issue 2764](https://github.com/scrapy/scrapy/issues/2764) ）
*   修正RST语法 `DOWNLOAD_FAIL_ON_DATALOSS` 文档库 [issue 2763](https://github.com/scrapy/scrapy/issues/2763) ）
*   StartProject参数描述中的小修复（ [issue 2866](https://github.com/scrapy/scrapy/issues/2866) ）
*   在response.body文档中澄清数据类型（ [issue 2922](https://github.com/scrapy/scrapy/issues/2922) ）
*   添加有关的注释 `request.meta['depth']` 到DepthmIddleware文档（ [issue 2374](https://github.com/scrapy/scrapy/issues/2374) ）
*   添加有关的注释 `request.meta['dont_merge_cookies']` CookiesMiddleware 文档（ [issue 2999](https://github.com/scrapy/scrapy/issues/2999) ）
*   最新的项目结构示例（ [issue 2964](https://github.com/scrapy/scrapy/issues/2964) ， [issue 2976](https://github.com/scrapy/scrapy/issues/2976) ）
*   itemexporters用法的更好示例（ [issue 2989](https://github.com/scrapy/scrapy/issues/2989) ）
*   文件 `from_crawler` Spider 和下载者中间商的方法（ [issue 3019](https://github.com/scrapy/scrapy/issues/3019) ）

## Scrapy 1.4.0（2017-05-18）

Scrapy1.4并没有带来那么多惊人的新功能，但还是有相当多的便利改进。

scrappy现在支持匿名ftp会话，通过新的 [`FTP_USER`](topics/settings.html#std:setting-FTP_USER) 和 [`FTP_PASSWORD`](topics/settings.html#std:setting-FTP_PASSWORD) 设置。如果您使用的是Twisted版本17.1.0或更高版本，那么ftp现在可用于python 3。

有一个新的 [`response.follow`](topics/request-response.html#scrapy.http.TextResponse.follow "scrapy.http.TextResponse.follow") 创建请求的方法； 现在，它是一种推荐的在“碎片 Spider ”中创建请求的方法。. 这种方法使得编写正确的spider更加容易； `response.follow` 与创建 `scrapy.Request` 直接对象：

*   它处理相关的URL；
*   它可以在非utf8页面上正确地使用非ASCII URL；
*   除了绝对和相对URL之外，它还支持选择器；用于 `&lt;a&gt;` 元素也可以提取它们的Href值。

例如，而不是：

```py
for href in response.css('li.page a::attr(href)').extract():
    url = response.urljoin(href)
    yield scrapy.Request(url, self.parse, encoding=response.encoding)

```

现在可以写下：

```py
for a in response.css('li.page a'):
    yield response.follow(a, self.parse)

```

链接提取器也得到了改进。它们的工作方式类似于常规的现代浏览器：从属性中删除前导空格和尾随空格（想想 `href="   http://example.com"` ）建造时 `Link` 物体。这种空白剥离也发生在 `action` 属性与 `FormRequest` .

[**](#id1)请注意，链接提取器在默认情况下不再规范化URL。[**](#id3)这让用户不时感到困惑，实际上浏览器并不是这样做的，因此我们删除了对提取链接的额外转换。

对于那些想要更多控制 `Referer:` 当跟踪链接时Scrapy发送的标题，您可以设置自己的 `Referrer Policy` . 在Scrapy 1.4之前，默认 `RefererMiddleware` 会简单而盲目地将其设置为生成HTTP请求的响应的URL（这可能会泄漏URL种子的信息）。默认情况下，scrappy现在的行为与常规浏览器非常相似。这个策略完全可以用W3C标准值定制（或者如果你愿意的话，可以用你自己定制的值）。见 [`REFERRER_POLICY`](topics/spider-middleware.html#std:setting-REFERRER_POLICY) 有关详细信息。

为了使scrappyspider更容易调试，scrappy在1.4中默认记录更多的统计信息：内存使用统计信息、详细的重试统计信息、详细的HTTP错误代码统计信息。类似的变化是，HTTP缓存路径现在也可以在日志中看到。

最后但同样重要的是，scrapy现在可以选择使用新的 [`FEED_EXPORT_INDENT`](topics/feed-exports.html#std:setting-FEED_EXPORT_INDENT) 设置。

享受！（或继续阅读此版本中的其他更改。）

### 折旧和向后不兼容的变更

*   默认为 `canonicalize=False` 在里面 `scrapy.linkextractors.LinkExtractor` （ [issue 2537](https://github.com/scrapy/scrapy/issues/2537) 修正 [issue 1941](https://github.com/scrapy/scrapy/issues/1941) 和 [issue 1982](https://github.com/scrapy/scrapy/issues/1982) ）：**警告，这是技术上向后不兼容的**
*   默认情况下启用memusage扩展（ [issue 2539](https://github.com/scrapy/scrapy/issues/2539) 修正 [issue 2187](https://github.com/scrapy/scrapy/issues/2187) ； **this is technically backward-incompatible** 因此，请检查您是否有任何非违约行为 `MEMUSAGE_***` 选项集。
*   `EDITOR` 环境变量现在优先于 `EDITOR` 在settings.py中定义的选项（ [issue 1829](https://github.com/scrapy/scrapy/issues/1829) ）； Scrapy 默认设置不再依赖于环境变量。 从技术上讲，这是一个前后不相容的变化.
*   `Spider.make_requests_from_url` 被贬低 [issue 1728](https://github.com/scrapy/scrapy/issues/1728) 修正 [issue 1495](https://github.com/scrapy/scrapy/issues/1495) ）

### 新特点

*   接受代理凭据 [`proxy`](topics/downloader-middleware.html#std:reqmeta-proxy) 请求元键（ [issue 2526](https://github.com/scrapy/scrapy/issues/2526) ）
*   支持 [brotli](https://github.com/google/brotli)-compressed content; requires optional [brotlipy](https://github.com/python-hyper/brotlipy/) ([issue 2535](https://github.com/scrapy/scrapy/issues/2535))
*   新的 [response.follow](intro/tutorial.html#response-follow-example) 创建请求的快捷方式（ [issue 1940](https://github.com/scrapy/scrapy/issues/1940) ）
*   补充 `flags` 参数和属性 [`Request`](topics/request-response.html#scrapy.http.Request "scrapy.http.Request") 对象（ [issue 2047](https://github.com/scrapy/scrapy/issues/2047) ）
*   支持匿名ftp（ [issue 2342](https://github.com/scrapy/scrapy/issues/2342) ）
*   补充 `retry/count` ， `retry/max_reached` 和 `retry/reason_count/&lt;reason&gt;` 统计到 [`RetryMiddleware`](topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware") （ [issue 2543](https://github.com/scrapy/scrapy/issues/2543) ）
*   补充 `httperror/response_ignored_count` 和 `httperror/response_ignored_status_count/&lt;status&gt;` 统计到 [`HttpErrorMiddleware`](topics/spider-middleware.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware") （ [issue 2566](https://github.com/scrapy/scrapy/issues/2566) ）
*   可定制的 [`Referrer policy`](topics/spider-middleware.html#std:setting-REFERRER_POLICY) 在里面 [`RefererMiddleware`](topics/spider-middleware.html#scrapy.spidermiddlewares.referer.RefererMiddleware "scrapy.spidermiddlewares.referer.RefererMiddleware") （ [issue 2306](https://github.com/scrapy/scrapy/issues/2306) ）
*   新的 `data:` URI下载处理程序（ [issue 2334](https://github.com/scrapy/scrapy/issues/2334) 修正 [issue 2156](https://github.com/scrapy/scrapy/issues/2156) ）
*   使用HTTP缓存时的日志缓存目录（ [issue 2611](https://github.com/scrapy/scrapy/issues/2611) 修正 [issue 2604](https://github.com/scrapy/scrapy/issues/2604) ）
*   当项目包含重复的 Spider 名称时警告用户（修复 [issue 2181](https://github.com/scrapy/scrapy/issues/2181) ）
*   `CaselessDict` 现在接受 `Mapping` 实例而不仅仅是听写（ [issue 2646](https://github.com/scrapy/scrapy/issues/2646) ）
*   [Media downloads](topics/media-pipeline.html#topics-media-pipeline) 用 `FilesPipelines` 或 `ImagesPipelines` ，现在可以选择使用新的 [`MEDIA_ALLOW_REDIRECTS`](topics/media-pipeline.html#std:setting-MEDIA_ALLOW_REDIRECTS) 设置（ [issue 2616](https://github.com/scrapy/scrapy/issues/2616) 修正 [issue 2004](https://github.com/scrapy/scrapy/issues/2004) ）
*   接受来自使用新的 [`DOWNLOAD_FAIL_ON_DATALOSS`](topics/settings.html#std:setting-DOWNLOAD_FAIL_ON_DATALOSS) 设置（ [issue 2590](https://github.com/scrapy/scrapy/issues/2590) 修正 [issue 2586](https://github.com/scrapy/scrapy/issues/2586) ）
*   JSON和XML项的可选漂亮打印通过 [`FEED_EXPORT_INDENT`](topics/feed-exports.html#std:setting-FEED_EXPORT_INDENT) 设置（ [issue 2456](https://github.com/scrapy/scrapy/issues/2456) 修正 [issue 1327](https://github.com/scrapy/scrapy/issues/1327) ）
*   允许删除字段 `FormRequest.from_response` 格式数据 `None` 值已传递（ [issue 667](https://github.com/scrapy/scrapy/issues/667) ）
*   每个请求使用新的 [`max_retry_times`](topics/request-response.html#std:reqmeta-max_retry_times) 元密钥（元密钥） [issue 2642](https://github.com/scrapy/scrapy/issues/2642) ）
*   `python -m scrapy` 作为更明确的替代方案 `scrapy` 命令（ [issue 2740](https://github.com/scrapy/scrapy/issues/2740) ）

### 错误修复

*   Linkextractor现在从属性中去掉前导空格和尾随空格。（ [issue 2547](https://github.com/scrapy/scrapy/issues/2547) 修正 [issue 1614](https://github.com/scrapy/scrapy/issues/1614) ）
*   在中正确处理action属性中的空白 `FormRequest` （ [issue 2548](https://github.com/scrapy/scrapy/issues/2548) ）
*   从代理服务器缓冲连接响应字节，直到收到所有HTTP头（ [issue 2495](https://github.com/scrapy/scrapy/issues/2495) 修正 [issue 2491](https://github.com/scrapy/scrapy/issues/2491) ）
*   ftp下载器现在可以在python 3上工作，前提是使用twisted&gt;=17.1（ [issue 2599](https://github.com/scrapy/scrapy/issues/2599) ）
*   在解压缩内容后使用body选择响应类型（ [issue 2393](https://github.com/scrapy/scrapy/issues/2393) 修正 [issue 2145](https://github.com/scrapy/scrapy/issues/2145) ）
*   总是解压缩 `Content-Encoding: gzip` 在 [`HttpCompressionMiddleware`](topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware") 阶段（阶段） [issue 2391](https://github.com/scrapy/scrapy/issues/2391) ）
*   尊重自定义日志级别 `Spider.custom_settings` （ [issue 2581](https://github.com/scrapy/scrapy/issues/2581) 修正 [issue 1612](https://github.com/scrapy/scrapy/issues/1612) ）
*   MacOS的“make htmlview”修复程序（ [issue 2661](https://github.com/scrapy/scrapy/issues/2661) ）
*   从命令列表中删除“命令”（ [issue 2695](https://github.com/scrapy/scrapy/issues/2695) ）
*   修复具有空正文的投递请求的重复内容长度头（ [issue 2677](https://github.com/scrapy/scrapy/issues/2677) ）
*   适当地取消大量下载，如上面所述 [`DOWNLOAD_MAXSIZE`](topics/settings.html#std:setting-DOWNLOAD_MAXSIZE) （ [issue 1616](https://github.com/scrapy/scrapy/issues/1616) ）
*   ImageSpipeline：使用调色板固定处理透明PNG图像（ [issue 2675](https://github.com/scrapy/scrapy/issues/2675) ）

### 清理和重构

*   测试：删除临时文件和文件夹（ [issue 2570](https://github.com/scrapy/scrapy/issues/2570) ）修复了OS X上的projectutilstest（ [issue 2569](https://github.com/scrapy/scrapy/issues/2569) ）在Travis CI上使用Linux的便携式pypy（ [issue 2710](https://github.com/scrapy/scrapy/issues/2710) ）
*   独立建筑请求 `_requests_to_follow` 爬行 Spider （ [issue 2562](https://github.com/scrapy/scrapy/issues/2562) ）
*   删除“python 3 progress”徽章（ [issue 2567](https://github.com/scrapy/scrapy/issues/2567) ）
*   再添加几行到 `.gitignore` （ [issue 2557](https://github.com/scrapy/scrapy/issues/2557) ）
*   删除BumpVersion预发布配置（ [issue 2159](https://github.com/scrapy/scrapy/issues/2159) ）
*   添加codecov.yml文件（ [issue 2750](https://github.com/scrapy/scrapy/issues/2750) ）
*   基于扭曲版本设置上下文工厂实现（ [issue 2577](https://github.com/scrapy/scrapy/issues/2577) 修正 [issue 2560](https://github.com/scrapy/scrapy/issues/2560) ）
*   添加省略 `self` 默认项目中间件模板中的参数（ [issue 2595](https://github.com/scrapy/scrapy/issues/2595) ）
*   删除冗余 `slot.add_request()` 调用ExecutionEngine（ [issue 2617](https://github.com/scrapy/scrapy/issues/2617) ）
*   捕捉更具体的 `os.error` 例外 `FSFilesStore` （ [issue 2644](https://github.com/scrapy/scrapy/issues/2644) ）
*   更改“localhost”测试服务器证书（ [issue 2720](https://github.com/scrapy/scrapy/issues/2720) ）
*   移除未使用的 `MEMUSAGE_REPORT` 设置（ [issue 2576](https://github.com/scrapy/scrapy/issues/2576) ）

### 文档

*   导出程序需要二进制模式（ [issue 2564](https://github.com/scrapy/scrapy/issues/2564) 修正 [issue 2553](https://github.com/scrapy/scrapy/issues/2553) ）
*   提及问题 [`FormRequest.from_response`](topics/request-response.html#scrapy.http.FormRequest.from_response "scrapy.http.FormRequest.from_response") 由于lxml中的错误（ [issue 2572](https://github.com/scrapy/scrapy/issues/2572) ）
*   在模板中统一使用单引号（ [issue 2596](https://github.com/scrapy/scrapy/issues/2596) ）
*   文件 `ftp_user` 和 `ftp_password` 元密钥（元密钥） [issue 2587](https://github.com/scrapy/scrapy/issues/2587) ）
*   已删除上的节，已弃用 `contrib/` （ [issue 2636](https://github.com/scrapy/scrapy/issues/2636) ）
*   在窗户上安装 Scrapy 时建议使用水蟒（ [issue 2477](https://github.com/scrapy/scrapy/issues/2477) 修正 [issue 2475](https://github.com/scrapy/scrapy/issues/2475) ）
*   常见问题解答：在Windows上重写关于python 3支持的说明（ [issue 2690](https://github.com/scrapy/scrapy/issues/2690) ）
*   重新排列选择器节（ [issue 2705](https://github.com/scrapy/scrapy/issues/2705) ）
*   去除 `__nonzero__` 从 `SelectorList` 文档库 [issue 2683](https://github.com/scrapy/scrapy/issues/2683) ）
*   在文档中说明如何禁用请求筛选 [`DUPEFILTER_CLASS`](topics/settings.html#std:setting-DUPEFILTER_CLASS) 设置（ [issue 2714](https://github.com/scrapy/scrapy/issues/2714) ）
*   在文档设置自述文件中添加sphinx_rtd_主题（ [issue 2668](https://github.com/scrapy/scrapy/issues/2668) ）
*   在json item writer示例中以文本模式打开文件（ [issue 2729](https://github.com/scrapy/scrapy/issues/2729) ）
*   澄清 `allowed_domains` 实例（例） [issue 2670](https://github.com/scrapy/scrapy/issues/2670) ）

## Scrapy 1.3.3（2017-03-10）

### 错误修复

*   制作 `SpiderLoader` 提升 `ImportError` 对于缺少依赖项和错误 [`SPIDER_MODULES`](topics/settings.html#std:setting-SPIDER_MODULES) . 从1.3.0开始，这些例外被作为警告而沉默。引入新的设置，以便在警告或异常（如果需要）之间切换；请参见 [`SPIDER_LOADER_WARN_ONLY`](topics/settings.html#std:setting-SPIDER_LOADER_WARN_ONLY) 有关详细信息。

## Scrapy 1.3.2（2017-02-13）

### 错误修复

*   在转换为/从dicts（utils.reqser）时保留请求类（ [issue 2510](https://github.com/scrapy/scrapy/issues/2510) ）
*   在教程中为作者字段使用一致的选择器（ [issue 2551](https://github.com/scrapy/scrapy/issues/2551) ）
*   在Twisted 17中修复TLS兼容性+（ [issue 2558](https://github.com/scrapy/scrapy/issues/2558) ）

## Scrapy 1.3.1（2017-02-08）

### 新特点

*   支持 `'True'` 和 `'False'` 布尔值设置的字符串值（ [issue 2519](https://github.com/scrapy/scrapy/issues/2519) ）你现在可以做 `scrapy crawl myspider -s REDIRECT_ENABLED=False` .
*   支持Kwargs `response.xpath()` 使用 [XPath variables](topics/selectors.html#topics-selectors-xpath-variables) 和特殊名称空间声明；这至少需要Parselv1.1（ [issue 2457](https://github.com/scrapy/scrapy/issues/2457) ）
*   添加对python 3.6的支持（ [issue 2485](https://github.com/scrapy/scrapy/issues/2485) ）
*   在pypy上运行测试（警告：某些测试仍然失败，因此pypy尚不受支持）。

### 错误修复

*   强制执行 `DNS_TIMEOUT` 设置（ [issue 2496](https://github.com/scrapy/scrapy/issues/2496) ）
*   固定 [`view`](topics/commands.html#std:command-view) 命令；这是v1.3.0中的回归（ [issue 2503](https://github.com/scrapy/scrapy/issues/2503) ）
*   修复有关的测试 `*_EXPIRES settings` 带有文件/图像管道（ [issue 2460](https://github.com/scrapy/scrapy/issues/2460) ）
*   使用基本项目模板时，修复生成的管道类的名称（ [issue 2466](https://github.com/scrapy/scrapy/issues/2466) ）
*   用扭曲17固定相容性+（ [issue 2496](https://github.com/scrapy/scrapy/issues/2496) ， [issue 2528](https://github.com/scrapy/scrapy/issues/2528) ）
*   固定 `scrapy.Item` python 3.6上的继承（ [issue 2511](https://github.com/scrapy/scrapy/issues/2511) ）
*   按顺序强制组件的数值 `SPIDER_MIDDLEWARES` ， `DOWNLOADER_MIDDLEWARES` ， `EXTENIONS` 和 `SPIDER_CONTRACTS` （ [issue 2420](https://github.com/scrapy/scrapy/issues/2420) ）

### 文档

*   修改了Coduct部分的代码并升级到Contributor Covenant v1.4（ [issue 2469](https://github.com/scrapy/scrapy/issues/2469) ）
*   澄清传递spider参数会将其转换为spider属性（ [issue 2483](https://github.com/scrapy/scrapy/issues/2483) ）
*   文件 `formid` 争论 `FormRequest.from_response()` （ [issue 2497](https://github.com/scrapy/scrapy/issues/2497) ）
*   向自述文件添加.rst扩展名（ [issue 2507](https://github.com/scrapy/scrapy/issues/2507) ）
*   提到级别数据库缓存存储后端（ [issue 2525](https://github.com/scrapy/scrapy/issues/2525) ）
*   使用 `yield` 在示例回调代码中（ [issue 2533](https://github.com/scrapy/scrapy/issues/2533) ）
*   添加有关HTML实体解码的说明 `.re()/.re_first()` （ [issue 1704](https://github.com/scrapy/scrapy/issues/1704) ）
*   打字错误 [issue 2512](https://github.com/scrapy/scrapy/issues/2512) ， [issue 2534](https://github.com/scrapy/scrapy/issues/2534) ， [issue 2531](https://github.com/scrapy/scrapy/issues/2531) ）

### 清除

*   拆下减速器签入 `MetaRefreshMiddleware` （ [issue 2542](https://github.com/scrapy/scrapy/issues/2542) ）
*   更快的入住 `LinkExtractor` 允许/拒绝模式（ [issue 2538](https://github.com/scrapy/scrapy/issues/2538) ）
*   删除支持旧扭曲版本的死码（ [issue 2544](https://github.com/scrapy/scrapy/issues/2544) ）

## Scrapy 1.3.0（2016-12-21）

这个版本出现在1.2.2之后不久，主要原因之一是：发现从0.18到1.2.2（包括）的版本使用了一些来自Twisted的反向端口代码（ `scrapy.xlib.tx.*` ，即使有新的扭曲模块可用。现在使用的 `twisted.web.client` 和 `twisted.internet.endpoints` 直接。（另请参见下面的清理。）

由于这是一个重大的变化，我们希望在不破坏任何使用1.2系列的项目的情况下，快速修复bug。

### 新特点

*   `MailSender` 现在接受单个字符串作为 `to` 和 `cc` 争论（ [issue 2272](https://github.com/scrapy/scrapy/issues/2272) ）
*   `scrapy fetch url` ， `scrapy shell url` 和 `fetch(url)` 在scrapy shell内部，现在默认遵循HTTP重定向（ [issue 2290](https://github.com/scrapy/scrapy/issues/2290) 见 [`fetch`](topics/commands.html#std:command-fetch) 和 [`shell`](topics/commands.html#std:command-shell) 有关详细信息。
*   `HttpErrorMiddleware` 现在记录错误 `INFO` 级别而不是 `DEBUG` ；从技术上讲 **backward incompatible** 所以请检查您的日志分析器。
*   默认情况下，记录器名称现在使用长格式路径，例如 `[scrapy.extensions.logstats]` 而不是先前版本（例如 `[scrapy]` 这是 **backward incompatible** 如果日志解析器需要短的logger name部分。您可以使用 [`LOG_SHORT_NAMES`](topics/settings.html#std:setting-LOG_SHORT_NAMES) 设置为 `True` .

### 依赖关系和清理

*   scrappy现在需要twisted&gt;=13.1，这已经是许多Linux发行版的情况了。
*   结果，我们摆脱了 `scrapy.xlib.tx.*` 模块，它复制了一些扭曲的代码，供用户使用“旧”的扭曲版本
*   `ChunkedTransferMiddleware` 已弃用并从默认的下载器中间软件中删除。

## Scrapy 1.2.3（2017-03-03）

*   打包修复：在setup.py中不允许不支持的扭曲版本

## Scrapy 1.2.2（2016-12-06）

### 错误修复

*   修复管道上发生故障时的神秘回溯 `open_spider()` （ [issue 2011](https://github.com/scrapy/scrapy/issues/2011) ）
*   修复嵌入的ipythonShell变量（修复 [issue 396](https://github.com/scrapy/scrapy/issues/396) 重新出现在1.2.0中，固定在 [issue 2418](https://github.com/scrapy/scrapy/issues/2418) ）
*   处理robots.txt时的几个补丁：
    *   处理（非标准）相对站点地图URL（ [issue 2390](https://github.com/scrapy/scrapy/issues/2390) ）
    *   在python 2中处理非ASCII URL和用户代理（ [issue 2373](https://github.com/scrapy/scrapy/issues/2373) ）

### 文档

*   文件 `"download_latency"` 键入 `Request` 的 `meta` DICT [issue 2033](https://github.com/scrapy/scrapy/issues/2033) ）
*   从目录中删除Ubuntu包上的页面（已弃用且不受支持）（ [issue 2335](https://github.com/scrapy/scrapy/issues/2335) ）
*   一些固定的打字错误（ [issue 2346](https://github.com/scrapy/scrapy/issues/2346) ， [issue 2369](https://github.com/scrapy/scrapy/issues/2369) ， [issue 2369](https://github.com/scrapy/scrapy/issues/2369) ， [issue 2380](https://github.com/scrapy/scrapy/issues/2380) ）和澄清（ [issue 2354](https://github.com/scrapy/scrapy/issues/2354) ， [issue 2325](https://github.com/scrapy/scrapy/issues/2325) ， [issue 2414](https://github.com/scrapy/scrapy/issues/2414) ）

### 其他变化

*   登广告 [conda-forge](https://anaconda.org/conda-forge/scrapy) as Scrapy's official conda channel ([issue 2387](https://github.com/scrapy/scrapy/issues/2387))
*   尝试使用时出现更多有用的错误消息 `.css()` 或 `.xpath()` 关于非文本响应（ [issue 2264](https://github.com/scrapy/scrapy/issues/2264) ）
*   `startproject` 命令现在生成一个示例 `middlewares.py` 文件（文件） [issue 2335](https://github.com/scrapy/scrapy/issues/2335) ）
*   在中添加更多依赖项的版本信息 `scrapy version` 详细输出（ [issue 2404](https://github.com/scrapy/scrapy/issues/2404) ）
*   全部删除 `*.pyc` 源分发中的文件（ [issue 2386](https://github.com/scrapy/scrapy/issues/2386) ）

## Scrapy 1.2.1（2016-10-21）

### 错误修复

*   在建立TLS/SSL连接时包括OpenSSL更为允许的默认密码（ [issue 2314](https://github.com/scrapy/scrapy/issues/2314) ）
*   修复非ASCII URL重定向上的“位置”HTTP头解码（ [issue 2321](https://github.com/scrapy/scrapy/issues/2321) ）

### 文档

*   修复jsonWriterPipeline示例（ [issue 2302](https://github.com/scrapy/scrapy/issues/2302) ）
*   各种注释： [issue 2330](https://github.com/scrapy/scrapy/issues/2330) 关于 Spider 的名字， [issue 2329](https://github.com/scrapy/scrapy/issues/2329) 在中间件方法处理顺序上， [issue 2327](https://github.com/scrapy/scrapy/issues/2327) 以列表形式获取多值HTTP头。

### 其他变化

*   远离的 `www.` 从 `start_urls` 内置 Spider 模板（ [issue 2299](https://github.com/scrapy/scrapy/issues/2299) ）

## Scrapy 1.2.0（2016-10-03）

### 新特点

*   新的 [`FEED_EXPORT_ENCODING`](topics/feed-exports.html#std:setting-FEED_EXPORT_ENCODING) 用于自定义将项写入文件时使用的编码的设置。可用于关闭 `\uXXXX` 在JSON输出中进行转义。这对于那些希望XML或CSV输出使用UTF-8以外的东西的人也很有用。（ [issue 2034](https://github.com/scrapy/scrapy/issues/2034) ）
*   `startproject` 命令现在支持一个可选的目标目录，以根据项目名称覆盖默认目录。（ [issue 2005](https://github.com/scrapy/scrapy/issues/2005) ）
*   新的 [`SCHEDULER_DEBUG`](topics/settings.html#std:setting-SCHEDULER_DEBUG) 设置为日志请求序列化失败（ [issue 1610](https://github.com/scrapy/scrapy/issues/1610) ）
*   JSON编码器现在支持序列化 `set` 实例（实例） [issue 2058](https://github.com/scrapy/scrapy/issues/2058) ）
*   解读 `application/json-amazonui-streaming` 作为 `TextResponse` （ [issue 1503](https://github.com/scrapy/scrapy/issues/1503) ）
*   `scrapy` 在使用shell工具时默认导入（ [`shell`](topics/commands.html#std:command-shell) ， [inspect_response](topics/shell.html#topics-shell-inspect-response) （ [issue 2248](https://github.com/scrapy/scrapy/issues/2248) ）

### 错误修复

*   defaultrequestheaders中间件现在在useragent中间件之前运行（ [issue 2088](https://github.com/scrapy/scrapy/issues/2088) ） 警告：这在技术上是向后不兼容的, 尽管我们认为这是错误修复。
*   HTTP缓存扩展和使用 `.scrapy` 数据目录现在在项目外部工作（ [issue 1581](https://github.com/scrapy/scrapy/issues/1581) ） 警告：这在技术上是向后不兼容的, 尽管我们认为这是错误修复。
*   `Selector` 不允许两者同时通过 `response` 和 `text` 不再（ [issue 2153](https://github.com/scrapy/scrapy/issues/2153) ）
*   修复了错误回调名称的日志记录 `scrapy parse` （ [issue 2169](https://github.com/scrapy/scrapy/issues/2169) ）
*   修复一个奇怪的gzip解压错误（ [issue 1606](https://github.com/scrapy/scrapy/issues/1606) ）
*   使用时修复所选回调 `CrawlSpider` 具有 [`scrapy parse`](topics/commands.html#std:command-parse) （ [issue 2225](https://github.com/scrapy/scrapy/issues/2225) ）
*   修复 Spider 不生成任何项时的无效JSON和XML文件（ [issue 872](https://github.com/scrapy/scrapy/issues/872) ）
*   实施 `flush()` FPR `StreamLogger` 避免日志中出现警告（ [issue 2125](https://github.com/scrapy/scrapy/issues/2125) ）

### 重构

*   `canonicalize_url` 已移至 [w3lib.url](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url) ([issue 2168](https://github.com/scrapy/scrapy/issues/2168)) .

### 测试和要求

Scrapy的新需求基线是Debian8“Jessie”。它以前是Ubuntu12.04精确版。实际上，这意味着我们至少要用这些（主要）包版本运行连续集成测试：twisted 14.0、pyopenssl 0.14、lxml 3.4。

Scrapy可以很好地处理这些包的旧版本（例如，代码库中仍然有用于旧的扭曲版本的开关），但不能保证（因为它不再被测试）。

### 文档

*   语法修正： [issue 2128](https://github.com/scrapy/scrapy/issues/2128) ， [issue 1566](https://github.com/scrapy/scrapy/issues/1566) .
*   从自述文件中删除“下载状态”徽章（ [issue 2160](https://github.com/scrapy/scrapy/issues/2160) ）
*   新污点 [architecture diagram](topics/architecture.html#topics-architecture) （ [issue 2165](https://github.com/scrapy/scrapy/issues/2165) ）
*   更新的 `Response` 参数文档（ [issue 2197](https://github.com/scrapy/scrapy/issues/2197) ）
*   改写误导 [`RANDOMIZE_DOWNLOAD_DELAY`](topics/settings.html#std:setting-RANDOMIZE_DOWNLOAD_DELAY) 描述（ [issue 2190](https://github.com/scrapy/scrapy/issues/2190) ）
*   添加stackoverflow作为支持通道（ [issue 2257](https://github.com/scrapy/scrapy/issues/2257) ）

## Scrapy 1.1.4（2017-03-03）

*   打包修复：在setup.py中不允许不支持的扭曲版本

## Scrapy 1.1.3（2016-09-22）

### 错误修复

*   子类的类属性 `ImagesPipeline` 和 `FilesPipeline` 像1.1.1之前那样工作（ [issue 2243](https://github.com/scrapy/scrapy/issues/2243) 修正 [issue 2198](https://github.com/scrapy/scrapy/issues/2198) ）

### 文档

*   [Overview](intro/overview.html#intro-overview) 和 [tutorial](intro/tutorial.html#intro-tutorial) 重写以使用http://toscrape.com网站（ [issue 2236](https://github.com/scrapy/scrapy/issues/2236) ， [issue 2249](https://github.com/scrapy/scrapy/issues/2249) ， [issue 2252](https://github.com/scrapy/scrapy/issues/2252) ）

## Scrapy 1.1.2（2016-08-18）

### 错误修复

*   介绍一个失踪者 [`IMAGES_STORE_S3_ACL`](topics/media-pipeline.html#std:setting-IMAGES_STORE_S3_ACL) 覆盖中默认ACL策略的设置 `ImagesPipeline` 将图像上载到S3时（请注意，默认的ACL策略是“private”--而不是“public read”--因为scrapy 1.1.0）
*   [`IMAGES_EXPIRES`](topics/media-pipeline.html#std:setting-IMAGES_EXPIRES) 默认值设回90（回归在1.1.1中引入）

## Scrapy 1.1.1（2016-07-13）

### 错误修复

*   在连接请求到HTTPS代理中添加“主机”头（ [issue 2069](https://github.com/scrapy/scrapy/issues/2069) ）
*   使用响应 `body` 选择响应类时（ [issue 2001](https://github.com/scrapy/scrapy/issues/2001) 修正 [issue 2000](https://github.com/scrapy/scrapy/issues/2000) ）
*   使用错误的netloc规范化URL时不要失败（ [issue 2038](https://github.com/scrapy/scrapy/issues/2038) 修正 [issue 2010](https://github.com/scrapy/scrapy/issues/2010) ）
*   一些修正 `HttpCompressionMiddleware` （和 `SitemapSpider` ）：
    *   不解码磁头响应（ [issue 2008](https://github.com/scrapy/scrapy/issues/2008) 修正 [issue 1899](https://github.com/scrapy/scrapy/issues/1899) ）
    *   gzip内容类型头中的句柄charset参数（ [issue 2050](https://github.com/scrapy/scrapy/issues/2050) 修正 [issue 2049](https://github.com/scrapy/scrapy/issues/2049) ）
    *   不解压缩gzip八进制流响应（ [issue 2065](https://github.com/scrapy/scrapy/issues/2065) 修正 [issue 2063](https://github.com/scrapy/scrapy/issues/2063) ）
*   根据IP地址主机验证证书时捕获（并忽略并发出警告）异常（ [issue 2094](https://github.com/scrapy/scrapy/issues/2094) 修正 [issue 2092](https://github.com/scrapy/scrapy/issues/2092) ）
*   制作 `FilesPipeline` 和 `ImagesPipeline` 关于使用遗留类属性进行自定义的向后兼容（ [issue 1989](https://github.com/scrapy/scrapy/issues/1989) 修正 [issue 1985](https://github.com/scrapy/scrapy/issues/1985) ）

### 新特点

*   在项目文件夹外启用genspider命令（ [issue 2052](https://github.com/scrapy/scrapy/issues/2052) ）
*   重试HTTPS连接 `TunnelError` 默认情况下（ [issue 1974](https://github.com/scrapy/scrapy/issues/1974) ）

### 文档

*   `FEED_TEMPDIR` 设置在词典编纂位置（ [commit 9b3c72c](https://github.com/scrapy/scrapy/commit/9b3c72c) ）
*   习惯用法 `.extract_first()` 概览（综述） [issue 1994](https://github.com/scrapy/scrapy/issues/1994) ）
*   版权公告中的更新年份（ [commit c2c8036](https://github.com/scrapy/scrapy/commit/c2c8036) ）
*   添加有关错误回复的信息和示例（ [issue 1995](https://github.com/scrapy/scrapy/issues/1995) ）
*   在下载器中间件示例中使用“url”变量（ [issue 2015](https://github.com/scrapy/scrapy/issues/2015) ）
*   语法修正（英文） [issue 2054](https://github.com/scrapy/scrapy/issues/2054) ， [issue 2120](https://github.com/scrapy/scrapy/issues/2120) ）
*   在 Spider 回调中使用美丽汤的新常见问题解答条目（ [issue 2048](https://github.com/scrapy/scrapy/issues/2048) ）
*   添加有关scrapy不在使用python 3的窗口上工作的注释（ [issue 2060](https://github.com/scrapy/scrapy/issues/2060) ）
*   在拉取请求中鼓励完整的标题（ [issue 2026](https://github.com/scrapy/scrapy/issues/2026) ）

### 测验

*   将travis ci和pin py test cov的py.test要求升级至2.2.1（ [issue 2095](https://github.com/scrapy/scrapy/issues/2095) ）

## Scrapy 1.1.0（2016-05-11）

这个1.1版本带来了许多有趣的特性和错误修复：

*   scrapy 1.1支持beta python 3（需要twisted&gt;=15.5）。见 [beta python 3支持](#news-betapy3) 更多细节和一些限制。
*   热门新功能：
    *   项目加载器现在支持嵌套加载器（ [issue 1467](https://github.com/scrapy/scrapy/issues/1467) ）
    *   `FormRequest.from_response` 改进（ [issue 1382](https://github.com/scrapy/scrapy/issues/1382) ， [issue 1137](https://github.com/scrapy/scrapy/issues/1137) ）
    *   附加设置 [`AUTOTHROTTLE_TARGET_CONCURRENCY`](topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY) 和改进的 AutoThrottle 文档（ [issue 1324](https://github.com/scrapy/scrapy/issues/1324) ）
    *   补充 `response.text` 以Unicode形式获取正文（ [issue 1730](https://github.com/scrapy/scrapy/issues/1730) ）
    *   匿名S3连接（ [issue 1358](https://github.com/scrapy/scrapy/issues/1358) ）
    *   下载器中间件中的延迟（ [issue 1473](https://github.com/scrapy/scrapy/issues/1473) ）这样可以更好地处理robots.txt（ [issue 1471](https://github.com/scrapy/scrapy/issues/1471) ）
    *   HTTP缓存现在更接近于RFC2616，增加了设置 [`HTTPCACHE_ALWAYS_STORE`](topics/downloader-middleware.html#std:setting-HTTPCACHE_ALWAYS_STORE) 和 [`HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS`](topics/downloader-middleware.html#std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS) （ [issue 1151](https://github.com/scrapy/scrapy/issues/1151) ）
    *   选择器被提取到Parsel_u库（ [issue 1409](https://github.com/scrapy/scrapy/issues/1409) ）这意味着您可以使用没有scrapy的scrapy选择器，也可以在不需要升级scrapy的情况下升级选择器引擎。
    *   现在，HTTPS下载器默认情况下执行TLS协议协商，而不是强制使用TLS 1.0。您还可以使用新的 [`DOWNLOADER_CLIENT_TLS_METHOD`](topics/settings.html#std:setting-DOWNLOADER_CLIENT_TLS_METHOD) .
*   这些错误修复可能需要您注意：
    *   默认情况下不重试错误请求（HTTP 400）（ [issue 1289](https://github.com/scrapy/scrapy/issues/1289) ）如果您需要旧的行为，请添加 `400` 到 [`RETRY_HTTP_CODES`](topics/downloader-middleware.html#std:setting-RETRY_HTTP_CODES) .
    *   修复shell文件参数处理（ [issue 1710](https://github.com/scrapy/scrapy/issues/1710) ， [issue 1550](https://github.com/scrapy/scrapy/issues/1550) ）如果你尝试 `scrapy shell index.html` 它将尝试加载URL [http://index.html](http://index.html)，使用 `scrapy shell ./index.html` 加载本地文件。
    *   现在，默认情况下，已为新创建的项目启用robots.txt遵从性（ [issue 1724](https://github.com/scrapy/scrapy/issues/1724) ）Scrapy还将等待robots.txt下载，然后再继续爬行。（ [issue 1735](https://github.com/scrapy/scrapy/issues/1735) ）如果要禁用此行为，请更新 [`ROBOTSTXT_OBEY`](topics/settings.html#std:setting-ROBOTSTXT_OBEY) 在里面 `settings.py` 创建新项目后的文件。
    *   导出程序现在使用Unicode，而不是默认的字节。（ [issue 1080](https://github.com/scrapy/scrapy/issues/1080) ）如果你使用 `PythonItemExporter` ，您可能希望更新代码以禁用二进制模式，但现在已弃用该模式。
    *   接受包含点的XML节点名为有效（ [issue 1533](https://github.com/scrapy/scrapy/issues/1533) ）
    *   将文件或图像上载到S3时（使用 `FilesPipeline` 或 `ImagesPipeline` ）默认的acl策略现在是“private”而不是“public”[**](#id1)警告：向后不兼容！[**](#id3)你可以使用 [`FILES_STORE_S3_ACL`](topics/media-pipeline.html#std:setting-FILES_STORE_S3_ACL) 改变它。
    *   我们重新实施了 `canonicalize_url()` 以获得更正确的输出，特别是对于具有非ASCII字符的URL（ [issue 1947](https://github.com/scrapy/scrapy/issues/1947) ）这可能会更改链接提取程序的输出，与以前的碎片版本相比。这也可能会使运行1.1之前的部分缓存项失效。**警告：向后不兼容！**

继续阅读以获取有关其他改进和错误修复的更多详细信息。

### beta python 3支持

我们一直在 [hard at work to make Scrapy run on Python 3](https://github.com/scrapy/scrapy/wiki/Python-3-Porting) . 因此，现在您可以在python 3.3、3.4和3.5上运行spider（twisted&gt;=15.5必需）。有些功能仍然缺失（有些功能可能永远无法移植）。

几乎所有内置扩展/中间产品都可以工作。但是，我们知道Python3中的一些限制：

*   Scrapy不适用于使用python 3的Windows
*   不支持发送电子邮件
*   不支持ftp下载处理程序
*   不支持telnet控制台

### 其他新功能和增强功能

*   Scrapy现在有一个 [Code of Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md) ([issue 1681](https://github.com/scrapy/scrapy/issues/1681)) .
*   命令行工具现在已经完成了zsh（ [issue 934](https://github.com/scrapy/scrapy/issues/934) ）
*   改进 `scrapy shell` ：
    *   支持bpython并通过 `SCRAPY_PYTHON_SHELL` （ [issue 1100](https://github.com/scrapy/scrapy/issues/1100) ， [issue 1444](https://github.com/scrapy/scrapy/issues/1444) ）
    *   支持没有方案的URL（ [issue 1498](https://github.com/scrapy/scrapy/issues/1498) ）**警告：向后不兼容！**
    *   恢复对相对文件路径的支持（ [issue 1710](https://github.com/scrapy/scrapy/issues/1710) ， [issue 1550](https://github.com/scrapy/scrapy/issues/1550) ）
*   补充 [`MEMUSAGE_CHECK_INTERVAL_SECONDS`](topics/settings.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS) 更改默认检查间隔的设置（ [issue 1282](https://github.com/scrapy/scrapy/issues/1282) ）
*   下载处理程序现在使用其方案在第一个请求上延迟加载（ [issue 1390](https://github.com/scrapy/scrapy/issues/1390) ， [issue 1421](https://github.com/scrapy/scrapy/issues/1421) ）
*   HTTPS下载处理程序不再强制TLS 1.0；相反，OpenSSL的 `SSLv23_method()/TLS_method()` 用于允许尝试与远程主机协商其可以达到的最高TLS协议版本（ [issue 1794](https://github.com/scrapy/scrapy/issues/1794) ， [issue 1629](https://github.com/scrapy/scrapy/issues/1629) ）
*   `RedirectMiddleware` 现在跳过状态代码 `handle_httpstatus_list` Spider 属性或 `Request` 的 `meta` 密钥（密钥） [issue 1334](https://github.com/scrapy/scrapy/issues/1334) ， [issue 1364](https://github.com/scrapy/scrapy/issues/1364) ， [issue 1447](https://github.com/scrapy/scrapy/issues/1447) ）
*   表格提交：
    *   现在工作 `&lt;button&gt;` 元素也一样（ [issue 1469](https://github.com/scrapy/scrapy/issues/1469) ）
    *   空字符串现在用于没有值的提交按钮（ [issue 1472](https://github.com/scrapy/scrapy/issues/1472) ）
*   类似dict的设置现在具有每个键的优先级（ [issue 1135](https://github.com/scrapy/scrapy/issues/1135) ， [issue 1149](https://github.com/scrapy/scrapy/issues/1149) 和 [issue 1586](https://github.com/scrapy/scrapy/issues/1586) ）
*   发送非ASCII电子邮件（ [issue 1662](https://github.com/scrapy/scrapy/issues/1662) ）
*   `CloseSpider` 和 `SpiderState` 如果没有设置相关设置，扩展现在将被禁用。（ [issue 1723](https://github.com/scrapy/scrapy/issues/1723) ， [issue 1725](https://github.com/scrapy/scrapy/issues/1725) ）
*   添加的方法 `ExecutionEngine.close` （ [issue 1423](https://github.com/scrapy/scrapy/issues/1423) ）
*   添加的方法 `CrawlerRunner.create_crawler` （ [issue 1528](https://github.com/scrapy/scrapy/issues/1528) ）
*   调度程序优先级队列现在可以通过 [`SCHEDULER_PRIORITY_QUEUE`](topics/settings.html#std:setting-SCHEDULER_PRIORITY_QUEUE) （ [issue 1822](https://github.com/scrapy/scrapy/issues/1822) ）
*   `.pps` 默认情况下，链接提取器中的链接现在被忽略。（ [issue 1835](https://github.com/scrapy/scrapy/issues/1835) ）
*   可以使用新的 [`FEED_TEMPDIR`](topics/settings.html#std:setting-FEED_TEMPDIR) 设置（ [issue 1847](https://github.com/scrapy/scrapy/issues/1847) ）
*   `FilesPipeline` 和 `ImagesPipeline` 设置现在是实例属性而不是类属性，启用特定于 Spider 的行为（ [issue 1891](https://github.com/scrapy/scrapy/issues/1891) ）
*   `JsonItemExporter` 现在，在自己的行（输出文件的第一行和最后一行）上设置打开和关闭方括号的格式（ [issue 1950](https://github.com/scrapy/scrapy/issues/1950) ）
*   如果可用， `botocore` 用于 `S3FeedStorage` ， `S3DownloadHandler` 和 `S3FilesStore` （ [issue 1761](https://github.com/scrapy/scrapy/issues/1761) ， [issue 1883](https://github.com/scrapy/scrapy/issues/1883) ）
*   大量文档更新和相关修复（ [issue 1291](https://github.com/scrapy/scrapy/issues/1291) ， [issue 1302](https://github.com/scrapy/scrapy/issues/1302) ， [issue 1335](https://github.com/scrapy/scrapy/issues/1335) ， [issue 1683](https://github.com/scrapy/scrapy/issues/1683) ， [issue 1660](https://github.com/scrapy/scrapy/issues/1660) ， [issue 1642](https://github.com/scrapy/scrapy/issues/1642) ， [issue 1721](https://github.com/scrapy/scrapy/issues/1721) ， [issue 1727](https://github.com/scrapy/scrapy/issues/1727) ， [issue 1879](https://github.com/scrapy/scrapy/issues/1879) ）
*   其他重构、优化和清理（ [issue 1476](https://github.com/scrapy/scrapy/issues/1476) ， [issue 1481](https://github.com/scrapy/scrapy/issues/1481) ， [issue 1477](https://github.com/scrapy/scrapy/issues/1477) ， [issue 1315](https://github.com/scrapy/scrapy/issues/1315) ， [issue 1290](https://github.com/scrapy/scrapy/issues/1290) ， [issue 1750](https://github.com/scrapy/scrapy/issues/1750) ， [issue 1881](https://github.com/scrapy/scrapy/issues/1881) ）

### 折旧和清除

*   补充 `to_bytes` 和 `to_unicode` 蔑视 `str_to_unicode` 和 `unicode_to_str` 功能（ [issue 778](https://github.com/scrapy/scrapy/issues/778) ）
*   `binary_is_text` 介绍，以取代使用 `isbinarytext` （但返回值相反）（ [issue 1851](https://github.com/scrapy/scrapy/issues/1851) ）
*   这个 `optional_features` 已删除集合（ [issue 1359](https://github.com/scrapy/scrapy/issues/1359) ）
*   这个 `--lsprof` 已删除命令行选项（ [issue 1689](https://github.com/scrapy/scrapy/issues/1689) ） 警告：向后不兼容, 但不会破坏用户代码。
*   下列数据类型已弃用（ [issue 1720](https://github.com/scrapy/scrapy/issues/1720) ）：
    *   `scrapy.utils.datatypes.MultiValueDictKeyError`
    *   `scrapy.utils.datatypes.MultiValueDict`
    *   `scrapy.utils.datatypes.SiteNode`
*   以前捆绑的 `scrapy.xlib.pydispatch` 库已被弃用并替换为 [pydispatcher](https://pypi.python.org/pypi/PyDispatcher) .

### 重新定位

*   `telnetconsole` 被重新安置到 `extensions/` （ [issue 1524](https://github.com/scrapy/scrapy/issues/1524) ）
    *   注意：在python 3上没有启用telnet（[https://github.com/scrapy/scrapy/pull/1524](https://github.com/scrapy/scrapy/pull/1524) issuecomment-146985595）

### 错误修正

*   Scrapy不会重试 `HTTP 400 Bad Request` 回复了。（ [issue 1289](https://github.com/scrapy/scrapy/issues/1289) ）**警告：向后不兼容！**
*   支持http_proxy config的空密码（ [issue 1274](https://github.com/scrapy/scrapy/issues/1274) ）
*   解读 `application/x-json` 作为 `TextResponse` （ [issue 1333](https://github.com/scrapy/scrapy/issues/1333) ）
*   支持多值链接rel属性（ [issue 1201](https://github.com/scrapy/scrapy/issues/1201) ）
*   固定的 `scrapy.http.FormRequest.from_response` 当有 `&lt;base&gt;` 标签（标签） [issue 1564](https://github.com/scrapy/scrapy/issues/1564) ）
*   固定的 [`TEMPLATES_DIR`](topics/settings.html#std:setting-TEMPLATES_DIR) 处理（ [issue 1575](https://github.com/scrapy/scrapy/issues/1575) ）
*   各种各样 `FormRequest` 修复（ [issue 1595](https://github.com/scrapy/scrapy/issues/1595) ， [issue 1596](https://github.com/scrapy/scrapy/issues/1596) ， [issue 1597](https://github.com/scrapy/scrapy/issues/1597) ）
*   使 `_monkeypatches` 更健壮（ [issue 1634](https://github.com/scrapy/scrapy/issues/1634) ）
*   固定错误 `XMLItemExporter` 项目中包含非字符串字段（ [issue 1738](https://github.com/scrapy/scrapy/issues/1738) ）
*   在OS X中修复了startproject命令（ [issue 1635](https://github.com/scrapy/scrapy/issues/1635) ）
*   非字符串项类型的固定pythonitexporter和csvexporter（ [issue 1737](https://github.com/scrapy/scrapy/issues/1737) ）
*   各种与日志相关的修复（ [issue 1294](https://github.com/scrapy/scrapy/issues/1294) ， [issue 1419](https://github.com/scrapy/scrapy/issues/1419) ， [issue 1263](https://github.com/scrapy/scrapy/issues/1263) ， [issue 1624](https://github.com/scrapy/scrapy/issues/1624) ， [issue 1654](https://github.com/scrapy/scrapy/issues/1654) ， [issue 1722](https://github.com/scrapy/scrapy/issues/1722) ， [issue 1726](https://github.com/scrapy/scrapy/issues/1726) 和 [issue 1303](https://github.com/scrapy/scrapy/issues/1303) ）
*   固定错误 `utils.template.render_templatefile()` （ [issue 1212](https://github.com/scrapy/scrapy/issues/1212) ）
*   从中提取站点地图 `robots.txt` 现在不区分大小写（ [issue 1902](https://github.com/scrapy/scrapy/issues/1902) ）
*   在同一远程主机上使用多个代理时，HTTPS+连接隧道可能会混淆。（ [issue 1912](https://github.com/scrapy/scrapy/issues/1912) ）

## Scrapy 1.0.7（2017-03-03）

*   打包修复：在setup.py中不允许不支持的扭曲版本

## Scrapy 1.0.6（2016-05-04）

*   修正：retrymiddleware现在对非标准的HTTP状态代码是健壮的。（ [issue 1857](https://github.com/scrapy/scrapy/issues/1857) ）
*   修复：文件存储HTTP缓存正在检查错误的修改时间（ [issue 1875](https://github.com/scrapy/scrapy/issues/1875) ）
*   文件：斯芬克斯1.4的支持+（ [issue 1893](https://github.com/scrapy/scrapy/issues/1893) ）
*   文档：选择器示例的一致性（ [issue 1869](https://github.com/scrapy/scrapy/issues/1869) ）

## Scrapy 1.0.5（2016-02-04）

*   修复：【backport】忽略linkextractor中的伪链接（修复 [issue 907](https://github.com/scrapy/scrapy/issues/907) ， [commit 108195e](https://github.com/scrapy/scrapy/commit/108195e) ）
*   tst:已将buildbot makefile更改为使用“pytest”（ [commit 1f3d90a](https://github.com/scrapy/scrapy/commit/1f3d90a) ）
*   文档：修复了教程和媒体管道中的拼写错误（ [commit 808a9ea](https://github.com/scrapy/scrapy/commit/808a9ea) 和 [commit 803bd87](https://github.com/scrapy/scrapy/commit/803bd87) ）
*   文档：在设置文档中将ajaxcrawlMiddleware添加到下载器中间件库（ [commit aa94121](https://github.com/scrapy/scrapy/commit/aa94121) ）

## Scrapy 1.0.4（2015-12-30）

*   忽略xlib/tx文件夹，具体取决于Twisted版本。（ [commit 7dfa979](https://github.com/scrapy/scrapy/commit/7dfa979) ）
*   在新Travis CI Infra上运行（ [commit 6e42f0b](https://github.com/scrapy/scrapy/commit/6e42f0b) ）
*   拼写修复（ [commit 823a1cc](https://github.com/scrapy/scrapy/commit/823a1cc) ）
*   在xmliter regex中转义nodename（ [commit da3c155](https://github.com/scrapy/scrapy/commit/da3c155) ）
*   用点测试XML节点名（ [commit 4418fc3](https://github.com/scrapy/scrapy/commit/4418fc3) ）
*   测试中不要使用坏枕头版本（ [commit a55078c](https://github.com/scrapy/scrapy/commit/a55078c) ）
*   禁用登录版本命令。关闭α1426 [commit 86fc330](https://github.com/scrapy/scrapy/commit/86fc330) ）
*   禁用登录StartProject命令（ [commit db4c9fe](https://github.com/scrapy/scrapy/commit/db4c9fe) ）
*   添加pypi下载状态徽章（ [commit df2b944](https://github.com/scrapy/scrapy/commit/df2b944) ）
*   如果一个pr是由一个 Scrapy / Scrapy 的分支生成的，则不要在travis上运行两次测试。（ [commit a83ab41](https://github.com/scrapy/scrapy/commit/a83ab41) ）
*   在自述文件中添加python 3移植状态徽章（ [commit 73ac80d](https://github.com/scrapy/scrapy/commit/73ac80d) ）
*   固定的rfpduefilter持久性（ [commit 97d080e](https://github.com/scrapy/scrapy/commit/97d080e) ）
*   TST显示Dupefilter持久性不起作用的测试（ [commit 97f2fb3](https://github.com/scrapy/scrapy/commit/97f2fb3) ）
*   在file://scheme handler上显式关闭文件（ [commit d9b4850](https://github.com/scrapy/scrapy/commit/d9b4850) ）
*   禁用shell中的dupefilter（ [commit c0d0734](https://github.com/scrapy/scrapy/commit/c0d0734) ）
*   文档：向侧边栏中显示的目录树添加标题（ [commit aa239ad](https://github.com/scrapy/scrapy/commit/aa239ad) ）
*   Doc从安装说明中删除了pywin32，因为它已经声明为依赖项。（ [commit 10eb400](https://github.com/scrapy/scrapy/commit/10eb400) ）
*   添加了有关在Windows和其他操作系统中使用Conda的安装说明。（ [commit 1c3600a](https://github.com/scrapy/scrapy/commit/1c3600a) ）
*   修正了小语法问题。（ [commit 7f4ddd5](https://github.com/scrapy/scrapy/commit/7f4ddd5) ）
*   修正了文档中的拼写错误。（ [commit b71f677](https://github.com/scrapy/scrapy/commit/b71f677) ）
*   版本1现在存在（ [commit 5456c0e](https://github.com/scrapy/scrapy/commit/5456c0e) ）
*   修复另一个无效的xpath错误（ [commit 0a1366e](https://github.com/scrapy/scrapy/commit/0a1366e) ）
*   修复值错误：selectors.rst上的xpath://div/[id=“not exists”]/text（）无效（ [commit ca8d60f](https://github.com/scrapy/scrapy/commit/ca8d60f) ）
*   拼写错误更正（ [commit 7067117](https://github.com/scrapy/scrapy/commit/7067117) ）
*   修复downloader-middleware.rst和exceptions.rst中的拼写错误，middlware-&gt;middleware（ [commit 32f115c](https://github.com/scrapy/scrapy/commit/32f115c) ）
*   在Ubuntu安装部分添加有关Debian兼容性的说明（ [commit 23fda69](https://github.com/scrapy/scrapy/commit/23fda69) ）
*   用virtualenv替换替代的osx安装解决方案（ [commit 98b63ee](https://github.com/scrapy/scrapy/commit/98b63ee) ）
*   有关安装说明，请参阅自制主页。（ [commit 1925db1](https://github.com/scrapy/scrapy/commit/1925db1) ）
*   将最旧支持的TOX版本添加到参与文档（ [commit 5d10d6d](https://github.com/scrapy/scrapy/commit/5d10d6d) ）
*   安装文档中关于pip已经包含在python中的说明&gt;=2.7.9（ [commit 85c980e](https://github.com/scrapy/scrapy/commit/85c980e) ）
*   在文档的Ubuntu安装部分添加非python依赖项（ [commit fbd010d](https://github.com/scrapy/scrapy/commit/fbd010d) ）
*   将OS X安装部分添加到文档（ [commit d8f4cba](https://github.com/scrapy/scrapy/commit/d8f4cba) ）
*   文档（enh）：显式指定RTD主题的路径（ [commit de73b1a](https://github.com/scrapy/scrapy/commit/de73b1a) ）
*   次要：scrapy.spider docs语法（ [commit 1ddcc7b](https://github.com/scrapy/scrapy/commit/1ddcc7b) ）
*   使常用实践示例代码与注释匹配（ [commit 1b85bcf](https://github.com/scrapy/scrapy/commit/1b85bcf) ）
*   下一个重复呼叫（心跳）。（ [commit 55f7104](https://github.com/scrapy/scrapy/commit/55f7104) ）
*   与Twisted 15.4.0的后端修复兼容性（ [commit b262411](https://github.com/scrapy/scrapy/commit/b262411) ）
*   插脚Pytest至2.7.3（ [commit a6535c2](https://github.com/scrapy/scrapy/commit/a6535c2) ）
*   合并请求1512来自mgedmin/patch-1（ [commit 8876111](https://github.com/scrapy/scrapy/commit/8876111) ）
*   合并请求1513来自mgedmin/patch-2（ [commit 5d4daf8](https://github.com/scrapy/scrapy/commit/5d4daf8) ）
*   Typo [commit f8d0682](https://github.com/scrapy/scrapy/commit/f8d0682) ）
*   修复列表格式（ [commit 5f83a93](https://github.com/scrapy/scrapy/commit/5f83a93) ）
*   在最近对queuelib进行了更改之后，修复 Scrapy 尖叫测试（ [commit 3365c01](https://github.com/scrapy/scrapy/commit/3365c01) ）
*   合并请求1475来自RWEindl/Patch-1（ [commit 2d688cd](https://github.com/scrapy/scrapy/commit/2d688cd) ）
*   更新tutorial.rst（ [commit fbc1f25](https://github.com/scrapy/scrapy/commit/fbc1f25) ）
*   合并请求1449，来自Rhoekman/Patch-1（ [commit 7d6538c](https://github.com/scrapy/scrapy/commit/7d6538c) ）
*   小的语法变化（ [commit 8752294](https://github.com/scrapy/scrapy/commit/8752294) ）
*   将openssl版本添加到version命令（ [commit 13c45ac](https://github.com/scrapy/scrapy/commit/13c45ac) ）

## Scrapy 1.0.3（2015-08-11）

*   将服务标识添加到Scrapy安装需要（ [commit cbc2501](https://github.com/scrapy/scrapy/commit/cbc2501) ）
*   Travis的解决方案296（ [commit 66af9cd](https://github.com/scrapy/scrapy/commit/66af9cd) ）

## Scrapy 1.0.2（2015-08-06）

*   Twisted 15.3.0不会引发picklinger或序列化lambda函数（ [commit b04dd7d](https://github.com/scrapy/scrapy/commit/b04dd7d) ）
*   次要方法名称修复（ [commit 6f85c7f](https://github.com/scrapy/scrapy/commit/6f85c7f) ）
*   小调：下流。 Spider 语法和清晰度（ [commit 9c9d2e0](https://github.com/scrapy/scrapy/commit/9c9d2e0) ）
*   宣传支持渠道（ [commit c63882b](https://github.com/scrapy/scrapy/commit/c63882b) ）
*   固定输入错误 [commit a9ae7b0](https://github.com/scrapy/scrapy/commit/a9ae7b0) ）
*   修复文档引用。（ [commit 7c8a4fe](https://github.com/scrapy/scrapy/commit/7c8a4fe) ）

## Scrapy 1.0.1（2015-07-01）

*   在传递到ftpclient之前取消引用请求路径，它已经转义了路径（ [commit cc00ad2](https://github.com/scrapy/scrapy/commit/cc00ad2) ）
*   在清单中包括测试/到源分发。（ [commit eca227e](https://github.com/scrapy/scrapy/commit/eca227e) ）
*   Doc Fix SelectJMES文档（ [commit b8567bc](https://github.com/scrapy/scrapy/commit/b8567bc) ）
*   Doc将Ubuntu和ArchLinux带到Windows子部分之外（ [commit 392233f](https://github.com/scrapy/scrapy/commit/392233f) ）
*   从Ubuntu包中删除版本后缀（ [commit 5303c66](https://github.com/scrapy/scrapy/commit/5303c66) ）
*   1.0的文档更新发布日期（ [commit c89fa29](https://github.com/scrapy/scrapy/commit/c89fa29) ）

## Scrapy 1.0.0（2015-06-19）

在这个主要版本中，您会发现许多新的特性和错误修复。确保检查我们的更新 [overview](intro/overview.html#intro-overview) 看看其中的一些变化，以及我们的刷 [tutorial](intro/tutorial.html#intro-tutorial) .

### 支持在spiders中返回字典

声明和返回 Scrapy 项目不再需要从您的 Spider 收集抓取的数据，您现在可以返回显式字典。

_经典版_

```py
class MyItem(scrapy.Item):
    url = scrapy.Field()

class MySpider(scrapy.Spider):
    def parse(self, response):
        return MyItem(url=response.url)

```

_新版本_

```py
class MySpider(scrapy.Spider):
    def parse(self, response):
        return {'url': response.url}

```

### 每个 Spider 设置（GSOC 2014）

去年的谷歌夏季代码项目完成了一项重要的机制重新设计，用于填充设置，引入明确的优先级来覆盖任何给定的设置。作为该目标的扩展，我们为专门针对单个 Spider 的设置提供了新的优先级，允许它们重新定义项目设置。

通过定义 [`custom_settings`](topics/spiders.html#scrapy.spiders.Spider.custom_settings "scrapy.spiders.Spider.custom_settings") Spider 中的类变量：

```py
class MySpider(scrapy.Spider):
    custom_settings = {
        "DOWNLOAD_DELAY": 5.0,
        "RETRY_ENABLED": False,
    }

```

阅读有关设置填充的详细信息： [设置](topics/settings.html#topics-settings)

### Python 测井

Scrapy1.0已经从扭曲的日志记录转移到支持python内置的默认日志记录系统。我们对大多数旧的自定义接口保持向后兼容性，以便调用日志记录函数，但是您将收到警告，以便完全切换到Python日志记录API。

_旧版本_

```py
from scrapy import log
log.msg('MESSAGE', log.INFO)

```

_新版本_

```py
import logging
logging.info('MESSAGE')

```

用 Spider 记录仍然是一样的，但在 [`log()`](topics/spiders.html#scrapy.spiders.Spider.log "scrapy.spiders.Spider.log") 方法可以访问自定义 [`logger`](topics/spiders.html#scrapy.spiders.Spider.logger "scrapy.spiders.Spider.logger") 为 Spider 发布日志事件而创建：

```py
class MySpider(scrapy.Spider):
    def parse(self, response):
        self.logger.info('Response received')

```

阅读日志文档中的更多内容： [Logging](topics/logging.html#topics-logging)

### 爬虫API重构（GSOC 2014）

上一个谷歌夏季代码的另一个里程碑是对内部API的重构，寻求更简单和更容易的使用。检查新的核心接口： [核心API](topics/api.html#topics-api)

您将要面对这些更改的一个常见情况是在从脚本运行scrapy时。以下是如何使用新API手动运行spider的快速示例：

```py
from scrapy.crawler import CrawlerProcess

process = CrawlerProcess({
    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'
})
process.crawl(MySpider)
process.start()

```

请记住，此功能仍在开发中，其API可能会更改，直到达到稳定状态。

请参阅运行scrappy的脚本的更多示例： [常用做法](topics/practices.html#topics-practices)

### 模块重新定位

为了改善 Scrapy 的总体结构，模块进行了大量的重新排列。主要的变化是将不同的子包分离成新的项目，并同时解散这两个项目。 `scrapy.contrib` 和 `scrapy.contrib_exp` 到顶级包中。内部重新定位之间保持向后兼容性，而导入不推荐使用的模块时会收到指示其新位置的警告。

#### 重新定位的完整列表

外包包

注解

这些扩展进行了一些小的更改，例如更改了一些设置名称。请检查每个新存储库中的文档以熟悉新用法。

| 老位置 | 新位置 |
| --- | --- |
| scrapy.commands.deploy | [scrapyd-client](https://github.com/scrapy/scrapyd-client) （见其他备选方案： [部署 Spider](topics/deploy.html#topics-deploy) ） |
| scrapy.contrib.djangoitem | [scrapy-djangoitem](https://github.com/scrapy-plugins/scrapy-djangoitem) |
| scrapy.webservice | [scrapy-jsonrpc](https://github.com/scrapy-plugins/scrapy-jsonrpc) |

`scrapy.contrib_exp` 和 `scrapy.contrib` 溶解

| 老位置 | 新位置 |
| --- | --- |
| scrapy.contribexp.downloadermiddleware.解压缩 | scrapy.downloadermiddleware.decompresson |
| scrapy.contrib_exp.iterators | scrapy.utils.iterators |
| scrapy.contrib.downloadermiddleware | scrapy.downloadermiddlewares |
| scrapy.contrib.exporter | scrapy.exporters |
| scrapy.contrib.linkextractors | scrapy.linkextractors |
| scrapy.contrib.loader | scrapy.loader |
| scrapy.contrib.loader.processor | scrapy.loader.processors |
| scrapy.contrib.pipeline | scrapy.pipelines |
| scrapy.contrib.spidermiddleware | scrapy.spidermiddlewares |
| scrapy.contrib.spiders | scrapy.spiders |
| 

*   scrapy.contrib.closespider
*   scrapy.contrib.corestats
*   scrapy.contrib.debug
*   scrapy.contrib.feedexport
*   scrapy.contrib.httpcache
*   scrapy.contrib.logstats
*   scrapy.contrib.memdebug
*   scrapy.contrib.memusage
*   scrapy.contrib.spiderstate
*   scrapy.contrib.statsmailer
*   scrapy.contrib.throttle

 | scrapy.extensions.* |

复数重命名与模块统一

| 老位置 | 新位置 |
| --- | --- |
| scrapy.command | scrapy.commands |
| scrapy.dupefilter | scrapy.dupefilters |
| scrapy.linkextractor | scrapy.linkextractors |
| scrapy.spider | scrapy.spiders |
| scrapy.squeue | scrapy.squeues |
| scrapy.statscol | scrapy.statscollectors |
| scrapy.utils.decorator | scrapy.utils.decorators |

类重命名

| 老位置 | 新位置 |
| --- | --- |
| scrapy.spidermanager.SpiderManager | scrapy.spiderloader.SpiderLoader |

设置重命名

| 老位置 | 新位置 |
| --- | --- |
| SPIDER_MANAGER_CLASS | SPIDER_LOADER_CLASS |

### Changelog

新功能和增强功能

*   python日志（ [issue 1060](https://github.com/scrapy/scrapy/issues/1060) ， [issue 1235](https://github.com/scrapy/scrapy/issues/1235) ， [issue 1236](https://github.com/scrapy/scrapy/issues/1236) ， [issue 1240](https://github.com/scrapy/scrapy/issues/1240) ， [issue 1259](https://github.com/scrapy/scrapy/issues/1259) ， [issue 1278](https://github.com/scrapy/scrapy/issues/1278) ， [issue 1286](https://github.com/scrapy/scrapy/issues/1286) ）
*   feed_export_fields选项（ [issue 1159](https://github.com/scrapy/scrapy/issues/1159) ， [issue 1224](https://github.com/scrapy/scrapy/issues/1224) ）
*   DNS缓存大小和超时选项（ [issue 1132](https://github.com/scrapy/scrapy/issues/1132) ）
*   支持xmliter？lxml中的命名空间前缀（ [issue 963](https://github.com/scrapy/scrapy/issues/963) ）
*   反应器线程池最大大小设置（ [issue 1123](https://github.com/scrapy/scrapy/issues/1123) ）
*   允许 Spider 返回听写。（ [issue 1081](https://github.com/scrapy/scrapy/issues/1081) ）
*   添加response.urljoin（）帮助程序（ [issue 1086](https://github.com/scrapy/scrapy/issues/1086) ）
*   在~/.config/scrappy.cfg中查找用户配置（ [issue 1098](https://github.com/scrapy/scrapy/issues/1098) ）
*   处理TLS SNI（ [issue 1101](https://github.com/scrapy/scrapy/issues/1101) ）
*   选择列表先提取（ [issue 624](https://github.com/scrapy/scrapy/issues/624) ， [issue 1145](https://github.com/scrapy/scrapy/issues/1145) ）
*   添加了jmesselect（ [issue 1016](https://github.com/scrapy/scrapy/issues/1016) ）
*   将gzip压缩添加到文件系统HTTP缓存后端（ [issue 1020](https://github.com/scrapy/scrapy/issues/1020) ）
*   链接提取器中的CSS支持（ [issue 983](https://github.com/scrapy/scrapy/issues/983) ）
*   httpcache不缓存meta 19 689（ [issue 821](https://github.com/scrapy/scrapy/issues/821) ）
*   添加调度程序丢弃请求时要发送的信号（ [issue 961](https://github.com/scrapy/scrapy/issues/961) ）
*   避免下载大响应（ [issue 946](https://github.com/scrapy/scrapy/issues/946) ）
*   允许在csvfeedspider中指定QuoteCar（ [issue 882](https://github.com/scrapy/scrapy/issues/882) ）
*   添加对“ Spider 错误处理”日志消息的引用（ [issue 795](https://github.com/scrapy/scrapy/issues/795) ）
*   处理robots.txt一次（ [issue 896](https://github.com/scrapy/scrapy/issues/896) ）
*   每个 Spider 的GSOC设置（ [issue 854](https://github.com/scrapy/scrapy/issues/854) ）
*   添加项目名称验证（ [issue 817](https://github.com/scrapy/scrapy/issues/817) ）
*   GSOC API清理（ [issue 816](https://github.com/scrapy/scrapy/issues/816) ， [issue 1128](https://github.com/scrapy/scrapy/issues/1128) ， [issue 1147](https://github.com/scrapy/scrapy/issues/1147) ， [issue 1148](https://github.com/scrapy/scrapy/issues/1148) ， [issue 1156](https://github.com/scrapy/scrapy/issues/1156) ， [issue 1185](https://github.com/scrapy/scrapy/issues/1185) ， [issue 1187](https://github.com/scrapy/scrapy/issues/1187) ， [issue 1258](https://github.com/scrapy/scrapy/issues/1258) ， [issue 1268](https://github.com/scrapy/scrapy/issues/1268) ， [issue 1276](https://github.com/scrapy/scrapy/issues/1276) ， [issue 1285](https://github.com/scrapy/scrapy/issues/1285) ， [issue 1284](https://github.com/scrapy/scrapy/issues/1284) ）
*   对IO操作的响应能力更强（ [issue 1074](https://github.com/scrapy/scrapy/issues/1074) 和 [issue 1075](https://github.com/scrapy/scrapy/issues/1075) ）
*   关闭时对httpcache执行leveldb压缩（ [issue 1297](https://github.com/scrapy/scrapy/issues/1297) ）

折旧和清除

*   取消预测htmlparser链接提取程序（ [issue 1205](https://github.com/scrapy/scrapy/issues/1205) ）
*   从FeedExporter中删除已弃用的代码（ [issue 1155](https://github.com/scrapy/scrapy/issues/1155) ）
*   用于.15兼容性的剩余部分（ [issue 925](https://github.com/scrapy/scrapy/issues/925) ）
*   放弃对每个 Spider 并发请求的支持（ [issue 895](https://github.com/scrapy/scrapy/issues/895) ）
*   删除旧的发动机代码（ [issue 911](https://github.com/scrapy/scrapy/issues/911) ）
*   拆除SGMLLinkextractor（ [issue 777](https://github.com/scrapy/scrapy/issues/777) ）

重新定位

*   将exporters/uuu init_uuu.py移动到exporters.py（ [issue 1242](https://github.com/scrapy/scrapy/issues/1242) ）
*   将基类移动到其包中（ [issue 1218](https://github.com/scrapy/scrapy/issues/1218) ， [issue 1233](https://github.com/scrapy/scrapy/issues/1233) ）
*   模块重新定位（ [issue 1181](https://github.com/scrapy/scrapy/issues/1181) ， [issue 1210](https://github.com/scrapy/scrapy/issues/1210) ）
*   将spiderManager重命名为spiderLoader（ [issue 1166](https://github.com/scrapy/scrapy/issues/1166) ）
*   移除Djangoitem（ [issue 1177](https://github.com/scrapy/scrapy/issues/1177) ）
*   删除 Scrapy 部署命令（ [issue 1102](https://github.com/scrapy/scrapy/issues/1102) ）
*   解除控制（ [issue 1134](https://github.com/scrapy/scrapy/issues/1134) ）
*   已从根目录中删除bin文件夹，修复913（ [issue 914](https://github.com/scrapy/scrapy/issues/914) ）
*   删除基于JSONRPC的WebService（ [issue 859](https://github.com/scrapy/scrapy/issues/859) ）
*   在项目根目录下移动测试用例（ [issue 827](https://github.com/scrapy/scrapy/issues/827) ， [issue 841](https://github.com/scrapy/scrapy/issues/841) ）
*   修复设置中重新定位路径的向后不兼容性（ [issue 1267](https://github.com/scrapy/scrapy/issues/1267) ）

文档

*   爬虫过程文档（ [issue 1190](https://github.com/scrapy/scrapy/issues/1190) ）
*   在描述中倾向于使用Web抓取而不是屏幕抓取（ [issue 1188](https://github.com/scrapy/scrapy/issues/1188) ）
*   对Scrapy教程的一些改进（ [issue 1180](https://github.com/scrapy/scrapy/issues/1180) ）
*   将文件管道与图像管道一起记录（ [issue 1150](https://github.com/scrapy/scrapy/issues/1150) ）
*   部署文档调整（ [issue 1164](https://github.com/scrapy/scrapy/issues/1164) ）
*   增加了部署部分，包括废料部署和SHUB（ [issue 1124](https://github.com/scrapy/scrapy/issues/1124) ）
*   向项目模板添加更多设置（ [issue 1073](https://github.com/scrapy/scrapy/issues/1073) ）
*   概述页面的一些改进（ [issue 1106](https://github.com/scrapy/scrapy/issues/1106) ）
*   更新了docs/topics/architecture.rst中的链接（ [issue 647](https://github.com/scrapy/scrapy/issues/647) ）
*   文档重新排序主题（ [issue 1022](https://github.com/scrapy/scrapy/issues/1022) ）
*   更新request.meta特殊键列表（ [issue 1071](https://github.com/scrapy/scrapy/issues/1071) ）
*   文档下载超时（ [issue 898](https://github.com/scrapy/scrapy/issues/898) ）
*   文档简化扩展文档（ [issue 893](https://github.com/scrapy/scrapy/issues/893) ）
*   泄漏文档 [issue 894](https://github.com/scrapy/scrapy/issues/894) ）
*   项目管道的爬虫方法的文档（ [issue 904](https://github.com/scrapy/scrapy/issues/904) ）
*   Spider 网错误不支持延迟（ [issue 1292](https://github.com/scrapy/scrapy/issues/1292) ）
*   修正和狮身人面像相关修正（ [issue 1220](https://github.com/scrapy/scrapy/issues/1220) ， [issue 1219](https://github.com/scrapy/scrapy/issues/1219) ， [issue 1196](https://github.com/scrapy/scrapy/issues/1196) ， [issue 1172](https://github.com/scrapy/scrapy/issues/1172) ， [issue 1171](https://github.com/scrapy/scrapy/issues/1171) ， [issue 1169](https://github.com/scrapy/scrapy/issues/1169) ， [issue 1160](https://github.com/scrapy/scrapy/issues/1160) ， [issue 1154](https://github.com/scrapy/scrapy/issues/1154) ， [issue 1127](https://github.com/scrapy/scrapy/issues/1127) ， [issue 1112](https://github.com/scrapy/scrapy/issues/1112) ， [issue 1105](https://github.com/scrapy/scrapy/issues/1105) ， [issue 1041](https://github.com/scrapy/scrapy/issues/1041) ， [issue 1082](https://github.com/scrapy/scrapy/issues/1082) ， [issue 1033](https://github.com/scrapy/scrapy/issues/1033) ， [issue 944](https://github.com/scrapy/scrapy/issues/944) ， [issue 866](https://github.com/scrapy/scrapy/issues/866) ， [issue 864](https://github.com/scrapy/scrapy/issues/864) ， [issue 796](https://github.com/scrapy/scrapy/issues/796) ， [issue 1260](https://github.com/scrapy/scrapy/issues/1260) ， [issue 1271](https://github.com/scrapy/scrapy/issues/1271) ， [issue 1293](https://github.com/scrapy/scrapy/issues/1293) ， [issue 1298](https://github.com/scrapy/scrapy/issues/1298) ）

错误修正

*   项目多继承修复（ [issue 353](https://github.com/scrapy/scrapy/issues/353) ， [issue 1228](https://github.com/scrapy/scrapy/issues/1228) ）
*   itemloader.load_item:迭代字段副本（ [issue 722](https://github.com/scrapy/scrapy/issues/722) ）
*   修复延迟（robotstxtmiddleware）中未处理的错误（ [issue 1131](https://github.com/scrapy/scrapy/issues/1131) ， [issue 1197](https://github.com/scrapy/scrapy/issues/1197) ）
*   强制读取下载超时为int（ [issue 954](https://github.com/scrapy/scrapy/issues/954) ）
*   scrapy.utils.misc.load_对象应打印完整的回溯（ [issue 902](https://github.com/scrapy/scrapy/issues/902) ）
*   修复“.local”主机名的错误（ [issue 878](https://github.com/scrapy/scrapy/issues/878) ）
*   修复已启用的扩展、中间软件、管道信息不再打印（ [issue 879](https://github.com/scrapy/scrapy/issues/879) ）
*   修复在meta设置为false时不合并cookies的不良行为（ [issue 846](https://github.com/scrapy/scrapy/issues/846) ）

python 3进行中支持

*   如果twisted.conch不可用，则禁用scrappy.telnet（ [issue 1161](https://github.com/scrapy/scrapy/issues/1161) ）
*   修复ajaxcrawl.py中的python 3语法错误（ [issue 1162](https://github.com/scrapy/scrapy/issues/1162) ）
*   Urllib的更多python3兼容性更改（ [issue 1121](https://github.com/scrapy/scrapy/issues/1121) ）
*   在Python3中，AssertItemSequal被重命名为AssertCountEqual。（ [issue 1070](https://github.com/scrapy/scrapy/issues/1070) ）
*   导入unittest.mock（如果可用）。（ [issue 1066](https://github.com/scrapy/scrapy/issues/1066) ）
*   更新了不推荐使用的cgi.parse_qsl以使用six的parse_qsl（ [issue 909](https://github.com/scrapy/scrapy/issues/909) ）
*   防止python 3端口回归（ [issue 830](https://github.com/scrapy/scrapy/issues/830) ）
*   py3:对python 3使用可变映射（ [issue 810](https://github.com/scrapy/scrapy/issues/810) ）
*   py3：使用six.bytesio和six.moves.cstringio（ [issue 803](https://github.com/scrapy/scrapy/issues/803) ）
*   py3:修复xmlrpclib和电子邮件导入（ [issue 801](https://github.com/scrapy/scrapy/issues/801) ）
*   PY3：使用6个用于robotparser和urlparse（ [issue 800](https://github.com/scrapy/scrapy/issues/800) ）
*   py3：使用6.iterkeys、6.iteritems和tempfile（ [issue 799](https://github.com/scrapy/scrapy/issues/799) ）
*   py3:fix有_键并使用six.moves.configparser（ [issue 798](https://github.com/scrapy/scrapy/issues/798) ）
*   py3:使用six.moves.cpickle（ [issue 797](https://github.com/scrapy/scrapy/issues/797) ）
*   py3使在python3中运行一些测试成为可能（ [issue 776](https://github.com/scrapy/scrapy/issues/776) ）

测验

*   从PY3中删除不必要的行忽略（ [issue 1243](https://github.com/scrapy/scrapy/issues/1243) ）
*   在收集测试时修复来自pytest的剩余警告（ [issue 1206](https://github.com/scrapy/scrapy/issues/1206) ）
*   将文档生成添加到Travis（ [issue 1234](https://github.com/scrapy/scrapy/issues/1234) ）
*   TST不从不推荐使用的模块收集测试。（ [issue 1165](https://github.com/scrapy/scrapy/issues/1165) ）
*   在测试中安装Service_Identity包以防止警告（ [issue 1168](https://github.com/scrapy/scrapy/issues/1168) ）
*   修复测试中不推荐使用的设置API（ [issue 1152](https://github.com/scrapy/scrapy/issues/1152) ）
*   使用post方法为WebClient添加测试，但未提供主体（ [issue 1089](https://github.com/scrapy/scrapy/issues/1089) ）
*   py3-ignores.txt支持注释（ [issue 1044](https://github.com/scrapy/scrapy/issues/1044) ）
*   使一些主张现代化（ [issue 835](https://github.com/scrapy/scrapy/issues/835) ）
*   选择器。重复测试（ [issue 779](https://github.com/scrapy/scrapy/issues/779) ）

代码重构

*   csvfeedspider清理：使用迭代 Spider 网输出（ [issue 1079](https://github.com/scrapy/scrapy/issues/1079) ）
*   从scrapy.utils.spider.iter_spider_输出中删除不必要的检查（ [issue 1078](https://github.com/scrapy/scrapy/issues/1078) ）
*   派送PEP8（ [issue 992](https://github.com/scrapy/scrapy/issues/992) ）
*   已从walk_modules（）中删除未使用的“load=false”参数（ [issue 871](https://github.com/scrapy/scrapy/issues/871) ）
*   为了保持一致，请使用 `job_dir` 帮手 `SpiderState` 延伸。（ [issue 805](https://github.com/scrapy/scrapy/issues/805) ）
*   将“sflo”局部变量重命名为不那么神秘的“log_observer”（ [issue 775](https://github.com/scrapy/scrapy/issues/775) ）

## Scrapy 0.24.6（2015-04-20）

*   使用py2下的unicode_转义对无效的xpath进行编码（ [commit 07cb3e5](https://github.com/scrapy/scrapy/commit/07cb3e5) ）
*   修复ipython shell作用域问题并加载ipython用户配置（ [commit 2c8e573](https://github.com/scrapy/scrapy/commit/2c8e573) ）
*   修复文档中的小错误（ [commit d694019](https://github.com/scrapy/scrapy/commit/d694019) ）
*   固定小打字错误（ [commit f92fa83](https://github.com/scrapy/scrapy/commit/f92fa83) ）
*   在提取数据时已将sel.xpath（）调用转换为response.xpath（）。（ [commit c2c6d15](https://github.com/scrapy/scrapy/commit/c2c6d15) ）

## Scrapy 0.24.5（2015-02-25）

*   在Twisted 15.0.0上支持新的getEndpoint代理签名（ [commit 540b9bc](https://github.com/scrapy/scrapy/commit/540b9bc) ）
*   多了几个参考文献（ [commit b4c454b](https://github.com/scrapy/scrapy/commit/b4c454b) ）
*   文档修复引用（ [commit e3c1260](https://github.com/scrapy/scrapy/commit/e3c1260) ）
*   T.I.B.ThreadeDresolver现在是一个新的类（ [commit 9e13f42](https://github.com/scrapy/scrapy/commit/9e13f42) ）
*   S3DownloadHandler:修复带引用路径/查询参数的请求的身份验证（ [commit cdb9a0b](https://github.com/scrapy/scrapy/commit/cdb9a0b) ）
*   修复了mailsender文档中的变量类型（ [commit bb3a848](https://github.com/scrapy/scrapy/commit/bb3a848) ）
*   重置项目而不是项目计数（ [commit edb07a4](https://github.com/scrapy/scrapy/commit/edb07a4) ）
*   关于阅读什么文件供贡献的暂定注意信息（ [commit 7ee6f7a](https://github.com/scrapy/scrapy/commit/7ee6f7a) ）
*   Mitmproxy 0.10.1也需要Netlib 0.10.1（ [commit 874fcdd](https://github.com/scrapy/scrapy/commit/874fcdd) ）
*   销Mitmproxy 0.10.1 as&gt;0.11不适用于测试（ [commit c6b21f0](https://github.com/scrapy/scrapy/commit/c6b21f0) ）
*   在本地测试parse命令，而不是针对外部URL（ [commit c3a6628](https://github.com/scrapy/scrapy/commit/c3a6628) ）
*   关闭httpDownloadHandler上的连接池时出现补丁扭曲问题（ [commit d0bf957](https://github.com/scrapy/scrapy/commit/d0bf957) ）
*   更新动态项类的文档。（ [commit eeb589a](https://github.com/scrapy/scrapy/commit/eeb589a) ）
*   来自Lazar-T/Patch-3的合并请求943（ [commit 5fdab02](https://github.com/scrapy/scrapy/commit/5fdab02) ）
*   打字错误（ [commit b0ae199](https://github.com/scrapy/scrapy/commit/b0ae199) ）
*   Twisted需要pywin32。关闭α937 [commit 5cb0cfb](https://github.com/scrapy/scrapy/commit/5cb0cfb) ）
*   更新install.rst（ [commit 781286b](https://github.com/scrapy/scrapy/commit/781286b) ）
*   来自Lazar-T/Patch-1的合并请求928（ [commit b415d04](https://github.com/scrapy/scrapy/commit/b415d04) ）
*   逗号而不是句号（ [commit 627b9ba](https://github.com/scrapy/scrapy/commit/627b9ba) ）
*   合并请求885来自JSMA/Patch-1（ [commit de909ad](https://github.com/scrapy/scrapy/commit/de909ad) ）
*   更新request-response.rst（ [commit 3f3263d](https://github.com/scrapy/scrapy/commit/3f3263d) ）
*   sgmlinkextractor-用于解析存在unicode的&lt;area&gt;标记的修复程序（ [commit 49b40f0](https://github.com/scrapy/scrapy/commit/49b40f0) ）

## Scrapy 0.24.4（2014-08-09）

*   mockserver使用PEM文件，scrapy bench需要。（ [commit 5eddc68](https://github.com/scrapy/scrapy/commit/5eddc68) ）
*   下脚料台需要下脚料。测试*（ [commit d6cb999](https://github.com/scrapy/scrapy/commit/d6cb999) ）

## Scrapy 0.24.3（2014-08-09）

*   无需在PY3上浪费Travis CI时间0.24（ [commit 8e080c1](https://github.com/scrapy/scrapy/commit/8e080c1) ）
*   更新安装文档（ [commit 1d0c096](https://github.com/scrapy/scrapy/commit/1d0c096) ）
*   有一个特洛夫分类器为 Scrapy 框架！（ [commit 4c701d7](https://github.com/scrapy/scrapy/commit/4c701d7) ）
*   更新提到w3lib版本的其他位置（ [commit d109c13](https://github.com/scrapy/scrapy/commit/d109c13) ）
*   将w3lib要求更新为1.8.0（ [commit 39d2ce5](https://github.com/scrapy/scrapy/commit/39d2ce5) ）
*   使用w3lib.html.replace_entities（）（不推荐使用remove_entities（））（ [commit 180d3ad](https://github.com/scrapy/scrapy/commit/180d3ad) ）
*   设置zip_safe=false（ [commit a51ee8b](https://github.com/scrapy/scrapy/commit/a51ee8b) ）
*   不装运测试包（ [commit ee3b371](https://github.com/scrapy/scrapy/commit/ee3b371) ）
*   不再需要scrappy.bat（ [commit c3861cf](https://github.com/scrapy/scrapy/commit/c3861cf) ）
*   现代化设置.py（ [commit 362e322](https://github.com/scrapy/scrapy/commit/362e322) ）
*   头不能处理非字符串值（ [commit 94a5c65](https://github.com/scrapy/scrapy/commit/94a5c65) ）
*   修复FTP测试用例（ [commit a274a7f](https://github.com/scrapy/scrapy/commit/a274a7f) ）
*   Travis CI构建的总结大约需要50分钟才能完成。（ [commit ae1e2cc](https://github.com/scrapy/scrapy/commit/ae1e2cc) ）
*   更新shell.rst typo（ [commit e49c96a](https://github.com/scrapy/scrapy/commit/e49c96a) ）
*   删除shell结果中的奇怪缩进（ [commit 1ca489d](https://github.com/scrapy/scrapy/commit/1ca489d) ）
*   改进了解释，澄清了博客文章的来源，在规范中添加了xpath字符串函数的链接（ [commit 65c8f05](https://github.com/scrapy/scrapy/commit/65c8f05) ）
*   已重命名usertimeouterrror和servertimeouterrror 583（ [commit 037f6ab](https://github.com/scrapy/scrapy/commit/037f6ab) ）
*   向选择器文档添加一些XPath提示（ [commit 2d103e0](https://github.com/scrapy/scrapy/commit/2d103e0) ）
*   修复测试以解释https://github.com/scrappy/w3lib/pull/23（ [commit f8d366a](https://github.com/scrapy/scrapy/commit/f8d366a) ）
*   获取_func_参数最大递归修复728（ [commit 81344ea](https://github.com/scrapy/scrapy/commit/81344ea) ）
*   根据560更新输入/输出处理器示例。（ [commit f7c4ea8](https://github.com/scrapy/scrapy/commit/f7c4ea8) ）
*   修复了教程中的python语法。（ [commit db59ed9](https://github.com/scrapy/scrapy/commit/db59ed9) ）
*   为隧道代理添加测试用例（ [commit f090260](https://github.com/scrapy/scrapy/commit/f090260) ）
*   使用隧道时将代理授权头泄漏到远程主机的错误修复（ [commit d8793af](https://github.com/scrapy/scrapy/commit/d8793af) ）
*   从具有mime类型“application/xml”的xhtml文档中提取链接（ [commit ed1f376](https://github.com/scrapy/scrapy/commit/ed1f376) ）
*   合并请求来自Roysc/Patch-1的793（ [commit 91a1106](https://github.com/scrapy/scrapy/commit/91a1106) ）
*   修复commands.rst中的拼写错误（ [commit 743e1e2](https://github.com/scrapy/scrapy/commit/743e1e2) ）
*   settings.overrides.setdefault的更好测试用例（ [commit e22daaf](https://github.com/scrapy/scrapy/commit/e22daaf) ）
*   根据HTTP 1.1定义使用CRLF作为行标记（ [commit 5ec430b](https://github.com/scrapy/scrapy/commit/5ec430b) ）

## Scrapy 0.24.2（2014-07-08）

*   使用可变映射来代理不推荐使用的设置。overrides和settings.defaults属性（ [commit e5e8133](https://github.com/scrapy/scrapy/commit/e5e8133) ）
*   尚未支持python3（ [commit 3cd6146](https://github.com/scrapy/scrapy/commit/3cd6146) ）
*   将python兼容版本集更新为debian包（ [commit fa5d76b](https://github.com/scrapy/scrapy/commit/fa5d76b) ）
*   发行说明中的文档修复格式（ [commit c6a9e20](https://github.com/scrapy/scrapy/commit/c6a9e20) ）

## Scrapy 0.24.1（2014-06-27）

*   修复不推荐使用的Crawlersettings并提高与.defaults属性的向后兼容性（ [commit 8e3f20a](https://github.com/scrapy/scrapy/commit/8e3f20a) ）

## Scrapy 0.24.0（2014-06-26）

### 增强功能

*   改进残缺的顶级命名空间（ [issue 494](https://github.com/scrapy/scrapy/issues/494) ， [issue 684](https://github.com/scrapy/scrapy/issues/684) ）
*   向响应添加选择器快捷方式（ [issue 554](https://github.com/scrapy/scrapy/issues/554) ， [issue 690](https://github.com/scrapy/scrapy/issues/690) ）
*   添加新的基于lxml的linkextractor以替换未包含的sgmlinkextractor（ [issue 559](https://github.com/scrapy/scrapy/issues/559) ， [issue 761](https://github.com/scrapy/scrapy/issues/761) ， [issue 763](https://github.com/scrapy/scrapy/issues/763) ）
*   清理设置API-每个 Spider 设置的一部分 **GSoC project** （ [issue 737](https://github.com/scrapy/scrapy/issues/737) ）
*   将utf8编码头添加到模板（ [issue 688](https://github.com/scrapy/scrapy/issues/688) ， [issue 762](https://github.com/scrapy/scrapy/issues/762) ）
*   Telnet控制台现在默认绑定到127.0.0.1（ [issue 699](https://github.com/scrapy/scrapy/issues/699) ）
*   更新debian/ubuntu安装说明（ [issue 509](https://github.com/scrapy/scrapy/issues/509) ， [issue 549](https://github.com/scrapy/scrapy/issues/549) ）
*   禁用LXML XPath计算中的智能字符串（ [issue 535](https://github.com/scrapy/scrapy/issues/535) ）
*   将基于文件系统的缓存还原为HTTP缓存中间件的默认缓存（ [issue 541](https://github.com/scrapy/scrapy/issues/541) ， [issue 500](https://github.com/scrapy/scrapy/issues/500) ， [issue 571](https://github.com/scrapy/scrapy/issues/571) ）
*   将当前爬行器暴露在 Scrapy 壳中（ [issue 557](https://github.com/scrapy/scrapy/issues/557) ）
*   改进测试套件，比较csv和xml导出器（ [issue 570](https://github.com/scrapy/scrapy/issues/570) ）
*   新的 `offsite/filtered` 和 `offsite/domains` 统计数据（统计） [issue 566](https://github.com/scrapy/scrapy/issues/566) ）
*   在Crawlspiper中支持进程链接作为生成器（ [issue 555](https://github.com/scrapy/scrapy/issues/555) ）
*   DupeFilter的详细日志记录和新统计计数器（ [issue 553](https://github.com/scrapy/scrapy/issues/553) ）
*   将mimetype参数添加到 `MailSender.send()` （ [issue 602](https://github.com/scrapy/scrapy/issues/602) ）
*   通用化文件管道日志消息（ [issue 622](https://github.com/scrapy/scrapy/issues/622) ）
*   用sgmlinkextractor中的HTML实体替换不可编码的代码点（ [issue 565](https://github.com/scrapy/scrapy/issues/565) ）
*   已将SEP文档转换为RST格式（ [issue 629](https://github.com/scrapy/scrapy/issues/629) ， [issue 630](https://github.com/scrapy/scrapy/issues/630) ， [issue 638](https://github.com/scrapy/scrapy/issues/638) ， [issue 632](https://github.com/scrapy/scrapy/issues/632) ， [issue 636](https://github.com/scrapy/scrapy/issues/636) ， [issue 640](https://github.com/scrapy/scrapy/issues/640) ， [issue 635](https://github.com/scrapy/scrapy/issues/635) ， [issue 634](https://github.com/scrapy/scrapy/issues/634) ， [issue 639](https://github.com/scrapy/scrapy/issues/639) ， [issue 637](https://github.com/scrapy/scrapy/issues/637) ， [issue 631](https://github.com/scrapy/scrapy/issues/631) ， [issue 633](https://github.com/scrapy/scrapy/issues/633) ， [issue 641](https://github.com/scrapy/scrapy/issues/641) ， [issue 642](https://github.com/scrapy/scrapy/issues/642) ）
*   用于表单请求中ClickData的nr索引的测试和文档（ [issue 646](https://github.com/scrapy/scrapy/issues/646) ， [issue 645](https://github.com/scrapy/scrapy/issues/645) ）
*   允许像禁用任何其他组件一样禁用下载程序处理程序（ [issue 650](https://github.com/scrapy/scrapy/issues/650) ）
*   在重定向过多后放弃请求时记录（ [issue 654](https://github.com/scrapy/scrapy/issues/654) ）
*   如果 Spider 回调不处理错误响应，则记录错误响应（ [issue 612](https://github.com/scrapy/scrapy/issues/612) ， [issue 656](https://github.com/scrapy/scrapy/issues/656) ）
*   向HTTP压缩mw添加内容类型检查（ [issue 193](https://github.com/scrapy/scrapy/issues/193) ， [issue 660](https://github.com/scrapy/scrapy/issues/660) ）
*   使用来自ppa的最新pypi运行pypypy测试（ [issue 674](https://github.com/scrapy/scrapy/issues/674) ）
*   使用pytest而不是trial运行测试套件（ [issue 679](https://github.com/scrapy/scrapy/issues/679) ）
*   建立文档并检查毒物环境中的死链接（ [issue 687](https://github.com/scrapy/scrapy/issues/687) ）
*   使scrappy.versionu info成为整数的元组（ [issue 681](https://github.com/scrapy/scrapy/issues/681) ， [issue 692](https://github.com/scrapy/scrapy/issues/692) ）
*   从文件扩展名推断导出程序的输出格式（ [issue 546](https://github.com/scrapy/scrapy/issues/546) ， [issue 659](https://github.com/scrapy/scrapy/issues/659) ， [issue 760](https://github.com/scrapy/scrapy/issues/760) ）
*   在中支持不区分大小写的域 `url_is_from_any_domain()` （ [issue 693](https://github.com/scrapy/scrapy/issues/693) ）
*   删除项目和Spider模板中的PEP8警告（ [issue 698](https://github.com/scrapy/scrapy/issues/698) ）
*   测试和文档 `request_fingerprint` 功能（ [issue 597](https://github.com/scrapy/scrapy/issues/597) ）
*   GSOC项目9月19日更新 `per-spider settings` （ [issue 705](https://github.com/scrapy/scrapy/issues/705) ）
*   合同失败时，将退出代码设置为非零（ [issue 727](https://github.com/scrapy/scrapy/issues/727) ）
*   添加一个设置以控制作为下载程序组件的类（ [issue 738](https://github.com/scrapy/scrapy/issues/738) ）
*   传入响应 `item_dropped` 信号（信号） [issue 724](https://github.com/scrapy/scrapy/issues/724) ）
*   改进 `scrapy check` 合同指挥部（ [issue 733](https://github.com/scrapy/scrapy/issues/733) ， [issue 752](https://github.com/scrapy/scrapy/issues/752) ）
*   文件 `spider.closed()` 快捷方式（捷径” [issue 719](https://github.com/scrapy/scrapy/issues/719) ）
*   文件 `request_scheduled` 信号（信号） [issue 746](https://github.com/scrapy/scrapy/issues/746) ）
*   添加有关报告安全问题的说明（ [issue 697](https://github.com/scrapy/scrapy/issues/697) ）
*   添加LevelDB HTTP缓存存储后端（ [issue 626](https://github.com/scrapy/scrapy/issues/626) ， [issue 500](https://github.com/scrapy/scrapy/issues/500) ）
*   排序 Spider 列表输出 `scrapy list` 命令（ [issue 742](https://github.com/scrapy/scrapy/issues/742) ）
*   多文档增强和修复（ [issue 575](https://github.com/scrapy/scrapy/issues/575) ， [issue 587](https://github.com/scrapy/scrapy/issues/587) ， [issue 590](https://github.com/scrapy/scrapy/issues/590) ， [issue 596](https://github.com/scrapy/scrapy/issues/596) ， [issue 610](https://github.com/scrapy/scrapy/issues/610) ， [issue 617](https://github.com/scrapy/scrapy/issues/617) ， [issue 618](https://github.com/scrapy/scrapy/issues/618) ， [issue 627](https://github.com/scrapy/scrapy/issues/627) ， [issue 613](https://github.com/scrapy/scrapy/issues/613) ， [issue 643](https://github.com/scrapy/scrapy/issues/643) ， [issue 654](https://github.com/scrapy/scrapy/issues/654) ， [issue 675](https://github.com/scrapy/scrapy/issues/675) ， [issue 663](https://github.com/scrapy/scrapy/issues/663) ， [issue 711](https://github.com/scrapy/scrapy/issues/711) ， [issue 714](https://github.com/scrapy/scrapy/issues/714) ）

### 错误修正

*   在regexlinkextractor中创建链接时编码unicode url值（ [issue 561](https://github.com/scrapy/scrapy/issues/561) ）
*   忽略项加载器处理器中的无值（ [issue 556](https://github.com/scrapy/scrapy/issues/556) ）
*   当sgmlinkxtractor和htmlparserlinkextractor中存在内部标记时修复链接文本（ [issue 485](https://github.com/scrapy/scrapy/issues/485) ， [issue 574](https://github.com/scrapy/scrapy/issues/574) ）
*   修复对已弃用类的子类的错误检查（ [issue 581](https://github.com/scrapy/scrapy/issues/581) ， [issue 584](https://github.com/scrapy/scrapy/issues/584) ）
*   处理由inspect.stack（）失败引起的错误（ [issue 582](https://github.com/scrapy/scrapy/issues/582) ）
*   修复对不存在的引擎属性的引用（ [issue 593](https://github.com/scrapy/scrapy/issues/593) ， [issue 594](https://github.com/scrapy/scrapy/issues/594) ）
*   修复类型（）的动态项类示例用法（ [issue 603](https://github.com/scrapy/scrapy/issues/603) ）
*   使用lucasdemarchi/codespell修复拼写错误（ [issue 628](https://github.com/scrapy/scrapy/issues/628) ）
*   将sgmlinkextractor中attrs参数的默认值固定为tuple（ [issue 661](https://github.com/scrapy/scrapy/issues/661) ）
*   修复站点地图阅读器中的XXE缺陷（ [issue 676](https://github.com/scrapy/scrapy/issues/676) ）
*   修复引擎以支持筛选的启动请求（ [issue 707](https://github.com/scrapy/scrapy/issues/707) ）
*   在没有主机名的URL上修复非现场中间件案例（ [issue 745](https://github.com/scrapy/scrapy/issues/745) ）
*   测试套件不再需要PIL（ [issue 585](https://github.com/scrapy/scrapy/issues/585) ）

## Scrapy 0.22.2（2014-02-14发布）

*   修复对不存在的engine.slots的引用。关闭α593 [commit 13c099a](https://github.com/scrapy/scrapy/commit/13c099a) ）
*   下载ermw-doc-typo（spidermw-doc-copy-remark）（ [commit 8ae11bf](https://github.com/scrapy/scrapy/commit/8ae11bf) ）
*   正确的拼写错误 [commit 1346037](https://github.com/scrapy/scrapy/commit/1346037) ）

## Scrapy 0.22.1（2014-02-08发布）

*   localhost666在某些情况下可以解决（ [commit 2ec2279](https://github.com/scrapy/scrapy/commit/2ec2279) ）
*   测试检查。堆栈故障（ [commit cc3eda3](https://github.com/scrapy/scrapy/commit/cc3eda3) ）
*   当inspect.stack（）失败时处理案例（ [commit 8cb44f9](https://github.com/scrapy/scrapy/commit/8cb44f9) ）
*   修复对已弃用类的子类的错误检查。关闭α581 [commit 46d98d6](https://github.com/scrapy/scrapy/commit/46d98d6) ）
*   文档：最终spider示例的4空间缩进（ [commit 13846de](https://github.com/scrapy/scrapy/commit/13846de) ）
*   修复htmlparserlinktextractor并在485合并后进行测试（ [commit 368a946](https://github.com/scrapy/scrapy/commit/368a946) ）
*   basesgmlinkextractor：修复了链接具有内部标记时缺少的空间（ [commit b566388](https://github.com/scrapy/scrapy/commit/b566388) ）
*   basesgmlinkextractor：添加带有内部标记的链接的单元测试（ [commit c1cb418](https://github.com/scrapy/scrapy/commit/c1cb418) ）
*   basesgmlinkextractor:修复了未知的_end tag（），以便在结束标记与开始标记匹配时只设置当前的_link=none（ [commit 7e4d627](https://github.com/scrapy/scrapy/commit/7e4d627) ）
*   修复Travis CI构建的测试（ [commit 76c7e20](https://github.com/scrapy/scrapy/commit/76c7e20) ）
*   用HTML实体替换不可编码的代码点。修复562和285（ [commit 5f87b17](https://github.com/scrapy/scrapy/commit/5f87b17) ）
*   regexlinkextractor:创建链接时编码URL Unicode值（ [commit d0ee545](https://github.com/scrapy/scrapy/commit/d0ee545) ）
*   用最新的输出更新了教程的爬行输出。（ [commit 8da65de](https://github.com/scrapy/scrapy/commit/8da65de) ）
*   使用爬虫引用更新了shell文档，并修复了实际shell输出。（ [commit 875b9ab](https://github.com/scrapy/scrapy/commit/875b9ab) ）
*   PEP8小编辑。（ [commit f89efaf](https://github.com/scrapy/scrapy/commit/f89efaf) ）
*   将当前爬行器暴露在 Scrapy 壳中。（ [commit 5349cec](https://github.com/scrapy/scrapy/commit/5349cec) ）
*   未使用的重新导入和PEP8小编辑。（ [commit 387f414](https://github.com/scrapy/scrapy/commit/387f414) ）
*   使用itemloader时忽略none的值。（ [commit 0632546](https://github.com/scrapy/scrapy/commit/0632546) ）
*   Doc修复了默认值中的httpcache_存储错误，该默认值现在是filesystem而不是dbm。（ [commit cde9a8c](https://github.com/scrapy/scrapy/commit/cde9a8c) ）
*   将Ubuntu安装指令显示为文本代码（ [commit fb5c9c5](https://github.com/scrapy/scrapy/commit/fb5c9c5) ）
*   更新Ubuntu安装说明（ [commit 70fb105](https://github.com/scrapy/scrapy/commit/70fb105) ）
*   合并请求550来自Missist Leone/Patch-1（ [commit 6f70b6a](https://github.com/scrapy/scrapy/commit/6f70b6a) ）
*   修改scrappy-ubuntu包的版本（ [commit 725900d](https://github.com/scrapy/scrapy/commit/725900d) ）
*   确定0.22.0发布日期（ [commit af0219a](https://github.com/scrapy/scrapy/commit/af0219a) ）
*   修复news.rst中的拼写错误并删除（尚未发布）标题（ [commit b7f58f4](https://github.com/scrapy/scrapy/commit/b7f58f4) ）

## Scrapy 0.22.0（2014-01-17发布）

### 增强功能

*   [**向后不兼容**]将httpcachemiddleware后端切换到文件系统（ [issue 541](https://github.com/scrapy/scrapy/issues/541) ）还原旧的后端集 `HTTPCACHE_STORAGE` 到 `scrapy.contrib.httpcache.DbmCacheStorage`
*   使用connect方法的代理服务器https://urls（ [issue 392](https://github.com/scrapy/scrapy/issues/392) ， [issue 397](https://github.com/scrapy/scrapy/issues/397) ）
*   添加一个中间件来对由Google定义的Ajax可爬行页面进行爬行。（ [issue 343](https://github.com/scrapy/scrapy/issues/343) ）
*   将scrapy.spider.basespider重命名为scrapy.spider.spider（ [issue 510](https://github.com/scrapy/scrapy/issues/510) ， [issue 519](https://github.com/scrapy/scrapy/issues/519) ）
*   选择器默认注册exslt命名空间（ [issue 472](https://github.com/scrapy/scrapy/issues/472) ）
*   统一与选择器重命名类似的项加载器（ [issue 461](https://github.com/scrapy/scrapy/issues/461) ）
*   制作 `RFPDupeFilter` 类容易子类化（ [issue 533](https://github.com/scrapy/scrapy/issues/533) ）
*   提高测试覆盖率和即将推出的python 3支持（ [issue 525](https://github.com/scrapy/scrapy/issues/525) ）
*   将设置和中间件的启动信息提升到信息级别（ [issue 520](https://github.com/scrapy/scrapy/issues/520) ）
*   支持部分 `get_func_args` 乌蒂尔 [issue 506](https://github.com/scrapy/scrapy/issues/506) ，问题：“504”
*   允许通过tox运行独立测试（ [issue 503](https://github.com/scrapy/scrapy/issues/503) ）
*   链接提取程序忽略了更新扩展（ [issue 498](https://github.com/scrapy/scrapy/issues/498) ）
*   添加中间件方法以获取文件/图像/拇指路径（ [issue 490](https://github.com/scrapy/scrapy/issues/490) ）
*   改进非现场中间件测试（ [issue 478](https://github.com/scrapy/scrapy/issues/478) ）
*   添加一种跳过由refermiddleware设置的默认referer头的方法（ [issue 475](https://github.com/scrapy/scrapy/issues/475) ）
*   请勿发送 `x-gzip` 默认情况下 `Accept-Encoding` 报头（ [issue 469](https://github.com/scrapy/scrapy/issues/469) ）
*   支持使用设置定义HTTP错误处理（ [issue 466](https://github.com/scrapy/scrapy/issues/466) ）
*   使用现代的python习惯用法，无论你在哪里找到遗产（ [issue 497](https://github.com/scrapy/scrapy/issues/497) ）
*   改进和更正文档（ [issue 527](https://github.com/scrapy/scrapy/issues/527) ， [issue 524](https://github.com/scrapy/scrapy/issues/524) ， [issue 521](https://github.com/scrapy/scrapy/issues/521) ， [issue 517](https://github.com/scrapy/scrapy/issues/517) ， [issue 512](https://github.com/scrapy/scrapy/issues/512) ， [issue 505](https://github.com/scrapy/scrapy/issues/505) ， [issue 502](https://github.com/scrapy/scrapy/issues/502) ， [issue 489](https://github.com/scrapy/scrapy/issues/489) ， [issue 465](https://github.com/scrapy/scrapy/issues/465) ， [issue 460](https://github.com/scrapy/scrapy/issues/460) ， [issue 425](https://github.com/scrapy/scrapy/issues/425) ， [issue 536](https://github.com/scrapy/scrapy/issues/536) ）

### 修正

*   更新Crawlspiper模板中的选择器类导入（ [issue 484](https://github.com/scrapy/scrapy/issues/484) ）
*   修复不存在的引用 `engine.slots` （ [issue 464](https://github.com/scrapy/scrapy/issues/464) ）
*   不要调用 `body_as_unicode()` 在非文本响应实例上（ [issue 462](https://github.com/scrapy/scrapy/issues/462) ）
*   在XpathitemLoader子类化时发出警告，以前它只在实例化时发出警告。（ [issue 523](https://github.com/scrapy/scrapy/issues/523) ）
*   在XpathSelector子类化时发出警告，以前它只在实例化时发出警告。（ [issue 537](https://github.com/scrapy/scrapy/issues/537) ）
*   对内存状态的多个修复（ [issue 531](https://github.com/scrapy/scrapy/issues/531) ， [issue 530](https://github.com/scrapy/scrapy/issues/530) ， [issue 529](https://github.com/scrapy/scrapy/issues/529) ）
*   修复中的重写URL `FormRequest.from_response()` （ [issue 507](https://github.com/scrapy/scrapy/issues/507) ）
*   在PIP 1.5下修复测试运行程序（ [issue 513](https://github.com/scrapy/scrapy/issues/513) ）
*   修复spider名称为unicode时的日志记录错误（ [issue 479](https://github.com/scrapy/scrapy/issues/479) ）

## Scrapy 0.20.2（2013-12-09发布）

*   使用选择器更改更新Crawlspiper模板（ [commit 6d1457d](https://github.com/scrapy/scrapy/commit/6d1457d) ）
*   在教程中修复方法名。关闭GH-480（GH-480） [commit b4fc359](https://github.com/scrapy/scrapy/commit/b4fc359)

## Scrapy 0.20.1（2013-11-28发布）

*   包含u软件包u从发布的源代码构建车轮需要数据（ [commit 5ba1ad5](https://github.com/scrapy/scrapy/commit/5ba1ad5) ）
*   进程并行正在泄漏内部延迟的故障。关闭α458 [commit 419a780](https://github.com/scrapy/scrapy/commit/419a780) ）

## Scrapy 0.20.0（2013-11-08发布）

### 增强功能

*   新选择器的API，包括CSS选择器（ [issue 395](https://github.com/scrapy/scrapy/issues/395) 和 [issue 426](https://github.com/scrapy/scrapy/issues/426) ）
*   请求/响应URL/主体属性现在是不可变的（修改它们已经被弃用了很长时间）
*   [`ITEM_PIPELINES`](topics/settings.html#std:setting-ITEM_PIPELINES) 现在定义为dict（而不是列表）
*   SitemapSpider可以获取备用URL（ [issue 360](https://github.com/scrapy/scrapy/issues/360) ）
*   `Selector.remove_namespaces()` 现在从元素的属性中移除名称空间。（ [issue 416](https://github.com/scrapy/scrapy/issues/416) ）
*   为python 3.3铺平道路+（ [issue 435](https://github.com/scrapy/scrapy/issues/435) ， [issue 436](https://github.com/scrapy/scrapy/issues/436) ， [issue 431](https://github.com/scrapy/scrapy/issues/431) ， [issue 452](https://github.com/scrapy/scrapy/issues/452) ）
*   使用具有嵌套支持的本机python类型的新项导出器（ [issue 366](https://github.com/scrapy/scrapy/issues/366) ）
*   调整http1.1池大小，使其与设置定义的并发性匹配（ [commit b43b5f575](https://github.com/scrapy/scrapy/commit/b43b5f575) ）
*   scrappy.mail.mailsender现在可以通过tls连接或使用starttls升级（ [issue 327](https://github.com/scrapy/scrapy/issues/327) ）
*   从ImageSpipeline中分解出功能的新文件管道（ [issue 370](https://github.com/scrapy/scrapy/issues/370) ， [issue 409](https://github.com/scrapy/scrapy/issues/409) ）
*   建议用枕头代替PIL来处理图像（ [issue 317](https://github.com/scrapy/scrapy/issues/317) ）
*   为Ubuntu Quantal和Raring添加Debian软件包（ [commit 86230c0](https://github.com/scrapy/scrapy/commit/86230c0) ）
*   模拟服务器（用于测试）可以侦听HTTPS请求（ [issue 410](https://github.com/scrapy/scrapy/issues/410) ）
*   从多个核心组件上拆下多个十字轴支架（ [issue 422](https://github.com/scrapy/scrapy/issues/422) ， [issue 421](https://github.com/scrapy/scrapy/issues/421) ， [issue 420](https://github.com/scrapy/scrapy/issues/420) ， [issue 419](https://github.com/scrapy/scrapy/issues/419) ， [issue 423](https://github.com/scrapy/scrapy/issues/423) ， [issue 418](https://github.com/scrapy/scrapy/issues/418) ）
*   Travis CI现在根据开发版本测试 Scrapy 更改 `w3lib` 和 `queuelib` python包。
*   将PYPY 2.1添加到持续集成测试中（ [commit ecfa7431](https://github.com/scrapy/scrapy/commit/ecfa7431) ）
*   pylinted、pep8并从源中删除了旧样式异常（ [issue 430](https://github.com/scrapy/scrapy/issues/430) ， [issue 432](https://github.com/scrapy/scrapy/issues/432) ）
*   将importlib用于参数导入（ [issue 445](https://github.com/scrapy/scrapy/issues/445) ）
*   处理python 2.7.5中引入的影响xmlItemExporter的回归（ [issue 372](https://github.com/scrapy/scrapy/issues/372) ）
*   修正了SIGINT上的爬行关闭（ [issue 450](https://github.com/scrapy/scrapy/issues/450) ）
*   不提交 `reset` 在FormRequest.From响应中键入输入（ [commit b326b87](https://github.com/scrapy/scrapy/commit/b326b87) ）
*   当请求errback引发异常时，不要消除下载错误（ [commit 684cfc0](https://github.com/scrapy/scrapy/commit/684cfc0) ）

### 错误修正

*   在Django 1.6下修复测试（ [commit b6bed44c](https://github.com/scrapy/scrapy/commit/b6bed44c) ）
*   使用HTTP1.1下载处理程序在断开连接的情况下重试中间件的许多错误修复
*   修复扭曲释放之间的不一致（ [issue 406](https://github.com/scrapy/scrapy/issues/406) ）
*   修复废壳虫（ [issue 418](https://github.com/scrapy/scrapy/issues/418) ， [issue 407](https://github.com/scrapy/scrapy/issues/407) ）
*   修复setup.py中的无效变量名（ [issue 429](https://github.com/scrapy/scrapy/issues/429) ）
*   修复教程引用（ [issue 387](https://github.com/scrapy/scrapy/issues/387) ）
*   改进请求响应文档（ [issue 391](https://github.com/scrapy/scrapy/issues/391) ）
*   改进最佳实践文档（ [issue 399](https://github.com/scrapy/scrapy/issues/399) ， [issue 400](https://github.com/scrapy/scrapy/issues/400) ， [issue 401](https://github.com/scrapy/scrapy/issues/401) ， [issue 402](https://github.com/scrapy/scrapy/issues/402) ）
*   改进Django集成文档（ [issue 404](https://github.com/scrapy/scrapy/issues/404) ）
*   文件 `bindaddress` 请求元 [commit 37c24e01d7](https://github.com/scrapy/scrapy/commit/37c24e01d7) ）
*   改进 `Request` 类文档（ [issue 226](https://github.com/scrapy/scrapy/issues/226) ）

### 其他

*   丢弃的python 2.6支持（ [issue 448](https://github.com/scrapy/scrapy/issues/448) ）
*   添加 [cssselect](https://github.com/SimonSapin/cssselect) python包作为安装依赖项
*   删除libxml2和多选择器的后端支持， [lxml](http://lxml.de/) 从现在开始是必需的。
*   最小扭曲版本增加到10.0.0，下降扭曲8.0支持。
*   现在运行测试套件需要 `mock` python库（ [issue 390](https://github.com/scrapy/scrapy/issues/390) ）

### 谢谢

感谢所有为这次发布做出贡献的人！

按提交次数排序的参与者列表：

```py
69 Daniel Graña <dangra@...>
37 Pablo Hoffman <pablo@...>
13 Mikhail Korobov <kmike84@...>
 9 Alex Cepoi <alex.cepoi@...>
 9 alexanderlukanin13 <alexander.lukanin.13@...>
 8 Rolando Espinoza La fuente <darkrho@...>
 8 Lukasz Biedrycki <lukasz.biedrycki@...>
 6 Nicolas Ramirez <nramirez.uy@...>
 3 Paul Tremberth <paul.tremberth@...>
 2 Martin Olveyra <molveyra@...>
 2 Stefan <misc@...>
 2 Rolando Espinoza <darkrho@...>
 2 Loren Davie <loren@...>
 2 irgmedeiros <irgmedeiros@...>
 1 Stefan Koch <taikano@...>
 1 Stefan <cct@...>
 1 scraperdragon <dragon@...>
 1 Kumara Tharmalingam <ktharmal@...>
 1 Francesco Piccinno <stack.box@...>
 1 Marcos Campal <duendex@...>
 1 Dragon Dave <dragon@...>
 1 Capi Etheriel <barraponto@...>
 1 cacovsky <amarquesferraz@...>
 1 Berend Iwema <berend@...>

```

## Scrapy 0.18.4（2013-10-10发布）

*   IPython拒绝更新命名空间。FixY 396 [commit 3d32c4f](https://github.com/scrapy/scrapy/commit/3d32c4f) ）
*   修复alreadycallederror替换shell命令中的请求。关闭α407 [commit b1d8919](https://github.com/scrapy/scrapy/commit/b1d8919) ）
*   修复启动请求延迟和提前挂起（ [commit 89faf52](https://github.com/scrapy/scrapy/commit/89faf52) ）

## Scrapy 0.18.3（2013-10-03发布）

*   修复对启动请求的延迟评估的回归（ [commit 12693a5](https://github.com/scrapy/scrapy/commit/12693a5) ）
*   表单：不提交重置输入（ [commit e429f63](https://github.com/scrapy/scrapy/commit/e429f63) ）
*   增加UnitTest超时以减少Travis假阳性故障（ [commit 912202e](https://github.com/scrapy/scrapy/commit/912202e) ）
*   json导出器的后台主修复程序（ [commit cfc2d46](https://github.com/scrapy/scrapy/commit/cfc2d46) ）
*   在生成sdist tarball之前，修复权限并设置umask（ [commit 06149e0](https://github.com/scrapy/scrapy/commit/06149e0) ）

## Scrapy 0.18.2（2013-09-03发布）

*   后端 `scrapy check` 命令修复和向后兼容的多爬虫进程（ [issue 339](https://github.com/scrapy/scrapy/issues/339) ）

## Scrapy 0.18.1（2013-08-27发布）

*   删除由cherry-picked更改添加的额外导入（ [commit d20304e](https://github.com/scrapy/scrapy/commit/d20304e) ）
*   在扭曲pre 11.0.0下修复爬行测试（ [commit 1994f38](https://github.com/scrapy/scrapy/commit/1994f38) ）
*   PY26不能格式化零长度字段（ [commit abf756f](https://github.com/scrapy/scrapy/commit/abf756f) ）
*   测试未绑定响应的潜在数据丢失错误（ [commit b15470d](https://github.com/scrapy/scrapy/commit/b15470d) ）
*   将没有内容长度或传输编码的响应视为良好响应（ [commit c4bf324](https://github.com/scrapy/scrapy/commit/c4bf324) ）
*   如果未启用http11处理程序，则不包括responsefailed（ [commit 6cbe684](https://github.com/scrapy/scrapy/commit/6cbe684) ）
*   新的HTTP客户端将连接丢失包装为responsefailed异常。FixY 373 [commit 1a20bba](https://github.com/scrapy/scrapy/commit/1a20bba) ）
*   限制Travis CI构建矩阵（ [commit 3b01bb8](https://github.com/scrapy/scrapy/commit/3b01bb8) ）
*   合并请求375来自Peterarenot/Patch-1（ [commit fa766d7](https://github.com/scrapy/scrapy/commit/fa766d7) ）
*   已修复，因此它引用了正确的文件夹（ [commit 3283809](https://github.com/scrapy/scrapy/commit/3283809) ）
*   添加Quantal和Raring以支持Ubuntu版本（ [commit 1411923](https://github.com/scrapy/scrapy/commit/1411923) ）
*   修复在升级到http1客户端后没有重试某些连接错误的重试中间件，关闭GH-373（ [commit bb35ed0](https://github.com/scrapy/scrapy/commit/bb35ed0) ）
*   在python 2.7.4和2.7.5中修复xmlItemExporter（ [commit de3e451](https://github.com/scrapy/scrapy/commit/de3e451) ）
*   0.18发行说明的小更新（ [commit c45e5f1](https://github.com/scrapy/scrapy/commit/c45e5f1) ）
*   修复控件列表格式（ [commit 0b60031](https://github.com/scrapy/scrapy/commit/0b60031) ）

## Scrapy 0.18.0（2013-08-09发布）

*   使用tox对testsuite运行进行了很多改进，包括在pypi上进行测试的方法
*   处理Ajax可爬行URL的get参数（ [commit 3fe2a32](https://github.com/scrapy/scrapy/commit/3fe2a32) ）
*   使用lxml recover选项分析站点地图（ [issue 347](https://github.com/scrapy/scrapy/issues/347) ）
*   错误修复cookie按主机名而不是按netloc合并（ [issue 352](https://github.com/scrapy/scrapy/issues/352) ）
*   支持禁用 `HttpCompressionMiddleware` 使用标志设置（ [issue 359](https://github.com/scrapy/scrapy/issues/359) ）
*   使用支持XML命名空间 `iternodes` 语法分析器 `XMLFeedSpider` （ [issue 12](https://github.com/scrapy/scrapy/issues/12) ）
*   支持 `dont_cache` 请求元标志（ [issue 19](https://github.com/scrapy/scrapy/issues/19) ）
*   修正错误 `scrapy.utils.gz.gunzip` 被python 2.7.4中的更改打断（ [commit 4dc76e](https://github.com/scrapy/scrapy/commit/4dc76e) ）
*   错误修复上的URL编码 `SgmlLinkExtractor` （ [issue 24](https://github.com/scrapy/scrapy/issues/24) ）
*   修正错误 `TakeFirst` 处理器不应丢弃零（0）值（ [issue 59](https://github.com/scrapy/scrapy/issues/59) ）
*   支持XML导出器中的嵌套项（ [issue 66](https://github.com/scrapy/scrapy/issues/66) ）
*   提高cookie处理性能（ [issue 77](https://github.com/scrapy/scrapy/issues/77) ）
*   记录重复筛选的请求一次（ [issue 105](https://github.com/scrapy/scrapy/issues/105) ）
*   将重定向中间件拆分为状态中间件和基于元的中间件（ [issue 78](https://github.com/scrapy/scrapy/issues/78) ）
*   使用http1.1作为默认的下载程序处理程序（ [issue 109](https://github.com/scrapy/scrapy/issues/109) 和 [issue 318](https://github.com/scrapy/scrapy/issues/318) ）
*   支持上的XPath表单选择 `FormRequest.from_response` （ [issue 185](https://github.com/scrapy/scrapy/issues/185) ）
*   修正上的Unicode解码错误 `SgmlLinkExtractor` （ [issue 199](https://github.com/scrapy/scrapy/issues/199) ）
*   Pypi解释器上的错误修复信号调度（ [issue 205](https://github.com/scrapy/scrapy/issues/205) ）
*   改进请求延迟和并发处理（ [issue 206](https://github.com/scrapy/scrapy/issues/206) ）
*   将rfc2616缓存策略添加到 `HttpCacheMiddleware` （ [issue 212](https://github.com/scrapy/scrapy/issues/212) ）
*   允许自定义引擎记录的消息（ [issue 214](https://github.com/scrapy/scrapy/issues/214) ）
*   多方面的改进 `DjangoItem` （ [issue 217](https://github.com/scrapy/scrapy/issues/217) ， [issue 218](https://github.com/scrapy/scrapy/issues/218) ， [issue 221](https://github.com/scrapy/scrapy/issues/221) ）
*   使用SETUPTOOLS入口点扩展废料命令（ [issue 260](https://github.com/scrapy/scrapy/issues/260) ）
*   允许 Spider `allowed_domains` 要设置的值/元组（ [issue 261](https://github.com/scrapy/scrapy/issues/261) ）
*   支持 `settings.getdict` （ [issue 269](https://github.com/scrapy/scrapy/issues/269) ）
*   简化内部 `scrapy.core.scraper` 插槽处理 [issue 271](https://github.com/scrapy/scrapy/issues/271) ）
*   补充 `Item.copy` （ [issue 290](https://github.com/scrapy/scrapy/issues/290) ）
*   收集空闲下载器插槽（ [issue 297](https://github.com/scrapy/scrapy/issues/297) ）
*   添加 `ftp://` 方案下载程序处理程序（ [issue 329](https://github.com/scrapy/scrapy/issues/329) ）
*   添加了Downloader Benchmark Web服务器和Spider工具 [Benchmarking](topics/benchmarking.html#benchmarking)
*   已将持久（磁盘上）队列移动到单独的项目（queuelib_u），而该项目现在依赖于
*   使用外部库添加 Scrapy 命令（ [issue 260](https://github.com/scrapy/scrapy/issues/260) ）
*   补充 `--pdb` 选择权 `scrapy` 命令行工具
*   补充 `XPathSelector.remove_namespaces()` 它允许从XML文档中删除所有名称空间以方便（使用不含名称空间的xpaths）。记录在 [选择器](topics/selectors.html#topics-selectors) .
*   Spider 合约的几个改进
*   名为metarefreshmiddldeware的新默认中间件，用于处理meta refresh html标记重定向，
*   MetaRefreshMiddlDeware和RedirectMiddleware有不同的优先级来解决62
*   从爬虫方法添加到 Spider
*   使用模拟服务器添加系统测试
*   Mac OS兼容性的更多改进（感谢Alex Cepoi）
*   多个单件清洁和多 Spider 支持（感谢Nicolas Ramirez）
*   支持自定义下载插槽
*   在“shell”命令中添加了--spider选项。
*   当Scrapy启动时记录覆盖的设置

感谢所有为这次发布做出贡献的人。以下是按提交次数排序的参与者列表：

```py
130 Pablo Hoffman <pablo@...>
 97 Daniel Graña <dangra@...>
 20 Nicolás Ramírez <nramirez.uy@...>
 13 Mikhail Korobov <kmike84@...>
 12 Pedro Faustino <pedrobandim@...>
 11 Steven Almeroth <sroth77@...>
  5 Rolando Espinoza La fuente <darkrho@...>
  4 Michal Danilak <mimino.coder@...>
  4 Alex Cepoi <alex.cepoi@...>
  4 Alexandr N Zamaraev (aka tonal) <tonal@...>
  3 paul <paul.tremberth@...>
  3 Martin Olveyra <molveyra@...>
  3 Jordi Llonch <llonchj@...>
  3 arijitchakraborty <myself.arijit@...>
  2 Shane Evans <shane.evans@...>
  2 joehillen <joehillen@...>
  2 Hart <HartSimha@...>
  2 Dan <ellisd23@...>
  1 Zuhao Wan <wanzuhao@...>
  1 whodatninja <blake@...>
  1 vkrest <v.krestiannykov@...>
  1 tpeng <pengtaoo@...>
  1 Tom Mortimer-Jones <tom@...>
  1 Rocio Aramberri <roschegel@...>
  1 Pedro <pedro@...>
  1 notsobad <wangxiaohugg@...>
  1 Natan L <kuyanatan.nlao@...>
  1 Mark Grey <mark.grey@...>
  1 Luan <luanpab@...>
  1 Libor Nenadál <libor.nenadal@...>
  1 Juan M Uys <opyate@...>
  1 Jonas Brunsgaard <jonas.brunsgaard@...>
  1 Ilya Baryshev <baryshev@...>
  1 Hasnain Lakhani <m.hasnain.lakhani@...>
  1 Emanuel Schorsch <emschorsch@...>
  1 Chris Tilden <chris.tilden@...>
  1 Capi Etheriel <barraponto@...>
  1 cacovsky <amarquesferraz@...>
  1 Berend Iwema <berend@...>

```

## Scrapy 0.16.5（2013-05-30发布）

*   当Scrapy Deploy重定向到新的端点时，遵守请求方法（ [commit 8c4fcee](https://github.com/scrapy/scrapy/commit/8c4fcee) ）
*   修复不准确的下载器中间件文档。参考文献280 [commit 40667cb](https://github.com/scrapy/scrapy/commit/40667cb) ）
*   文档：删除diveintopython.org的链接，该链接不再可用。关闭α246 [commit bd58bfa](https://github.com/scrapy/scrapy/commit/bd58bfa) ）
*   在无效的HTML5文档中查找表单节点（ [commit e3d6945](https://github.com/scrapy/scrapy/commit/e3d6945) ）
*   修正了错误的标签属性类型bool而不是list（ [commit a274276](https://github.com/scrapy/scrapy/commit/a274276) ）

## Scrapy 0.16.4（2013-01-23发布）

*   修复文档中的拼写错误（ [commit 6d2b3aa](https://github.com/scrapy/scrapy/commit/6d2b3aa) ）
*   添加关于禁用扩展的文档。参考文献132 [commit c90de33](https://github.com/scrapy/scrapy/commit/c90de33) ）
*   已修复错误消息格式。log.err（）不支持酷格式，出现错误时，消息为：“错误：错误处理%（item）s”（ [commit c16150c](https://github.com/scrapy/scrapy/commit/c16150c) ）
*   整理和改进图像管道错误记录（ [commit 56b45fc](https://github.com/scrapy/scrapy/commit/56b45fc) ）
*   固定文档错误（ [commit 243be84](https://github.com/scrapy/scrapy/commit/243be84) ）
*   添加文档主题：广泛的爬行和常见实践（ [commit 1fbb715](https://github.com/scrapy/scrapy/commit/1fbb715) ）
*   当没有显式指定spider时，修复scrapy parse命令中的错误。关闭α209 [commit c72e682](https://github.com/scrapy/scrapy/commit/c72e682) ）
*   更新docs/topics/commands.rst（ [commit 28eac7a](https://github.com/scrapy/scrapy/commit/28eac7a) ）

## Scrapy 0.16.3（2012-12-07发布）

*   在使用下载延迟时删除并发限制，并仍然确保强制执行请求间延迟（ [commit 487b9b5](https://github.com/scrapy/scrapy/commit/487b9b5) ）
*   当图像管道失败时添加错误详细信息（ [commit 8232569](https://github.com/scrapy/scrapy/commit/8232569) ）
*   改善Mac OS兼容性（ [commit 8dcf8aa](https://github.com/scrapy/scrapy/commit/8dcf8aa) ）
*   setup.py:使用readme.rst填充long_描述（ [commit 7b5310d](https://github.com/scrapy/scrapy/commit/7b5310d) ）
*   文档：删除了对ClientForm的过时引用（ [commit 80f9bb6](https://github.com/scrapy/scrapy/commit/80f9bb6) ）
*   为默认存储后端更正文档（ [commit 2aa491b](https://github.com/scrapy/scrapy/commit/2aa491b) ）
*   文档：从常见问题解答中删除了断开的proxyhub链接（ [commit bdf61c4](https://github.com/scrapy/scrapy/commit/bdf61c4) ）
*   SpiderOpenCloseLogging示例中的固定文档拼写错误（ [commit 7184094](https://github.com/scrapy/scrapy/commit/7184094) ）

## Scrapy 0.16.2（2012-11-09发布）

*   废料合同：python2.6兼容（ [commit a4a9199](https://github.com/scrapy/scrapy/commit/a4a9199) ）
*   残缺合同详细选项（ [commit ec41673](https://github.com/scrapy/scrapy/commit/ec41673) ）
*   适当的单元测试，如残缺合同的输出（ [commit 86635e4](https://github.com/scrapy/scrapy/commit/86635e4) ）
*   在调试文档中添加了“在浏览器中打开”（ [commit c9b690d](https://github.com/scrapy/scrapy/commit/c9b690d) ）
*   已从设置文档中删除对全局 Scrapy 状态的引用（ [commit dd55067](https://github.com/scrapy/scrapy/commit/dd55067) ）
*   修复Windows平台中的spiderstate错误（ [commit 58998f4](https://github.com/scrapy/scrapy/commit/58998f4) ）

## Scrapy 0.16.1（2012-10-26发布）

*   修复了logstats扩展，它在0.16版本之前的错误合并后被破坏。（ [commit 8c780fd](https://github.com/scrapy/scrapy/commit/8c780fd) ）
*   更好地向后兼容scrapy.conf.settings（ [commit 3403089](https://github.com/scrapy/scrapy/commit/3403089) ）
*   有关如何从扩展访问爬虫统计信息的扩展文档（ [commit c4da0b5](https://github.com/scrapy/scrapy/commit/c4da0b5) ）
*   删除了.hgtags（现在scriby使用git就不再需要了）（ [commit d52c188](https://github.com/scrapy/scrapy/commit/d52c188) ）
*   固定RST标题下的破折号（ [commit fa4f7f9](https://github.com/scrapy/scrapy/commit/fa4f7f9) ）
*   在新闻中设置0.16.0的发布日期（ [commit e292246](https://github.com/scrapy/scrapy/commit/e292246) ）

## Scrapy 0.16.0（2012-10-18发布）

Scrapy 变化：

*   补充 [Spider 合约](topics/contracts.html#topics-contracts) 以正式/可复制的方式测试 Spider 的机制。
*   增加选项 `-o` 和 `-t` 到 [`runspider`](topics/commands.html#std:command-runspider) 命令
*   文件化的 [AutoThrottle 扩展](topics/autothrottle.html) 并添加到默认安装的扩展。您仍然需要启用它 [`AUTOTHROTTLE_ENABLED`](topics/autothrottle.html#std:setting-AUTOTHROTTLE_ENABLED)
*   主要统计数据收集重构：删除全局/每个 Spider 统计数据的分离，删除与统计数据相关的信号（ `stats_spider_opened` 等）。统计信息现在要简单得多，在统计信息收集器API和信号上保持向后兼容性。
*   补充 `process_start_requests()` Spider 中间商的方法
*   信号单件丢失。现在应该通过crawler.signals属性接受信号。有关更多信息，请参阅信号文档。
*   信号单件丢失。现在应该通过crawler.signals属性接受信号。有关更多信息，请参阅信号文档。
*   删除了统计收集器singleton。现在可以通过crawler.stats属性访问状态。有关详细信息，请参阅统计信息收集文档。
*   文件化的 [核心API](topics/api.html#topics-api)
*   `lxml` is now the default selectors backend instead of `libxml2`
*   将formRequest.from_response（）移植到 [lxml](http://lxml.de/) 而不是 [ClientForm](http://wwwsearch.sourceforge.net/old/ClientForm/)
*   删除的模块： `scrapy.xlib.BeautifulSoup` 和 `scrapy.xlib.ClientForm`
*   SiteMapSpider:添加了对以.xml和.xml.gz结尾的站点地图URL的支持，即使它们公布了错误的内容类型（ [commit 10ed28b](https://github.com/scrapy/scrapy/commit/10ed28b) ）
*   stacktracedump扩展：同时转储trackref活动引用（ [commit fe2ce93](https://github.com/scrapy/scrapy/commit/fe2ce93) ）
*   现在JSON和JSONLINES导出器完全支持嵌套项
*   补充 [`cookiejar`](topics/downloader-middleware.html#std:reqmeta-cookiejar) 请求meta-key以支持每个spider的多个cookie会话
*   去耦编码检测码 [w3lib.encoding](https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py) 并移植了一些垃圾代码以使用该模块
*   放弃了对python 2.5的支持。见https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/
*   扭曲2.5的下降支架
*   补充 [`REFERER_ENABLED`](topics/spider-middleware.html#std:setting-REFERER_ENABLED) 设置，控制引用中间件
*   已将默认用户代理更改为： `Scrapy/VERSION (+http://scrapy.org)`
*   已删除（未记录） `HTMLImageLinkExtractor` 类从 `scrapy.contrib.linkextractors.image`
*   根据 Spider 设置删除（替换为实例化多个爬行器对象）
*   `USER_AGENT` Spider 属性将不再工作，请使用 `user_agent` 改为属性
*   `DOWNLOAD_TIMEOUT` Spider 属性将不再工作，请使用 `download_timeout` 改为属性
*   远离的 `ENCODING_ALIASES` 设置，因为编码自动检测已移动到 [w3lib](https://github.com/scrapy/w3lib) 类库
*   促进 [DjangoItem](topics/djangoitem.html#topics-djangoitem) 对主控
*   logformatter方法现在返回dict（而不是字符串）以支持惰性格式。（ [issue 164](https://github.com/scrapy/scrapy/issues/164) ， [commit dcef7b0](https://github.com/scrapy/scrapy/commit/dcef7b0) ）
*   下载程序处理程序（ [`DOWNLOAD_HANDLERS`](topics/settings.html#std:setting-DOWNLOAD_HANDLERS) setting）现在接收设置作为构造函数的第一个参数
*   已将内存使用率替换为（更便携） [resource](https://docs.python.org/2/library/resource.html) 移除模块 `scrapy.utils.memory` 模块
*   删除信号： `scrapy.mail.mail_sent`
*   远离的 `TRACK_REFS` 设置，现在 [trackrefs](topics/leaks.html#topics-leaks-trackrefs) 始终启用
*   DBM现在是HTTP缓存中间件的默认存储后端
*   日志消息的数量（每个级别）现在通过碎片统计（stat name: `log_count/LEVEL` ）
*   接收到的响应数现在通过scrapy stats（stat name: `response_received_count` ）
*   远离的 `scrapy.log.started` 属性

## Scrapy 0.144

*   为支持的Ubuntu发行版增加了精确性（ [commit b7e46df](https://github.com/scrapy/scrapy/commit/b7e46df) ）
*   修复了在https://groups.google.com/forum/中报告的json-rpc-webservice中的错误！主题/垃圾用户/QGVBMFYBNAQ/讨论。也从extras/scrapy-ws.py中删除了不再支持的“run”命令（ [commit 340fbdb](https://github.com/scrapy/scrapy/commit/340fbdb) ）
*   内容类型http equiv的元标记属性可以是任意顺序。（123） [commit 0cb68af](https://github.com/scrapy/scrapy/commit/0cb68af) ）
*   将“导入图像”替换为更标准的“从PIL导入图像”。关闭α88 [commit 4d17048](https://github.com/scrapy/scrapy/commit/4d17048) ）
*   将试用状态返回为bin/runtests.sh exit值。（118） [commit b7b2e7f](https://github.com/scrapy/scrapy/commit/b7b2e7f) ）

## Scrapy 0.143

*   忘记包含PyDispatch许可证。（118） [commit fd85f9c](https://github.com/scrapy/scrapy/commit/fd85f9c) ）
*   包括testsuite在源分发中使用的egg文件。（118） [commit c897793](https://github.com/scrapy/scrapy/commit/c897793) ）
*   更新项目模板中的docstring以避免与genspider命令混淆，这可能被视为高级功能。参考文献107 [commit 2548dcc](https://github.com/scrapy/scrapy/commit/2548dcc) ）
*   在docs/topics/firebug.rst中添加了关于关闭google目录的注释（ [commit 668e352](https://github.com/scrapy/scrapy/commit/668e352) ）
*   空的时候不要丢弃插槽，只需保存在另一个dict中，以便在需要时再次回收。（ [commit 8e9f607](https://github.com/scrapy/scrapy/commit/8e9f607) ）
*   在支持libxml2的选择器中处理unicode xpaths不会失败（ [commit b830e95](https://github.com/scrapy/scrapy/commit/b830e95) ）
*   修正了请求对象文档中的小错误（ [commit bf3c9ee](https://github.com/scrapy/scrapy/commit/bf3c9ee) ）
*   修复了链接提取器文档中的次要缺陷（ [commit ba14f38](https://github.com/scrapy/scrapy/commit/ba14f38) ）
*   删除了一些与Scrapy中的sqlite支持相关的过时的剩余代码（ [commit 0665175](https://github.com/scrapy/scrapy/commit/0665175) ）

## Scrapy 0.142

*   在计算校验和之前，移动指向文件开头的缓冲区。参考文献92 [commit 6a5bef2](https://github.com/scrapy/scrapy/commit/6a5bef2) ）
*   在保存图像之前计算图像校验和。关闭α92 [commit 9817df1](https://github.com/scrapy/scrapy/commit/9817df1) ）
*   删除缓存失败中的泄漏引用（ [commit 673a120](https://github.com/scrapy/scrapy/commit/673a120) ）
*   修正了memoryusage扩展中的错误：get_engine_status（）只接受1个参数（给定0）（ [commit 11133e9](https://github.com/scrapy/scrapy/commit/11133e9) ）
*   修复了HTTP压缩中间件上的struct.error。关闭α87 [commit 1423140](https://github.com/scrapy/scrapy/commit/1423140) ）
*   Ajax爬网没有扩展Unicode URL（ [commit 0de3fb4](https://github.com/scrapy/scrapy/commit/0de3fb4) ）
*   catch start_请求迭代器错误。参考文献83 [commit 454a21d](https://github.com/scrapy/scrapy/commit/454a21d) ）
*   加速libxml2 xpathselector（ [commit 2fbd662](https://github.com/scrapy/scrapy/commit/2fbd662) ）
*   根据最近的更改更新版本文档（ [commit 0a070f5](https://github.com/scrapy/scrapy/commit/0a070f5) ）
*   Scrapy ：固定文档链接（ [commit 2b4e4c3](https://github.com/scrapy/scrapy/commit/2b4e4c3) ）
*   extras/makedeb.py:不再从git获取版本（ [commit caffe0e](https://github.com/scrapy/scrapy/commit/caffe0e) ）

## Scrapy 0.141

*   extras/makedeb.py:不再从git获取版本（ [commit caffe0e](https://github.com/scrapy/scrapy/commit/caffe0e) ）
*   缓冲版本为0.14.1（ [commit 6cb9e1c](https://github.com/scrapy/scrapy/commit/6cb9e1c) ）
*   修复了对教程目录的引用（ [commit 4b86bd6](https://github.com/scrapy/scrapy/commit/4b86bd6) ）
*   文档：从request.replace（）中删除了重复的回调参数（ [commit 1aeccdd](https://github.com/scrapy/scrapy/commit/1aeccdd) ）
*   固定 Scrapy 单格式（ [commit 8bf19e6](https://github.com/scrapy/scrapy/commit/8bf19e6) ）
*   为所有正在运行的线程转储堆栈并修复StackTraceDump扩展转储的引擎状态（ [commit 14a8e6e](https://github.com/scrapy/scrapy/commit/14a8e6e) ）
*   添加了关于为什么我们在boto图像上传上禁用SSL的注释（ [commit 5223575](https://github.com/scrapy/scrapy/commit/5223575) ）
*   当与S3进行太多并行连接时，SSL握手挂起（ [commit 63d583d](https://github.com/scrapy/scrapy/commit/63d583d) ）
*   更改教程以跟踪dmoz网站上的更改（ [commit bcb3198](https://github.com/scrapy/scrapy/commit/bcb3198) ）
*   避免在Twisted中出现断开连接的deferred attributeerror异常&gt;=11.1.0（ [commit 98f3f87](https://github.com/scrapy/scrapy/commit/98f3f87) ）
*   允许spider设置autothrottle最大并发性（ [commit 175a4b5](https://github.com/scrapy/scrapy/commit/175a4b5) ）

## Scrapy 0.14

### 新功能和设置

*   支持 [AJAX crawleable urls](https://developers.google.com/webmasters/ajax-crawling/docs/getting-started?csw=1)
*   在磁盘上存储请求的新的永久性计划程序，允许挂起和恢复爬网（ [r2737](http://hg.scrapy.org/scrapy/changeset/2737) ）
*   补充 `-o` 选择权 `scrapy crawl` ，将刮掉的项目转储到文件（或使用 `-` ）
*   添加了对将自定义设置传递到ScrapyD的支持 `schedule.json` 原料药（API） [r2779](http://hg.scrapy.org/scrapy/changeset/2779) ， [r2783](http://hg.scrapy.org/scrapy/changeset/2783) ）
*   新的 `ChunkedTransferMiddleware` （默认启用）以支持 [chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding) ([r2769](http://hg.scrapy.org/scrapy/changeset/2769))
*   添加对S3下载器处理程序的boto 2.0支持（ [r2763](http://hg.scrapy.org/scrapy/changeset/2763) ）
*   补充 [marshal](https://docs.python.org/2/library/marshal.html) to formats supported by feed exports ([r2744](http://hg.scrapy.org/scrapy/changeset/2744))
*   在请求错误回复中，有问题的请求现在接收到 `failure.request` 属性（属性） [r2738](http://hg.scrapy.org/scrapy/changeset/2738) ）
*   ```py
    大下载重构以支持每个域/IP并发限制（  ）
    ```

    *   ```py
        CONCURRENT_REQUESTS_PER_SPIDER 设置已被弃用，并替换为：
        ```

        *   [`CONCURRENT_REQUESTS`](topics/settings.html#std:setting-CONCURRENT_REQUESTS), [`CONCURRENT_REQUESTS_PER_DOMAIN`](topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN), [`CONCURRENT_REQUESTS_PER_IP`](topics/settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP)
    *   查看文档了解更多详细信息
*   添加了内置缓存DNS解析程序（ [r2728](http://hg.scrapy.org/scrapy/changeset/2728) ）
*   将与Amazon AWS相关的组件/扩展（sqs spider queue，simpledb stats collector）移动到单独的项目：[scaws]（[https://github.com/scrapinghub/scaws](https://github.com/scrapinghub/scaws)）（ [r2706](http://hg.scrapy.org/scrapy/changeset/2706) ， [r2714](http://hg.scrapy.org/scrapy/changeset/2714) ）
*   已将spider队列移动到scrapyd: `scrapy.spiderqueue` &gt; `scrapyd.spiderqueue` （ [r2708](http://hg.scrapy.org/scrapy/changeset/2708) ）
*   已将sqlite utils移动到scrapyd: `scrapy.utils.sqlite` &gt; `scrapyd.sqlite` （ [r2781](http://hg.scrapy.org/scrapy/changeset/2781) ）
*   对返回迭代器的真正支持 `start_requests()` 方法。当 Spider 空闲时，迭代器现在在爬行过程中被消耗。（ [r2704](http://hg.scrapy.org/scrapy/changeset/2704) ）
*   补充 [`REDIRECT_ENABLED`](topics/downloader-middleware.html#std:setting-REDIRECT_ENABLED) 快速启用/禁用重定向中间件的设置（ [r2697](http://hg.scrapy.org/scrapy/changeset/2697) ）
*   补充 [`RETRY_ENABLED`](topics/downloader-middleware.html#std:setting-RETRY_ENABLED) 设置为快速启用/禁用重试中间件（ [r2694](http://hg.scrapy.org/scrapy/changeset/2694) ）
*   补充 `CloseSpider` 手动关闭星形齿轮的例外情况（ [r2691](http://hg.scrapy.org/scrapy/changeset/2691) ）
*   通过添加对HTML5元字符集声明的支持来改进编码检测（ [r2690](http://hg.scrapy.org/scrapy/changeset/2690) ）
*   重构CloseSpider行为，等待所有下载完成并由Spider处理，然后关闭Spider（ [r2688](http://hg.scrapy.org/scrapy/changeset/2688) ）
*   补充 `SitemapSpider` （见Spiders页面中的文档）（ [r2658](http://hg.scrapy.org/scrapy/changeset/2658) ）
*   补充 `LogStats` 用于定期记录基本统计信息（如已爬网页和已擦除项）的扩展（ [r2657](http://hg.scrapy.org/scrapy/changeset/2657) ）
*   使gzipped响应的处理更加可靠（319， [r2643](http://hg.scrapy.org/scrapy/changeset/2643) ）现在，scrappy将尝试尽可能多地从gzip响应中解压缩，而不是使用 `IOError` .
*   简化！memoryDebugger扩展，用于转储内存调试信息（ [r2639](http://hg.scrapy.org/scrapy/changeset/2639) ）
*   添加了编辑spider的新命令： `scrapy edit` （ [r2636](http://hg.scrapy.org/scrapy/changeset/2636) ） `-e` 旗到 `genspider` 使用它的命令（ [r2653](http://hg.scrapy.org/scrapy/changeset/2653) ）
*   已将项目的默认表示形式更改为打印精美的dict。（ [r2631](http://hg.scrapy.org/scrapy/changeset/2631) ）这提高了默认日志记录的可读性，使日志在默认情况下，既可用于刮掉的行，也可用于丢弃的行。
*   补充 [`spider_error`](topics/signals.html#std:signal-spider_error) 信号（信号） [r2628](http://hg.scrapy.org/scrapy/changeset/2628) ）
*   补充 [`COOKIES_ENABLED`](topics/downloader-middleware.html#std:setting-COOKIES_ENABLED) 设置（ [r2625](http://hg.scrapy.org/scrapy/changeset/2625) ）
*   统计信息现在被转储到 Scrapy 日志（默认值为 [`STATS_DUMP`](topics/settings.html#std:setting-STATS_DUMP) 设置已更改为 `True` ）这是为了让Scrapy用户更加了解Scrapy统计和在那里收集的数据。
*   增加了对动态调整下载延迟和最大并发请求的支持（ [r2599](http://hg.scrapy.org/scrapy/changeset/2599) ）
*   添加了新的DBM HTTP缓存存储后端（ [r2576](http://hg.scrapy.org/scrapy/changeset/2576) ）
*   补充 `listjobs.json` API到ScrapyDy（ [r2571](http://hg.scrapy.org/scrapy/changeset/2571) ）
*   `CsvItemExporter` ：增加 `join_multivalued` 参数（ [r2578](http://hg.scrapy.org/scrapy/changeset/2578) ）
*   向添加了命名空间支持 `xmliter_lxml` （ [r2552](http://hg.scrapy.org/scrapy/changeset/2552) ）
*   改进了cookies中间件 `COOKIES_DEBUG` 更好的记录它（ [r2579](http://hg.scrapy.org/scrapy/changeset/2579) ）
*   废轮胎和连杆拆卸机的几点改进

### 重新排列和删除代码

*   ```py
    合并的项传递和项抓取概念，因为它们在过去常常被证明是混淆的。这意味着：  ）
    ```

    *   原始项目刮伤信号被移除
    *   原始项目通过信号被重命名为项目刮除
    *   老原木 `Scraped Item...` 被移除
    *   老原木 `Passed Item...` 已重命名为 `Scraped Item...` 并降级至 `DEBUG` 水平
*   ```py
    通过将部分废弃代码分为两个新库来减少废弃代码库：
    ```

    *   [w3lib](https://github.com/scrapy/w3lib) （几个函数来自 `scrapy.utils.{{http,markup,multipart,response,url}}` ，在做 [r2584](http://hg.scrapy.org/scrapy/changeset/2584) ）
    *   [scrapely](https://github.com/scrapy/scrapely) 是 `scrapy.contrib.ibl` ，在做 [r2586](http://hg.scrapy.org/scrapy/changeset/2586) ）
*   删除了未使用的功能： `scrapy.utils.request.request_info()` （ [r2577](http://hg.scrapy.org/scrapy/changeset/2577) ）
*   已从中删除googledir项目 `examples/googledir` . 现在有一个新的示例项目叫做 `dirbot` GitHub上提供：[https://github.com/scrappy/dirbot](https://github.com/scrappy/dirbot)
*   已删除对 Scrapy 项目中默认字段值的支持（ [r2616](http://hg.scrapy.org/scrapy/changeset/2616) ）
*   移除实验爬行 Spider 2（ [r2632](http://hg.scrapy.org/scrapy/changeset/2632) ）
*   删除了调度程序中间件以简化体系结构。重复过滤器现在在调度程序本身中完成，使用与以前相同的重复过滤类。（ `DUPEFILTER_CLASS` 设置） [r2640](http://hg.scrapy.org/scrapy/changeset/2640) ）
*   已删除对将URL传递到的支持 `scrapy crawl` 命令（使用） `scrapy parse` 取而代之的是） [r2704](http://hg.scrapy.org/scrapy/changeset/2704) ）
*   已删除不推荐使用的执行队列（ [r2704](http://hg.scrapy.org/scrapy/changeset/2704) ）
*   已删除（未记录）spider上下文扩展（来自scrapy.contrib.spiderContext）（ [r2780](http://hg.scrapy.org/scrapy/changeset/2780) ）
*   远离的 `CONCURRENT_SPIDERS` 设置（使用ScrapyD MaxProc代替）（ [r2789](http://hg.scrapy.org/scrapy/changeset/2789) ）
*   核心组件的重命名属性：downloader.sites-&gt;downloader.slots、scraper.sites-&gt;scraper.slots（ [r2717](http://hg.scrapy.org/scrapy/changeset/2717) ， [r2718](http://hg.scrapy.org/scrapy/changeset/2718) ）
*   重命名设置 `CLOSESPIDER_ITEMPASSED` 到 [`CLOSESPIDER_ITEMCOUNT`](topics/extensions.html#std:setting-CLOSESPIDER_ITEMCOUNT) （ [r2655](http://hg.scrapy.org/scrapy/changeset/2655) ）保持向后兼容性。

## Scrapy 0.12

旧问题追踪器（trac）中的nnn参考票等数字不再可用。

### 新功能和改进

*   传递的项现在发送到 `item` 论证 `item_passed` （273）
*   向添加了详细选项 `scrapy version` 命令，用于错误报告（298）
*   HTTP缓存现在默认存储在项目数据目录中（279）
*   增加了项目数据存储目录（276，277）
*   Scrapy 项目的文档结构（见命令行工具文档）
*   xpath选择器的新lxml后端（147）
*   每个 Spider 设置（245）
*   支持退出代码，以在scrapy命令中发出错误信号（248）
*   补充 `-c` 参数 `scrapy shell` 命令
*   制造 `libxml2` 可选择的（第260）
*   新的 `deploy` 命令（第261）
*   补充 [`CLOSESPIDER_PAGECOUNT`](topics/extensions.html#std:setting-CLOSESPIDER_PAGECOUNT) 设置（α253）
*   补充 [`CLOSESPIDER_ERRORCOUNT`](topics/extensions.html#std:setting-CLOSESPIDER_ERRORCOUNT) 设置（α254）

### 抓取变化

*   ScrapyD现在每个 Spider 使用一个进程
*   它为每个 Spider 运行存储一个日志文件，并将其旋转以保持每个 Spider 最新的5个日志（默认情况下）
*   添加了一个最小的Web UI，默认情况下可从http://localhost:6800获得。
*   现在有一个 `scrapy server` 启动当前项目的ScrapyD服务器的命令

### 对设置的更改

*   补充 `HTTPCACHE_ENABLED` 设置（默认为false）以启用HTTP缓存中间件
*   改变 `HTTPCACHE_EXPIRATION_SECS` 语义：现在零意味着“永不过期”。

### 弃用/废弃功能

*   已弃用 `runserver` 有利于…的命令 `server` 启动ScrapyD服务器的命令。另请参见：ScrapyD更改
*   已弃用 `queue` 有利于使用ScrapyD的命令 `schedule.json` 应用程序编程接口。另请参见：ScrapyD更改
*   移除了！lxmlitemloader（从未升级到主控件的实验控件）

## Scrapy 0.10

旧问题追踪器（trac）中的nnn参考票等数字不再可用。

### 新功能和改进

*   调用了新的Scrapy服务 `scrapyd` 用于在生产中部署 Scrapy 爬虫（218）（提供文档）
*   简化的图像管道使用，现在无需对自己的图像管道进行子类化（217）
*   Scrapy Shell现在默认显示Scrapy日志（206）
*   重构公共基本代码中的执行队列和称为“spider队列”的可插拔后端（220）
*   新的持久 Spider 队列（基于sqlite）（198），默认情况下可用，允许在服务器模式下启动scrappy，然后安排 Spider 运行。
*   添加了scrapy命令行工具及其所有可用子命令的文档。（提供文件）
*   具有可插拔后端的饲料出口商（197）（提供文档）
*   延迟信号（193）
*   向item pipeline open_spider（）添加了两个新方法，使用延迟支持关闭_spider（）（195）
*   支持覆盖每个spider的默认请求头（181）
*   将默认的spider管理器替换为具有类似功能但不依赖于双绞线插件的管理器（186）
*   将Debian包拆分为两个包-库和服务（187）
*   Scrapy 原木重构（188）
*   在不同的运行中保持持久的 Spider 上下文的新扩展（203）
*   补充 `dont_redirect` 避免重定向的request.meta键（233）
*   补充 `dont_retry` 用于避免重试的request.meta密钥（234）

### 命令行工具更改

*   新的 `scrapy` 替换旧命令的命令 `scrapy-ctl.py` （199）-只有一个全局 `scrapy` 现在命令，而不是一个 `scrapy-ctl.py` 每个项目-已添加 `scrapy.bat` 用于从Windows更方便地运行的脚本
*   将bash完成添加到命令行工具（210）
*   重命名命令 `start` 到 `runserver` （209）

### API更改

*   `url` 和 `body` 请求对象的属性现在是只读的（230）
*   `Request.copy()` 和 `Request.replace()` 现在也复制他们的 `callback` 和 `errback` 属性（231）
*   远离的 `UrlFilterMiddleware` 从 `scrapy.contrib` （默认情况下已禁用）
*   非现场Middelware不会过滤掉来自没有允许域属性的spider的任何请求（225）
*   删除 Spider 管理器 `load()` 方法。现在spider被加载到构造函数本身中。
*   ```py
    对Scrapy Manager（现在称为“crawler”）的更改：
    ```

    *   `scrapy.core.manager.ScrapyManager` class renamed to `scrapy.crawler.Crawler`
    *   `scrapy.core.manager.scrapymanager` singleton moved to `scrapy.project.crawler`
*   移动模块： `scrapy.contrib.spidermanager` 到 `scrapy.spidermanager`
*   Spider 经理辛格尔顿从 `scrapy.spider.spiders` 到 [``](#id1)spiders` attribute of `` crapy.project.crawler``单件。
*   ```py
    已移动的统计信息收集器类：（204）
    ```

    *   `scrapy.stats.collector.StatsCollector` to `scrapy.statscol.StatsCollector`
    *   `scrapy.stats.collector.SimpledbStatsCollector` to `scrapy.contrib.statscol.SimpledbStatsCollector`
*   默认的每个命令设置现在在 `default_settings` 命令对象类的属性（201）
*   ```py
    已更改项管道的参数 process_item() 方法从 (spider, item) 到 (item, spider)
    ```

    *   保持向后兼容性（带有反预测警告）
*   ```py
    感动 scrapy.core.signals 模块到 scrapy.signals
    ```

    *   保持向后兼容性（带有反预测警告）
*   ```py
    感动 scrapy.core.exceptions 模块到 scrapy.exceptions
    ```

    *   保持向后兼容性（带有反预测警告）
*   补充 `handles_request()` 类方法 `BaseSpider`
*   下降 `scrapy.log.exc()` 功能（使用） `scrapy.log.err()` 相反）
*   下降 `component` 的参数 `scrapy.log.msg()` 功能
*   下降 `scrapy.log.log_level` 属性
*   补充 `from_settings()` 向Spider管理器和项目管道管理器提供类方法

### 对设置的更改

*   补充 `HTTPCACHE_IGNORE_SCHEMES` 设置为忽略某些方案打开！httpcachemiddleware（225）
*   补充 `SPIDER_QUEUE_CLASS` 定义要使用的 Spider 队列的设置（220）
*   补充 `KEEP_ALIVE` 设置（α220）
*   远离的 `SERVICE_QUEUE` 设置（α220）
*   远离的 `COMMANDS_SETTINGS_MODULE` 设置（α201）
*   更名 `REQUEST_HANDLERS` 到 `DOWNLOAD_HANDLERS` 并使下载处理程序类（而不是函数）

## Scrapy 0.9

旧问题追踪器（trac）中的nnn参考票等数字不再可用。

### 新功能和改进

*   向scrappy.mail添加了smtp-auth支持
*   添加的新设置： `MAIL_USER` ， `MAIL_PASS` （ [r2065](http://hg.scrapy.org/scrapy/changeset/2065) （149）
*   添加了新的scrappy ctl view命令-在浏览器中查看url，如scrappy所见（ [r2039](http://hg.scrapy.org/scrapy/changeset/2039) ）
*   添加了用于控制 Scrapy 进程的Web服务（这也会取消Web控制台的支持）。（ [r2053](http://hg.scrapy.org/scrapy/changeset/2053) （167）
*   支持将Scrapy作为服务运行，用于生产系统（ [r1988](http://hg.scrapy.org/scrapy/changeset/1988) ， [r2054](http://hg.scrapy.org/scrapy/changeset/2054) ， [r2055](http://hg.scrapy.org/scrapy/changeset/2055) ， [r2056](http://hg.scrapy.org/scrapy/changeset/2056) ， [r2057](http://hg.scrapy.org/scrapy/changeset/2057) （168）
*   添加了包装感应库（文档目前仅在源代码中可用）。（ [r2011](http://hg.scrapy.org/scrapy/changeset/2011) ）
*   简化和改进的响应编码支持（ [r1961](http://hg.scrapy.org/scrapy/changeset/1961) ， [r1969](http://hg.scrapy.org/scrapy/changeset/1969) ）
*   补充 `LOG_ENCODING` 设置（ [r1956](http://hg.scrapy.org/scrapy/changeset/1956) ，文档可用）
*   补充 `RANDOMIZE_DOWNLOAD_DELAY` 设置（默认启用）（ [r1923](http://hg.scrapy.org/scrapy/changeset/1923) ，文档可用）
*   `MailSender` 不再是IO阻塞（ [r1955](http://hg.scrapy.org/scrapy/changeset/1955) （146）
*   Linkextractor和新的Crawlspider现在处理相对的基标记URL（ [r1960](http://hg.scrapy.org/scrapy/changeset/1960) （148）
*   项目加载器和处理器的几个改进（ [r2022](http://hg.scrapy.org/scrapy/changeset/2022) ， [r2023](http://hg.scrapy.org/scrapy/changeset/2023) ， [r2024](http://hg.scrapy.org/scrapy/changeset/2024) ， [r2025](http://hg.scrapy.org/scrapy/changeset/2025) ， [r2026](http://hg.scrapy.org/scrapy/changeset/2026) ， [r2027](http://hg.scrapy.org/scrapy/changeset/2027) ， [r2028](http://hg.scrapy.org/scrapy/changeset/2028) ， [r2029](http://hg.scrapy.org/scrapy/changeset/2029) ， [r2030](http://hg.scrapy.org/scrapy/changeset/2030) ）
*   增加了对向telnet控制台添加变量的支持（ [r2047](http://hg.scrapy.org/scrapy/changeset/2047) （165）
*   支持不带回调的请求（ [r2050](http://hg.scrapy.org/scrapy/changeset/2050) （166）

### API更改

*   变化 `Spider.domain_name` 到 `Spider.name` （SET-012， [r1975](http://hg.scrapy.org/scrapy/changeset/1975) ）
*   `Response.encoding` 现在是检测到的编码（ [r1961](http://hg.scrapy.org/scrapy/changeset/1961) ）
*   `HttpErrorMiddleware` 现在不返回任何值或引发异常（ [r2006](http://hg.scrapy.org/scrapy/changeset/2006) （157）
*   `scrapy.command` 模块重新定位（ [r2035](http://hg.scrapy.org/scrapy/changeset/2035) ， [r2036](http://hg.scrapy.org/scrapy/changeset/2036) ， [r2037](http://hg.scrapy.org/scrapy/changeset/2037) ）
*   补充 `ExecutionQueue` 用来喂 Spider （ [r2034](http://hg.scrapy.org/scrapy/changeset/2034) ）
*   远离的 `ExecutionEngine` 独生子女 [r2039](http://hg.scrapy.org/scrapy/changeset/2039) ）
*   端口 `S3ImagesStore` （图像管道）使用boto和线程（ [r2033](http://hg.scrapy.org/scrapy/changeset/2033) ）
*   移动模块： `scrapy.management.telnet` 到 `scrapy.telnet` （ [r2047](http://hg.scrapy.org/scrapy/changeset/2047) ）

### 更改为默认设置

*   更改的默认值 `SCHEDULER_ORDER` 到 `DFO` （ [r1939](http://hg.scrapy.org/scrapy/changeset/1939) ）

## Scrapy 0.8

旧问题追踪器（trac）中的nnn参考票等数字不再可用。

### 新特点

*   添加了默认的响应编码设置（ [r1809](http://hg.scrapy.org/scrapy/changeset/1809) ）
*   补充 `dont_click` 参数 `FormRequest.from_response()` 方法（ [r1813](http://hg.scrapy.org/scrapy/changeset/1813) ， [r1816](http://hg.scrapy.org/scrapy/changeset/1816) ）
*   补充 `clickdata` 参数 `FormRequest.from_response()` 方法（ [r1802](http://hg.scrapy.org/scrapy/changeset/1802) ， [r1803](http://hg.scrapy.org/scrapy/changeset/1803) ）
*   添加了对HTTP代理的支持（ `HttpProxyMiddleware` （ [r1781](http://hg.scrapy.org/scrapy/changeset/1781) ， [r1785](http://hg.scrapy.org/scrapy/changeset/1785) ）
*   当过滤掉请求时，异地 Spider 中间件现在记录消息。（ [r1841](http://hg.scrapy.org/scrapy/changeset/1841) ）

### 向后不兼容的更改

*   改变 `scrapy.utils.response.get_meta_refresh()` 签名（签名） [r1804](http://hg.scrapy.org/scrapy/changeset/1804) ）
*   已删除，已弃用 `scrapy.item.ScrapedItem` 类使用 `scrapy.item.Item instead` （ [r1838](http://hg.scrapy.org/scrapy/changeset/1838) ）
*   已删除，已弃用 `scrapy.xpath` 模块使用 `scrapy.selector` 相反。（ [r1836](http://hg.scrapy.org/scrapy/changeset/1836) ）
*   已删除，已弃用 `core.signals.domain_open` 信号使用 `core.signals.domain_opened` 而不是（ [r1822](http://hg.scrapy.org/scrapy/changeset/1822) ）
*   ```py
    log.msg() 现在收到一个 spider 论证（论证）  ）
    ```

    *   旧的域参数已被弃用，将在0.9中删除。对于 Spider ，你应该经常使用 `spider` 参数并传递spider引用。如果确实要传递字符串，请使用 `component` 改为参数。
*   改变核心信号 `domain_opened` ， `domain_closed` ， `domain_idle`
*   ```py
    将项目管道更改为使用spider而不是域
    ```

    *   这个 `domain` 的参数 `process_item()` 项目管道方法已更改为 `spider` ，新签名为： `process_item(spider, item)` （ [r1827](http://hg.scrapy.org/scrapy/changeset/1827) （105）
    *   要快速移植代码（使用Scrapy0.8），只需使用 `spider.domain_name` 你以前用过的地方 `domain` .
*   ```py
    更改了stats API以使用spider而不是域（  （113）
    ```

    *   `StatsCollector` 已更改为在其方法中接收 Spider 引用（而不是域）（ `set_value` ， `inc_value` 等）。
    *   补充 `StatsCollector.iter_spider_stats()` 方法
    *   远离的 `StatsCollector.list_domains()` 方法
    *   另外，stats信号被重命名，现在传递 Spider 引用（而不是域）。以下是更改的摘要：
    *   要快速移植代码（使用Scrapy0.8），只需使用 `spider.domain_name` 你以前用过的地方 `domain` . `spider_stats` 包含与完全相同的数据 `domain_stats` .
*   ```py
    CloseDomain 扩展移动到 scrapy.contrib.closespider.CloseSpider （  ）
    ```

    *   ```py
        其设置也被重命名：
        ```

        *   `CLOSEDOMAIN_TIMEOUT` to `CLOSESPIDER_TIMEOUT`
        *   `CLOSEDOMAIN_ITEMCOUNT` to `CLOSESPIDER_ITEMCOUNT`
*   已删除，已弃用 `SCRAPYSETTINGS_MODULE` 环境变量-使用 `SCRAPY_SETTINGS_MODULE` 而不是（ [r1840](http://hg.scrapy.org/scrapy/changeset/1840) ）
*   重命名的设置： `REQUESTS_PER_DOMAIN` 到 `CONCURRENT_REQUESTS_PER_SPIDER` （ [r1830](http://hg.scrapy.org/scrapy/changeset/1830) ， [r1844](http://hg.scrapy.org/scrapy/changeset/1844) ）
*   重命名的设置： `CONCURRENT_DOMAINS` 到 `CONCURRENT_SPIDERS` （ [r1830](http://hg.scrapy.org/scrapy/changeset/1830) ）
*   重构HTTP缓存中间件
*   HTTP缓存中间件经过了大量的重构，保留了相同的功能，但删除了域分段。（ [r1843](http://hg.scrapy.org/scrapy/changeset/1843) ）
*   重命名的异常： `DontCloseDomain` 到 `DontCloseSpider` （ [r1859](http://hg.scrapy.org/scrapy/changeset/1859) （120）
*   重命名的扩展名： `DelayedCloseDomain` 到 `SpiderCloseDelay` （ [r1861](http://hg.scrapy.org/scrapy/changeset/1861) （121）
*   已删除已过时 `scrapy.utils.markup.remove_escape_chars` 功能使用 `scrapy.utils.markup.replace_escape_chars` 而不是（ [r1865](http://hg.scrapy.org/scrapy/changeset/1865) ）

## Scrapy 0.7

第一次发行的Scrapy。