# Logging

> 译者：[OSGeo 中国](https://www.osgeo.cn/)

注解

`scrapy.log` 已经不赞成与函数一起使用，而赞成显式调用Python标准日志记录。继续阅读以了解有关新日志记录系统的更多信息。

Scrapy 用途 [Python's builtin logging system](https://docs.python.org/3/library/logging.html) 用于事件日志记录。我们将提供一些简单的示例来帮助您入门，但对于更高级的用例，强烈建议您仔细阅读其文档。

日志记录是开箱即用的，可以在某种程度上使用中列出的碎片设置进行配置。 [日志记录设置](#topics-logging-settings) .

Scrapy电话 [`scrapy.utils.log.configure_logging()`](#scrapy.utils.log.configure_logging "scrapy.utils.log.configure_logging") 设置一些合理的默认值并在 [日志记录设置](#topics-logging-settings) 当运行命令时，建议在运行脚本的scrapy时手动调用它，如中所述。 [从脚本中运行Scrapy](practices.html#run-from-script) .

## 日志级别

python的内置日志记录定义了5个不同的级别，以指示给定日志消息的严重性。以下是标准的，按降序排列：

1.  `logging.CRITICAL` -对于严重错误（严重性最高）
2.  `logging.ERROR` -对于常规错误
3.  `logging.WARNING` -用于警告消息
4.  `logging.INFO` -以获取信息性消息
5.  `logging.DEBUG` -用于调试消息（最低严重性）

## 如何记录消息

下面是如何使用 `logging.WARNING`

```py
import logging
logging.warning("This is a warning")

```

在标准的5个级别中，有一个用于发布日志消息的快捷方式，还有一个常规的 `logging.log` 方法，该方法将给定的级别作为参数。如果需要，最后一个示例可以重写为：

```py
import logging
logging.log(logging.WARNING, "This is a warning")

```

除此之外，您还可以创建不同的“记录器”来封装消息。（例如，常见的做法是为每个模块创建不同的记录器）。这些记录器可以独立配置，并且允许层次结构。

前面的示例在后台使用根记录器，它是一个顶级记录器，所有消息都在其中传播（除非另有规定）。使用 `logging` 帮助程序只是显式获取根记录器的快捷方式，因此这也相当于最后一段代码：

```py
import logging
logger = logging.getLogger()
logger.warning("This is a warning")

```

您可以使用不同的记录器，只需将其名称 `logging.getLogger` 功能：

```py
import logging
logger = logging.getLogger('mycustomlogger')
logger.warning("This is a warning")

```

最后，通过使用 `__name__` 变量，用当前模块的路径填充：

```py
import logging
logger = logging.getLogger(__name__)
logger.warning("This is a warning")

```

参见

```py
模块日志记录， 
```

基本日志教程

```py
模块日志记录， 
```

关于伐木工人的进一步文件

## 从 Spider 记录

Scrapy提供了 [`logger`](spiders.html#scrapy.spiders.Spider.logger "scrapy.spiders.Spider.logger") 在每个 Spider 实例中，可以这样访问和使用：

```py
import scrapy

class MySpider(scrapy.Spider):

    name = 'myspider'
    start_urls = ['https://scrapinghub.com']

    def parse(self, response):
        self.logger.info('Parse function called on %s', response.url)

```

这个记录器是使用 Spider 的名称创建的，但是您可以使用任何您想要的自定义Python记录器。例如：：

```py
import logging
import scrapy

logger = logging.getLogger('mycustomlogger')

class MySpider(scrapy.Spider):

    name = 'myspider'
    start_urls = ['https://scrapinghub.com']

    def parse(self, response):
        logger.info('Parse function called on %s', response.url)

```

## 日志记录配置

日志记录者自己不管理如何显示通过它们发送的消息。对于此任务，可以将不同的“处理程序”附加到任何记录器实例，它们将这些消息重定向到适当的目标，例如标准输出、文件、电子邮件等。

默认情况下，scrappy根据下面的设置为根记录器设置和配置处理程序。

### 日志记录设置

这些设置可用于配置日志记录：

*   [`LOG_FILE`](settings.html#std:setting-LOG_FILE)
*   [`LOG_ENABLED`](settings.html#std:setting-LOG_ENABLED)
*   [`LOG_ENCODING`](settings.html#std:setting-LOG_ENCODING)
*   [`LOG_LEVEL`](settings.html#std:setting-LOG_LEVEL)
*   [`LOG_FORMAT`](settings.html#std:setting-LOG_FORMAT)
*   [`LOG_DATEFORMAT`](settings.html#std:setting-LOG_DATEFORMAT)
*   [`LOG_STDOUT`](settings.html#std:setting-LOG_STDOUT)
*   [`LOG_SHORT_NAMES`](settings.html#std:setting-LOG_SHORT_NAMES)

前两个设置定义日志消息的目标。如果 [`LOG_FILE`](settings.html#std:setting-LOG_FILE) 设置后，通过根记录器发送的消息将被重定向到名为 [`LOG_FILE`](settings.html#std:setting-LOG_FILE) 带编码 [`LOG_ENCODING`](settings.html#std:setting-LOG_ENCODING) . 如果未设置 [`LOG_ENABLED`](settings.html#std:setting-LOG_ENABLED) 是 `True` ，将在标准错误上显示日志消息。最后，如果 [`LOG_ENABLED`](settings.html#std:setting-LOG_ENABLED) 是 `False` ，将不会有任何可见的日志输出。

[`LOG_LEVEL`](settings.html#std:setting-LOG_LEVEL) 确定要显示的最低严重性级别，将筛选出严重性较低的消息。它的范围包括 [日志级别](#topics-logging-levels) .

[`LOG_FORMAT`](settings.html#std:setting-LOG_FORMAT) 和 [`LOG_DATEFORMAT`](settings.html#std:setting-LOG_DATEFORMAT) 指定用作所有消息布局的格式字符串。这些字符串可以包含中列出的任何占位符 [logging's logrecord attributes docs](https://docs.python.org/2/library/logging.html#logrecord-attributes) 和 [datetime's strftime and strptime directives](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior) 分别。

如果 [`LOG_SHORT_NAMES`](settings.html#std:setting-LOG_SHORT_NAMES) 设置后，日志将不会显示打印日志的 Scrapy 组件。默认情况下，它是未设置的，因此日志包含负责该日志输出的废料组件。

### 命令行选项

有一些命令行参数可用于所有命令，您可以使用这些参数来覆盖有关日志记录的一些零碎设置。

*   ```py
    --logfile FILE
    ```

    重写 [`LOG_FILE`](settings.html#std:setting-LOG_FILE)
*   ```py
    --loglevel/-L LEVEL
    ```

    重写 [`LOG_LEVEL`](settings.html#std:setting-LOG_LEVEL)
*   ```py
    --nolog
    ```

    集合 [`LOG_ENABLED`](settings.html#std:setting-LOG_ENABLED) 到 `False`

参见

```py
模块 
```

有关可用处理程序的进一步文档

### 高级自定义

因为scrapy使用stdlib日志记录模块，所以可以使用stdlib日志记录的所有功能自定义日志记录。

例如，假设您正在抓取一个返回许多HTTP 404和500响应的网站，并且您希望隐藏像这样的所有消息：

```py
2016-12-16 22:00:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring
response <500 http://quotes.toscrape.com/page/1-34/>: HTTP status code
is not handled or not allowed

```

首先要注意的是一个记录器名称-它在括号中： `[scrapy.spidermiddlewares.httperror]` . 如果你得到公正 `[scrapy]` 然后 [`LOG_SHORT_NAMES`](settings.html#std:setting-LOG_SHORT_NAMES) 可能设置为true；设置为false并重新运行爬网。

接下来，我们可以看到消息具有信息级别。为了隐藏它，我们应该为 `scrapy.spidermiddlewares.httperror` 高于信息；信息后的下一级是警告。可以这样做，例如在 Spider 的 `__init__` 方法：

```py
import logging
import scrapy

class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        logger = logging.getLogger('scrapy.spidermiddlewares.httperror')
        logger.setLevel(logging.WARNING)
        super().__init__(*args, **kwargs)

```

如果您再次运行此 Spider ，则从 `scrapy.spidermiddlewares.httperror` 日志记录器将消失。

## scrapy.utils.log模块

```py
scrapy.utils.log.configure_logging(settings=None, install_root_handler=True)
```

初始化Scrapy的日志记录默认值。

| 参数: | 

*   **settings** (dict, [`Settings`](api.html#scrapy.settings.Settings "scrapy.settings.Settings") object or `None`) -- 用于创建和配置根记录器处理程序的设置（默认值：无）。
*   **install_root_handler** (_bool_) -- 是否安装根日志记录处理程序（默认值：true）

 |
| --- | --- |

此功能可以：

*   通过python标准日志记录路由警告和扭曲日志记录
*   分别将调试和错误级别分配给残缺和扭曲的记录器
*   如果日志输出设置为真，则将stdout路由到日志

什么时候？ `install_root_handler` 为true（默认值），此函数还根据给定的设置为根记录器创建处理程序（请参见 [日志记录设置](#topics-logging-settings) ）可以使用替代默认选项 `settings` 争论。什么时候？ `settings` 为空或无，使用默认值。

`configure_logging` 在使用scrappy命令时自动调用，但在运行自定义脚本时需要显式调用。在这种情况下，不需要使用它，但建议使用它。

如果您计划自己配置处理程序，仍然建议您调用此函数，通过 `install_root_handler=False` . 请记住，在这种情况下，不会默认设置任何日志输出。

要开始手动配置日志输出，可以使用 [logging.basicConfig()](https://docs.python.org/2/library/logging.html#logging.basicConfig) 设置基本根处理程序。这是一个关于如何重定向的示例 `INFO` 或更高的文件消息：

```py
import logging
from scrapy.utils.log import configure_logging

configure_logging(install_root_handler=False)
logging.basicConfig(
    filename='log.txt',
    format='%(levelname)s: %(message)s',
    level=logging.INFO
)

```

参照 [从脚本中运行Scrapy](practices.html#run-from-script) 有关使用scrapy的更多详细信息。